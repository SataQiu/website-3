"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[5151],{7130:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"release-v1.1.0","metadata":{"permalink":"/zh-Hans/blog/release-v1.1.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2023-01-03-release/index.md","source":"@site/blog/2023-01-03-release/index.md","title":"Koordinator v1.1: \u8ba9\u8c03\u5ea6\u611f\u77e5\u8d1f\u8f7d\u4e0e\u5e72\u6270\u68c0\u6d4b\u91c7\u96c6","description":"\u80cc\u666f","date":"2023-01-03T00:00:00.000Z","formattedDate":"2023\u5e741\u67083\u65e5","tags":[{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":16.875,"truncated":false,"authors":[{"name":"Siyu Wang","title":"Koordinator maintainer","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"frontMatter":{"slug":"release-v1.1.0","title":"Koordinator v1.1: \u8ba9\u8c03\u5ea6\u611f\u77e5\u8d1f\u8f7d\u4e0e\u5e72\u6270\u68c0\u6d4b\u91c7\u96c6","authors":["FillZpp"],"tags":["release"]},"nextItem":{"title":"Koordinator v1.0: \u6b63\u5f0f\u53d1\u5e03","permalink":"/zh-Hans/blog/release-v1.0.0"}},"content":"## \u80cc\u666f\\n\\nKoordinator \u65e8\u5728\u4e3a\u7528\u6237\u63d0\u4f9b\u5b8c\u6574\u7684\u6df7\u90e8\u5de5\u4f5c\u8d1f\u8f7d\u7f16\u6392\u3001\u6df7\u90e8\u8d44\u6e90\u8c03\u5ea6\u3001\u6df7\u90e8\u8d44\u6e90\u9694\u79bb\u53ca\u6027\u80fd\u8c03\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u5e2e\u52a9\u7528\u6237\u63d0\u9ad8\u5ef6\u8fdf\u654f\u611f\u670d\u52a1\u7684\u8fd0\u884c\u6027\u80fd\uff0c\u6316\u6398\u7a7a\u95f2\u8282\u70b9\u8d44\u6e90\u5e76\u5206\u914d\u7ed9\u771f\u6b63\u6709\u9700\u8981\u7684\u8ba1\u7b97\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u9ad8\u5168\u5c40\u7684\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002\\n\\n\u4ece 2022 \u5e74 4 \u6708\u53d1\u5e03\u4ee5\u6765\uff0cKoordinator \u8fc4\u4eca\u4e00\u5171\u8fed\u4ee3\u53d1\u5e03\u4e86 9 \u4e2a\u7248\u672c\u3002\u9879\u76ee\u7ecf\u5386\u7684\u5927\u534a\u5e74\u53d1\u5c55\u8fc7\u7a0b\u4e2d\uff0c\u793e\u533a\u5438\u7eb3\u4e86\u5305\u62ec\u963f\u91cc\u5df4\u5df4\u3001\u5c0f\u7c73\u3001\u5c0f\u7ea2\u4e66\u3001\u7231\u5947\u827a\u3001360\u3001\u6709\u8d5e \u7b49\u5728\u5185\u7684\u5927\u91cf\u4f18\u79c0\u5de5\u7a0b\u5e08\uff0c\u8d21\u732e\u4e86\u4f17\u591a\u7684\u60f3\u6cd5\u3001\u4ee3\u7801\u548c\u573a\u666f\uff0c\u4e00\u8d77\u63a8\u52a8 Koordinator \u9879\u76ee\u7684\u6210\u719f\u3002\\n\\n\u4eca\u5929\uff0c\u5f88\u9ad8\u5174\u7684\u5ba3\u5e03 Koordinator v1.1 \u6b63\u5f0f\u53d1\u5e03\uff0c\u5b83\u5305\u542b\u4e86\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6/\u91cd\u8c03\u5ea6\u3001cgroup v2 \u652f\u6301\u3001\u5e72\u6270\u68c0\u6d4b\u6307\u6807\u91c7\u96c6\uff0c\u4ee5\u53ca\u5176\u4ed6\u4e00\u7cfb\u5217\u4f18\u5316\u70b9\u3002\u63a5\u4e0b\u6765\u6211\u4eec\u5c31\u9488\u5bf9\u8fd9\u4e9b\u65b0\u589e\u7279\u6027\u505a\u6df1\u5165\u89e3\u8bfb\u4e0e\u8bf4\u660e\u3002\\n\\n## \u7248\u672c\u7279\u6027\u6df1\u5165\u89e3\u8bfb\\n\\n### \u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\\n\\n#### \u652f\u6301\u6309\u5de5\u4f5c\u8d1f\u8f7d\u7c7b\u578b\u7edf\u8ba1\u548c\u5747\u8861\u8d1f\u8f7d\u6c34\u4f4d\\n\\nKoordinator v1.0 \u53ca\u4e4b\u524d\u7684\u7248\u672c\uff0c\u63d0\u4f9b\u4e86\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u63d0\u4f9b\u57fa\u672c\u7684\u5229\u7528\u7387\u9608\u503c\u8fc7\u6ee4\u4fdd\u62a4\u9ad8\u8d1f\u8f7d\u6c34\u4f4d\u7684\u8282\u70b9\u7ee7\u7eed\u6076\u5316\u5f71\u54cd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8fd0\u884c\u65f6\u8d28\u91cf\uff0c\u4ee5\u53ca\u901a\u8fc7\u9884\u4f30\u673a\u5236\u89e3\u51b3\u89e3\u51b3\u51b7\u8282\u70b9\u8fc7\u8f7d\u7684\u60c5\u51b5\u3002\u5df2\u6709\u7684\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u80fd\u89e3\u51b3\u5f88\u591a\u5e38\u89c1\u573a\u666f\u7684\u95ee\u9898\u3002\u4f46\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u4f5c\u4e3a\u4e00\u79cd\u4f18\u5316\u624b\u6bb5\uff0c\u8fd8\u6709\u6bd4\u8f83\u591a\u7684\u573a\u666f\u662f\u9700\u8981\u5b8c\u5584\u7684\u3002\\n\\n\u76ee\u524d\u7684\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u4e3b\u8981\u89e3\u51b3\u4e86\u96c6\u7fa4\u5185\u6574\u673a\u7ef4\u5ea6\u7684\u8d1f\u8f7d\u5747\u8861\u6548\u679c\uff0c\u4f46\u6709\u53ef\u80fd\u51fa\u73b0\u4e00\u4e9b\u7279\u6b8a\u7684\u60c5\u51b5\uff1a\u8282\u70b9\u90e8\u7f72\u4e86\u4e0d\u5c11\u79bb\u7ebfPod\u8fd0\u884c\uff0c\u62c9\u9ad8\u4e86\u6574\u673a\u7684\u5229\u7528\u7387\uff0c\u4f46\u5728\u7ebf\u5e94\u7528\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4f53\u5229\u7528\u7387\u504f\u4f4e\u3002\u8fd9\u4e2a\u65f6\u5019\u5982\u679c\u6709\u65b0\u7684\u5728\u7ebfPod\uff0c\u4e14\u6574\u4e2a\u96c6\u7fa4\u5185\u7684\u8d44\u6e90\u6bd4\u8f83\u7d27\u5f20\u65f6\uff0c\u4f1a\u6709\u5982\u4e0b\u7684\u95ee\u9898\uff1a\\n\\n1. \u6709\u53ef\u80fd\u56e0\u4e3a\u6574\u673a\u5229\u7528\u7387\u8d85\u8fc7\u6574\u673a\u5b89\u5168\u9608\u503c\u5bfc\u81f4\u65e0\u6cd5\u8c03\u5ea6\u5230\u8fd9\u4e2a\u8282\u70b9\u4e0a\u7684\uff1b\\n2. \u8fd8\u53ef\u80fd\u51fa\u73b0\u4e00\u4e2a\u8282\u70b9\u7684\u5229\u7528\u7387\u867d\u7136\u76f8\u5bf9\u6bd4\u8f83\u4f4e\uff0c\u4f46\u4e0a\u9762\u8dd1\u7684\u5168\u662f\u5728\u7ebf\u5e94\u7528\u7387\uff0c\u4ece\u5728\u7ebf\u5e94\u7528\u89d2\u5ea6\u770b\uff0c\u5229\u7528\u7387\u5df2\u7ecf\u504f\u9ad8\u4e86\uff0c\u4f46\u6309\u7167\u5f53\u524d\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u8fd8\u4f1a\u7ee7\u7eed\u8c03\u5ea6\u8fd9\u4e2aPod\u4e0a\u6765\uff0c\u5bfc\u81f4\u8be5\u8282\u70b9\u5806\u79ef\u4e86\u5927\u91cf\u7684\u5728\u7ebf\u5e94\u7528\uff0c\u6574\u4f53\u7684\u8fd0\u884c\u6548\u679c\u5e76\u4e0d\u597d\u3002\\n\\n\u5728 Koordinator v1.1 \u4e2d\uff0ckoord-scheduler \u652f\u6301\u611f\u77e5\u5de5\u4f5c\u8d1f\u8f7d\u7c7b\u578b\uff0c\u533a\u5206\u4e0d\u540c\u7684\u6c34\u4f4d\u548c\u7b56\u7565\u8fdb\u884c\u8c03\u5ea6\u3002\\n\\n\u5728 Filter \u9636\u6bb5\uff0c\u65b0\u589e threshold \u914d\u7f6e `prodUsageThresholds`\uff0c\u8868\u793a\u5728\u7ebf\u5e94\u7528\u7684\u5b89\u5168\u9608\u503c\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u3002\u5982\u679c\u5f53\u524d\u8c03\u5ea6\u7684 Pod \u662f Prod \u7c7b\u578b\uff0ckoord-scheduler \u4f1a\u4ece\u5f53\u524d\u8282\u70b9\u7684 NodeMetric \u4e2d\u7edf\u8ba1\u6240\u6709\u5728\u7ebf\u5e94\u7528\u7684\u5229\u7528\u7387\u4e4b\u548c\uff0c\u5982\u679c\u8d85\u8fc7\u4e86 `prodUsageThresholds` \u5c31\u8fc7\u6ee4\u6389\u8be5\u8282\u70b9\uff1b\u5982\u679c\u662f\u79bb\u7ebf Pod\uff0c\u6216\u8005\u6ca1\u6709\u914d\u7f6e `prodUsageThresholds`\uff0c\u4fdd\u6301\u539f\u6709\u7684\u903b\u8f91\uff0c\u6309\u6574\u673a\u5229\u7528\u7387\u5904\u7406\u3002\\n\\n\u5728 Score \u9636\u6bb5\uff0c\u65b0\u589e\u5f00\u5173 `scoreAccordingProdUsage` \u8868\u793a\u662f\u5426\u6309 Prod \u7c7b\u578b\u7684\u5229\u7528\u7387\u6253\u5206\u5747\u8861\u3002\u9ed8\u8ba4\u4e0d\u542f\u7528\u3002\u5f53\u5f00\u542f\u540e\uff0c\u4e14\u5f53\u524d Pod \u662f Prod \u7c7b\u578b\u7684\u8bdd\uff0ckoord-scheduler \u5728\u9884\u4f30\u7b97\u6cd5\u4e2d\u53ea\u5904\u7406 Prod \u7c7b\u578b\u7684 Pod\uff0c\u5e76\u5bf9 NodeMetrics \u4e2d\u8bb0\u5f55\u7684\u5176\u4ed6\u7684\u672a\u4f7f\u7528\u9884\u4f30\u673a\u5236\u5904\u7406\u7684\u5728\u7ebf\u5e94\u7528\u7684 Pod \u7684\u5f53\u524d\u5229\u7528\u7387\u503c\u8fdb\u884c\u6c42\u548c\uff0c\u6c42\u548c\u540e\u7684\u503c\u53c2\u4e0e\u6700\u7ec8\u7684\u6253\u5206\u3002\u5982\u679c\u6ca1\u6709\u5f00\u542f `scoreAccordingProdUsage`\uff0c\u6216\u8005\u662f\u79bb\u7ebfPod\uff0c\u4fdd\u6301\u539f\u6709\u903b\u8f91\uff0c\u6309\u6574\u673a\u5229\u7528\u7387\u5904\u7406\u3002\\n\\n#### \u652f\u6301\u6309\u767e\u5206\u4f4d\u6570\u5229\u7528\u7387\u5747\u8861\\n\\nKoordinator v1.0\u53ca\u4ee5\u524d\u7684\u7248\u672c\u90fd\u662f\u6309\u7167 koordlet \u4e0a\u62a5\u7684\u5e73\u5747\u5229\u7528\u7387\u6570\u636e\u8fdb\u884c\u8fc7\u6ee4\u548c\u6253\u5206\u3002\u4f46\u5e73\u5747\u503c\u9690\u85cf\u4e86\u6bd4\u8f83\u591a\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u5728 Koordinator v1.1 \u4e2d koordlet \u65b0\u589e\u4e86\u6839\u636e\u767e\u5206\u4f4d\u6570\u7edf\u8ba1\u7684\u5229\u7528\u7387\u805a\u5408\u6570\u636e\u3002\u8c03\u5ea6\u5668\u4fa7\u4e5f\u8ddf\u7740\u505a\u4e86\u76f8\u5e94\u7684\u9002\u914d\u3002\\n\\n\u66f4\u6539\u8c03\u5ea6\u5668\u7684 LoadAware \u63d2\u4ef6\u7684\u914d\u7f6e\uff0c`aggregated` \u8868\u793a\u6309\u7167\u767e\u5206\u4f4d\u6570\u805a\u5408\u6570\u636e\u8fdb\u884c\u6253\u5206\u548c\u8fc7\u6ee4\u3002`aggregated.usageThresholds` \u8868\u793a\u8fc7\u6ee4\u65f6\u7684\u6c34\u4f4d\u9608\u503c\uff1b`aggregated.usageAggregationType` \u8868\u793a\u8fc7\u6ee4\u9636\u6bb5\u8981\u4f7f\u7528\u7684\u767e\u5206\u4f4d\u6570\u7c7b\u578b\uff0c\u652f\u6301 `avg`\uff0c`p99`, `p95`, `p90` \u548c `p50`\uff1b`aggregated.usageAggregatedDuration` \u8868\u793a\u8fc7\u6ee4\u9636\u6bb5\u671f\u671b\u4f7f\u7528\u7684\u805a\u5408\u5468\u671f\uff0c\u5982\u679c\u4e0d\u914d\u7f6e\uff0c\u8c03\u5ea6\u5668\u5c06\u4f7f\u7528 NodeMetrics \u4e2d\u4e0a\u62a5\u7684\u6700\u5927\u5468\u671f\u7684\u6570\u636e\uff1b`aggregated.scoreAggregationType` \u8868\u793a\u5728\u6253\u5206\u9636\u6bb5\u671f\u671b\u4f7f\u7528\u7684\u767e\u5206\u4f4d\u6570\u7c7b\u578b\uff1b`aggregated.scoreAggregatedDuration` \u8868\u793a\u6253\u5206\u9636\u6bb5\u671f\u671b\u4f7f\u7528\u7684\u805a\u5408\u5468\u671f\uff0c\u5982\u679c\u4e0d\u914d\u7f6e\uff0c\u8c03\u5ea6\u5668\u5c06\u4f7f\u7528 NodeMetrics \u4e2d\u4e0a\u62a5\u7684\u6700\u5927\u5468\u671f\u7684\u6570\u636e\u3002\\n\\n\u5728 Filter \u9636\u6bb5\uff0c\u5982\u679c\u914d\u7f6e\u4e86 `aggregated.usageThresholds` \u4ee5\u53ca\u5bf9\u5e94\u7684\u805a\u5408\u7c7b\u578b\uff0c\u8c03\u5ea6\u5668\u5c06\u6309\u8be5\u767e\u5206\u4f4d\u6570\u7edf\u8ba1\u503c\u8fdb\u884c\u8fc7\u6ee4\uff1b\\n\\n\u5728 Score \u9636\u6bb5\uff0c\u5982\u679c\u914d\u7f6e\u4e86 `aggregated.scoreAggregationType`\uff0c\u8c03\u5ea6\u5668\u5c06\u4f1a\u6309\u8be5\u767e\u5206\u4f4d\u6570\u7edf\u8ba1\u503c\u6253\u5206\uff1b\u76ee\u524d\u6682\u65f6\u4e0d\u652f\u6301 Prod Pod \u4f7f\u7528\u767e\u5206\u4f4d\u6570\u8fc7\u6ee4\u3002\\n\\n### \u8d1f\u8f7d\u611f\u77e5\u91cd\u8c03\u5ea6\\n\\nKoordinator \u5728\u8fc7\u53bb\u7684\u51e0\u4e2a\u7248\u672c\u4e2d\uff0c\u6301\u7eed\u7684\u6f14\u8fdb\u91cd\u8c03\u5ea6\u5668\uff0c\u5148\u540e\u4e86\u5f00\u6e90\u5b8c\u6574\u7684\u6846\u67b6\uff0c\u52a0\u5f3a\u4e86\u5b89\u5168\u6027\uff0c\u907f\u514d\u56e0\u8fc7\u5ea6\u9a71\u9010 Pod \u5f71\u54cd\u5728\u7ebf\u5e94\u7528\u7684\u7a33\u5b9a\u6027\u3002\u8fd9\u4e5f\u5f71\u54cd\u4e86\u91cd\u8c03\u5ea6\u529f\u80fd\u7684\u8fdb\u5c55\uff0c\u8fc7\u53bb Koordinator \u6682\u65f6\u6ca1\u6709\u592a\u591a\u529b\u91cf\u5efa\u8bbe\u91cd\u8c03\u5ea6\u80fd\u529b\u3002\u8fd9\u4e00\u60c5\u51b5\u5c06\u4f1a\u5f97\u5230\u6539\u53d8\u3002\\n\\nKoordinator v1.1 \u4e2d\u6211\u4eec\u65b0\u589e\u4e86\u8d1f\u8f7d\u611f\u77e5\u91cd\u8c03\u5ea6\u529f\u80fd\u3002\u65b0\u7684\u63d2\u4ef6\u79f0\u4e3a `LowNodeLoad`\uff0c\u8be5\u63d2\u4ef6\u914d\u5408\u7740\u8c03\u5ea6\u5668\u7684\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u80fd\u529b\uff0c\u53ef\u4ee5\u5f62\u6210\u4e00\u4e2a\u95ed\u73af\uff0c\u8c03\u5ea6\u5668\u7684\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u5728\u8c03\u5ea6\u65f6\u523b\u51b3\u7b56\u9009\u62e9\u6700\u4f18\u8282\u70b9\uff0c\u4f46\u968f\u7740\u65f6\u95f4\u548c\u96c6\u7fa4\u73af\u5883\u4ee5\u53ca\u5de5\u4f5c\u8d1f\u8f7d\u9762\u5bf9\u7684\u6d41\u91cf/\u8bf7\u6c42\u7684\u53d8\u5316\u65f6\uff0c\u8d1f\u8f7d\u611f\u77e5\u91cd\u8c03\u5ea6\u53ef\u4ee5\u4ecb\u5165\u8fdb\u6765\uff0c\u5e2e\u52a9\u4f18\u5316\u8d1f\u8f7d\u6c34\u4f4d\u8d85\u8fc7\u5b89\u5168\u9608\u503c\u7684\u8282\u70b9\u3002 `LowNodeLoad` \u4e0e K8s descheduler \u7684\u63d2\u4ef6 LowNodeUtilization \u4e0d\u540c\u7684\u662f\uff0cLowNodeLoad\u662f\u6839\u636e\u8282\u70b9\u771f\u5b9e\u5229\u7528\u7387\u7684\u60c5\u51b5\u51b3\u7b56\u91cd\u8c03\u5ea6\uff0c\u800c LowNodeUtilization \u662f\u6839\u636e\u8d44\u6e90\u5206\u914d\u7387\u51b3\u7b56\u91cd\u8c03\u5ea6\u3002\\n\\n`LowNodeLoad` \u63d2\u4ef6\u6709\u4e24\u4e2a\u6700\u91cd\u8981\u7684\u53c2\u6570\uff0c\u5206\u522b\u662f `highThresholds` \u548c `lowThresholds`\uff1a\\n\\n- `highThresholds` \u8868\u793a\u8d1f\u8f7d\u6c34\u4f4d\u7684\u8b66\u6212\u9608\u503c\uff0c\u8d85\u8fc7\u8be5\u9608\u503c\u7684\u8282\u70b9\u4e0a\u7684Pod\u5c06\u53c2\u4e0e\u91cd\u8c03\u5ea6\uff1b\\n- `lowThresholds` \u8868\u793a\u8d1f\u8f7d\u6c34\u4f4d\u7684\u5b89\u5168\u6c34\u4f4d\u3002\u4f4e\u4e8e\u8be5\u9608\u503c\u7684\u8282\u70b9\u4e0a\u7684Pod\u4e0d\u4f1a\u88ab\u91cd\u8c03\u5ea6\u3002\\n\\n\u4ee5\u4e0b\u56fe\u4e3a\u4f8b\uff0clowThresholds \u4e3a45%\uff0chighThresholds \u4e3a 70%\uff0c\u90a3\u4e48\u4f4e\u4e8e 45% \u7684\u8282\u70b9\u662f\u5b89\u5168\u7684\uff0c\u56e0\u4e3a\u6c34\u4f4d\u5df2\u7ecf\u5f88\u4f4e\u4e86\uff1b\u9ad8\u4e8e45%\uff0c\u4f46\u662f\u4f4e\u4e8e 70%\u7684\u662f\u533a\u95f4\u662f\u6211\u4eec\u671f\u671b\u7684\u8d1f\u8f7d\u6c34\u4f4d\u8303\u56f4\uff1b\u9ad8\u4e8e70%\u7684\u8282\u70b9\u5c31\u4e0d\u5b89\u5168\u4e86\uff0c\u5e94\u8be5\u628a\u8d85\u8fc770%\u7684\u8fd9\u90e8\u5206\uff08\u5047\u8bbe\u5f53\u524d\u8282\u70b9A\u7684\u8d1f\u8f7d\u6c34\u4f4d\u662f85%\uff09\uff0c\u90a3\u4e48 85% - 70% = 15% \u7684\u8d1f\u8f7d\u964d\u4f4e\uff0c\u7b5b\u9009 Pod \u540e\u6267\u884c\u8fc1\u79fb\u3002\\n\\n![LowNodeLoad \u793a\u4f8b](/img/lownodeload-sample.png)\\n\\n\u8fc1\u79fb\u65f6\uff0c\u8fd8\u8981\u8003\u8651\u5230\u4f4e\u4e8e 45% \u7684\u8fd9\u90e8\u5206\u8282\u70b9\u662f\u6211\u4eec\u91cd\u8c03\u5ea6\u540e\u8981\u627f\u8f7d\u65b0Pod\u7684\u8282\u70b9\uff0c\u6211\u4eec\u9700\u8981\u786e\u4fdd\u8fc1\u79fb\u7684Pod\u7684\u8d1f\u8f7d\u603b\u91cf\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e9b\u4f4e\u8d1f\u8f7d\u8282\u70b9\u7684\u627f\u8f7d\u4e0a\u9650\u3002\u8fd9\u4e2a\u627f\u8f7d\u4e0a\u9650\u5373\u662f highThresholds - \u8282\u70b9\u5f53\u524d\u8d1f\u8f7d\uff0c\u5047\u8bbe\u8282\u70b9B\u7684\u8d1f\u8f7d\u6c34\u4f4d\u662f20%\uff0c\u90a3\u4e48 70%-20% = 50%\uff0c\u8fd950%\u5c31\u662f\u53ef\u4ee5\u627f\u8f7d\u7684\u5bb9\u91cf\u4e86\u3002\u56e0\u6b64\u8fc1\u79fb\u65f6\u6bcf\u9a71\u9010\u4e00\u4e2a Pod\uff0c\u8fd9\u4e2a\u627f\u8f7d\u5bb9\u91cf\u5c31\u5e94\u8be5\u6263\u6389\u5f53\u524d\u91cd\u8c03\u5ea6 Pod \u7684\u5f53\u524d\u8d1f\u8f7d\u6216\u8005\u9884\u4f30\u8d1f\u8f7d\u6216\u8005\u753b\u50cf\u503c\uff08\u8fd9\u90e8\u5206\u503c\u4e0e\u8d1f\u8f7d\u8c03\u5ea6\u91cc\u7684\u503c\u5bf9\u5e94\uff09\u3002\u8fd9\u6837\u5c31\u53ef\u4ee5\u786e\u4fdd\u4e0d\u4f1a\u591a\u8fc1\u79fb\u3002 \\n\\n\u5982\u679c\u4e00\u4e2a\u96c6\u7fa4\u603b\u662f\u53ef\u80fd\u4f1a\u51fa\u73b0\u67d0\u4e9b\u8282\u70b9\u7684\u8d1f\u8f7d\u5c31\u662f\u6bd4\u8f83\u9ad8\uff0c\u800c\u4e14\u6570\u91cf\u5e76\u4e0d\u591a\uff0c\u8fd9\u4e2a\u65f6\u5019\u5982\u679c\u9891\u7e41\u7684\u91cd\u8c03\u5ea6\u8fd9\u4e9b\u8282\u70b9\uff0c\u4e5f\u4f1a\u5e26\u6765\u5b89\u5168\u9690\u60a3\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba9\u7528\u6237\u6309\u9700\u8bbe\u7f6e `numberOfNodes`\u3002\\n\\n\u53e6\u5916\uff0c`LowNodeLoad` \u8bc6\u522b\u51fa\u8d85\u8fc7\u9608\u503c\u7684\u8282\u70b9\u540e\u4f1a\u7b5b\u9009 Pod\uff0c\u5f53\u7b5b\u9009 Pod \u65f6\uff0c\u53ef\u4ee5\u914d\u7f6e\u8981\u652f\u6301\u6216\u8005\u8fc7\u6ee4\u7684 namespace\uff0c\u6216\u8005\u914d\u7f6e pod selector \u7b5b\u9009\uff0c\u4e5f\u53ef\u4ee5\u914d\u7f6e `nodeFit` \u68c0\u67e5\u6bcf\u4e2a\u5907\u9009 Pod \u5bf9\u5e94\u7684 Node Affinity/Node Selector/Toleration \u662f\u5426\u6709\u4e0e\u4e4b\u5339\u914d\u7684 Node\uff0c\u5982\u679c\u6ca1\u6709\u7684\u8bdd\uff0c\u8fd9\u79cd\u8282\u70b9\u4e5f\u4f1a\u88ab\u5ffd\u7565\u3002\u5f53\u7136\u53ef\u4ee5\u8003\u8651\u4e0d\u542f\u7528\u8fd9\u4e2a\u80fd\u529b\uff0c\u901a\u8fc7\u914d\u7f6e `nodeFit` \u4e3a false \u540e\u5373\u53ef\u7981\u7528\uff0c\u6b64\u65f6\u5b8c\u5168\u7531\u5e95\u5c42\u7684 `MigrationController` \u901a\u8fc7 Koordinator Reservation \u9884\u7559\u8d44\u6e90\uff1b\\n\\n\u5f53\u7b5b\u9009\u51fa Pod \u540e\uff0c\u4f1a\u5bf9\u8fd9\u4e9b Pod \u8fdb\u884c\u6392\u5e8f\u3002\u4f1a\u4f9d\u9760Koordinator QoSClass\u3001Kubernetes QoSClass\u3001Priority\u3001\u7528\u91cf\u548c\u521b\u5efa\u65f6\u95f4\u7b49\u591a\u4e2a\u7ef4\u5ea6\u6392\u5e8f\u3002\\n\\n### cgroup v2 \u652f\u6301\\n\\n#### \u80cc\u666f\\n\\nKoordinator \u4e2d\u4f17\u591a\u5355\u673a QoS \u80fd\u529b\u548c\u8d44\u6e90\u538b\u5236/\u5f39\u6027\u7b56\u7565\u6784\u5efa\u5728 Linux Control Group (cgroups) \u673a\u5236\u4e0a\uff0c\u6bd4\u5982 CPU QoS (cpu)\u3001Memory QoS (memory)\u3001CPU Burst (cpu)\u3001CPU Suppress (cpu, cpuset)\uff0ckoordlet \u7ec4\u4ef6\u53ef\u4ee5\u901a\u8fc7 cgroups (v1) \u9650\u5236\u5bb9\u5668\u53ef\u7528\u8d44\u6e90\u7684\u65f6\u95f4\u7247\u3001\u6743\u91cd\u3001\u4f18\u5148\u7ea7\u3001\u62d3\u6251\u7b49\u5c5e\u6027\u3002Linux \u9ad8\u7248\u672c\u5185\u6838\u4e5f\u5728\u6301\u7eed\u589e\u5f3a\u548c\u8fed\u4ee3\u4e86 cgroups \u673a\u5236\uff0c\u5e26\u6765\u4e86 cgroups v2 \u673a\u5236\uff0c\u7edf\u4e00 cgroups \u76ee\u5f55\u7ed3\u6784\uff0c\u6539\u5584 v1 \u4e2d\u4e0d\u540c subsystem/cgroup controller \u4e4b\u95f4\u7684\u534f\u4f5c\uff0c\u5e76\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u90e8\u5206\u5b50\u7cfb\u7edf\u7684\u8d44\u6e90\u7ba1\u7406\u548c\u76d1\u63a7\u80fd\u529b\u3002Kubernetes \u81ea 1.25 \u8d77\u5c06 cgroups v2 \u4f5c\u4e3a GA (general availability) \u7279\u6027\uff0c\u5728 Kubelet \u4e2d\u542f\u7528\u8be5\u7279\u6027\u8fdb\u884c\u5bb9\u5668\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u5728\u7edf\u4e00\u7684 cgroups \u5c42\u6b21\u4e0b\u8bbe\u7f6e\u5bb9\u5668\u7684\u8d44\u6e90\u9694\u79bb\u53c2\u6570\uff0c\u652f\u6301 MemoryQoS \u7684\u589e\u5f3a\u7279\u6027\u3002\\n\\n![cgroup v1/v2 \u7ed3\u6784](/img/cgroup-v1-and-v2.svg)\\n\\n\u5728 Koordinator v1.1 \u4e2d\uff0c\u5355\u673a\u7ec4\u4ef6 koordlet \u65b0\u589e\u5bf9 cgroups v2 \u7684\u652f\u6301\uff0c\u5305\u62ec\u5982\u4e0b\u5de5\u4f5c\uff1a\\n\\n- \u91cd\u6784\u4e86 Resource Executor \u6a21\u5757\uff0c\u4ee5\u7edf\u4e00\u76f8\u540c\u6216\u8fd1\u4f3c\u7684 cgroup \u63a5\u53e3\u5728 v1 \u548c v2 \u4e0d\u540c\u7248\u672c\u4e0a\u7684\u6587\u4ef6\u64cd\u4f5c\uff0c\u4fbf\u4e8e koordlet \u7279\u6027\u517c\u5bb9 cgroups v2 \u548c\u5408\u5e76\u8bfb\u5199\u51b2\u7a81\u3002\\n- \u5728\u5f53\u524d\u5df2\u5f00\u653e\u7684\u5355\u673a\u7279\u6027\u4e2d\u9002\u914d cgroups v2\uff0c\u91c7\u7528\u65b0\u7684 Resource Executor \u6a21\u5757\u66ff\u6362 cgroup \u64cd\u4f5c\uff0c\u4f18\u5316\u4e0d\u540c\u7cfb\u7edf\u73af\u5883\u4e0b\u7684\u62a5\u9519\u65e5\u5fd7\u3002\\n\\nKoordinator v1.1 \u4e2d\u5927\u90e8\u5206 koordlet \u7279\u6027\u5df2\u7ecf\u517c\u5bb9 cgroups v2\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a\\n\\n- \u8d44\u6e90\u5229\u7528\u7387\u91c7\u96c6\\n- \u52a8\u6001\u8d44\u6e90\u8d85\u5356\\n- Batch \u8d44\u6e90\u9694\u79bb\uff08BatchResource\uff0c\u5e9f\u5f03BECgroupReconcile\uff09\\n- CPU QoS\uff08GroupIdentity\uff09\\n- Memory QoS\uff08CgroupReconcile\uff09\\n- CPU \u52a8\u6001\u538b\u5236\uff08BECPUSuppress\uff09\\n- \u5185\u5b58\u9a71\u9010\uff08BEMemoryEvict\uff09\\n- CPU Burst\uff08CPUBurst\uff09\\n- L3 Cache \u53ca\u5185\u5b58\u5e26\u5bbd\u9694\u79bb\uff08RdtResctrl\uff09\\n\\n\u9057\u7559\u7684\u672a\u517c\u5bb9\u7279\u6027\u5982 PSICollector \u5c06\u5728\u63a5\u4e0b\u6765\u7684 v1.2 \u7248\u672c\u4e2d\u8fdb\u884c\u9002\u914d\uff0c\u53ef\u4ee5\u8ddf\u8fdb issue#407 \u83b7\u53d6\u6700\u65b0\u8fdb\u5c55\u3002\u63a5\u4e0b\u6765\u7684 Koordinator \u7248\u672c\u4e2d\u4e5f\u5c06\u9010\u6e10\u5f15\u5165\u66f4\u591a cgroups v2 \u7684\u589e\u5f3a\u529f\u80fd\uff0c\u656c\u8bf7\u671f\u5f85\u3002\\n\\n#### \u4f7f\u7528 cgroups v2\\n\\n\u5728 Koordinator v1.1 \u4e2d\uff0ckoordlet \u5bf9 cgroups v2 \u7684\u9002\u914d\u5bf9\u4e0a\u5c42\u529f\u80fd\u914d\u7f6e\u900f\u660e\uff0c\u9664\u4e86\u88ab\u5e9f\u5f03\u7279\u6027\u7684 feature-gate \u4ee5\u5916\uff0c\u60a8\u65e0\u9700\u53d8\u52a8 ConfigMap `slo-controller-config` \u548c\u5176\u4ed6 feature-gate \u914d\u7f6e\u3002\u5f53 koordlet \u8fd0\u884c\u5728\u542f\u7528 cgroups v2 \u7684\u8282\u70b9\u4e0a\u65f6\uff0c\u76f8\u5e94\u5355\u673a\u7279\u6027\u5c06\u81ea\u52a8\u5207\u6362\u5230 cgroups-v2 \u7cfb\u7edf\u63a5\u53e3\u8fdb\u884c\u64cd\u4f5c\u3002\\n\\n\u6b64\u5916\uff0ccgroups v2 \u662f Linux \u9ad8\u7248\u672c\u5185\u6838\uff08\u5efa\u8bae >=5.8\uff09\u7684\u7279\u6027\uff0c\u5bf9\u7cfb\u7edf\u5185\u6838\u7248\u672c\u548c Kubernetes \u7248\u672c\u6709\u4e00\u5b9a\u4f9d\u8d56\u3002\u5efa\u8bae\u91c7\u7528\u9ed8\u8ba4\u542f\u7528 cgroups v2 \u7684 Linux \u53d1\u884c\u7248\u4ee5\u53ca Kubernetes v1.24 \u4ee5\u4e0a\u7248\u672c\u3002\\n\\n\u66f4\u591a\u5173\u4e8e\u5982\u4f55\u542f\u7528 cgroups v2 \u7684\u8bf4\u660e\uff0c\u8bf7\u53c2\u7167 Kubernetes \u793e\u533a[\u6587\u6863](https://kubernetes.io/docs/concepts/architecture/cgroups/#using-cgroupv2)\u3002\\n\\n### \u5e72\u6270\u68c0\u6d4b\u6307\u6807\u91c7\u96c6\\n\\n\u5728\u771f\u5b9e\u7684\u751f\u4ea7\u73af\u5883\u4e0b\uff0c\u5355\u673a\u7684\u8fd0\u884c\u65f6\u72b6\u6001\u662f\u4e00\u4e2a\u201c\u6df7\u6c8c\u7cfb\u7edf\u201d\uff0c\u8d44\u6e90\u7ade\u4e89\u4ea7\u751f\u7684\u5e94\u7528\u5e72\u6270\u65e0\u6cd5\u7edd\u5bf9\u907f\u514d\u3002Koordinator \u6b63\u5728\u5efa\u7acb\u5e72\u6270\u68c0\u6d4b\u4e0e\u4f18\u5316\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u63d0\u53d6\u5e94\u7528\u8fd0\u884c\u72b6\u6001\u7684\u6307\u6807\uff0c\u8fdb\u884c\u5b9e\u65f6\u7684\u5206\u6790\u548c\u68c0\u6d4b\uff0c\u5728\u53d1\u73b0\u5e72\u6270\u540e\u5bf9\u76ee\u6807\u5e94\u7528\u548c\u5e72\u6270\u6e90\u91c7\u53d6\u66f4\u5177\u9488\u5bf9\u6027\u7684\u7b56\u7565\u3002\\n\\n\u5f53\u524d Koordinator \u5df2\u7ecf\u5b9e\u73b0\u4e86\u4e00\u7cfb\u5217 `Performance Collector`\uff0c\u5728\u5355\u673a\u4fa7\u91c7\u96c6\u4e0e\u5e94\u7528\u8fd0\u884c\u72b6\u6001\u9ad8\u76f8\u5173\u6027\u7684\u5e95\u5c42\u6307\u6807\uff0c\u5e76\u901a\u8fc7 Prometheus \u66b4\u9732\u51fa\u6765\uff0c\u4e3a\u5e72\u6270\u68c0\u6d4b\u80fd\u529b\u548c\u96c6\u7fa4\u5e94\u7528\u8c03\u5ea6\u63d0\u4f9b\u652f\u6301\u3002\\n\\n#### \u6307\u6807\u91c7\u96c6\\n\\nPerformance Collector \u7531\u591a\u4e2a feature-gate \u8fdb\u884c\u63a7\u5236\uff0cKoordinator \u76ee\u524d\u63d0\u4f9b\u4ee5\u4e0b\u51e0\u4e2a\u6307\u6807\u91c7\u96c6\u5668\uff1a\\n\\n- `CPICollector`\uff1a\u7528\u4e8e\u63a7\u5236 CPI \u6307\u6807\u91c7\u96c6\u5668\u3002CPI\uff1aCycles Per Instruction\u3002\u6307\u4ee4\u5728\u8ba1\u7b97\u673a\u4e2d\u6267\u884c\u6240\u9700\u8981\u7684\u5e73\u5747\u65f6\u949f\u5468\u671f\u6570\u3002CPI \u91c7\u96c6\u5668\u57fa\u4e8e Cycles \u548c Instructions \u8fd9\u4e24\u4e2a Kernel PMU\uff08Performance Monitoring Unit\uff09\u4e8b\u4ef6\u4ee5\u53ca perf_event_open(2) \u7cfb\u7edf\u8c03\u7528\u5b9e\u73b0\u3002\\n- `PSICollector`\uff1a\u7528\u4e8e\u63a7\u5236 PSI \u6307\u6807\u91c7\u96c6\u5668\u3002PSI\uff1aPressure Stall Information\u3002\u8868\u793a\u5bb9\u5668\u5728\u91c7\u96c6\u65f6\u95f4\u95f4\u9694\u5185\uff0c\u56e0\u4e3a\u7b49\u5f85 cpu\u3001\u5185\u5b58\u3001IO \u8d44\u6e90\u5206\u914d\u800c\u963b\u585e\u7684\u4efb\u52a1\u6570\u3002\u4f7f\u7528 PSI \u91c7\u96c6\u5668\u524d\uff0c\u9700\u8981\u5728 Anolis OS \u4e2d\u5f00\u542f PSI \u529f\u80fd\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003[\u6587\u6863](https://help.aliyun.com/document_detail/155464.html)\u83b7\u53d6\u5f00\u542f\u65b9\u6cd5\u3002\\n\\nPerformance Collector \u76ee\u524d\u662f\u9ed8\u8ba4\u5173\u95ed\u7684\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 Koordlet \u7684 feature-gates \u9879\u6765\u4f7f\u7528\u5b83\uff0c\u6b64\u9879\u4fee\u6539\u4e0d\u4f1a\u5f71\u54cd\u5176\u4ed6 feature-gate\\n\\n```\\nkubectl edit ds koordlet -n koordinator-system\\n```\\n\\n```yaml\\n...\\nspec:\\n  ...\\n    spec:\\n      containers:\\n      - args:\\n        ...\\n        # modify here\\n        # - -feature-gates=BECPUEvict=true,BEMemoryEvict=true,CgroupReconcile=true,Accelerators=true\\n        - -feature-gates=BECPUEvict=true,BEMemoryEvict=true,CgroupReconcile=true,Accelerators=true,CPICollector=true,PSICollector=true\\n```\\n\\n#### ServiceMonitor\\n\\nv1.1.0 \u7248\u672c\u7684 Koordinator \u4e3a Koordlet \u589e\u52a0\u4e86 ServiceMonitor \u7684\u80fd\u529b\uff0c\u5c06\u6240\u91c7\u96c6\u6307\u6807\u901a\u8fc7 Prometheus \u66b4\u9732\u51fa\u6765\uff0c\u7528\u6237\u53ef\u57fa\u4e8e\u6b64\u80fd\u529b\u91c7\u96c6\u76f8\u5e94\u6307\u6807\u8fdb\u884c\u5e94\u7528\u7cfb\u7edf\u7684\u5206\u6790\u4e0e\u7ba1\u7406\u3002\\n\\nServiceMonitor \u7531 Prometheus \u5f15\u5165\uff0c\u6545\u5728 helm chart \u4e2d\u8bbe\u7f6e\u9ed8\u8ba4\u4e0d\u5f00\u542f\u5b89\u88c5\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5ServiceMonitor\uff1a\\n\\n```\\nhelm install koordinator https://... --set koordlet.enableServiceMonitor=true\\n```\\n\\n\u90e8\u7f72\u540e\u53ef\u5728 Prometheus UI \u627e\u5230\u8be5 Targets\u3002\\n\\n```\\n# HELP koordlet_container_cpi Container cpi collected by koordlet\\n# TYPE koordlet_container_cpi gauge\\nkoordlet_container_cpi{container_id=\\"containerd://498de02ddd3ad7c901b3c80f96c57db5b3ed9a817dbfab9d16b18be7e7d2d047\\",container_name=\\"koordlet\\",cpi_field=\\"cycles\\",node=\\"your-node-name\\",pod_name=\\"koordlet-x8g2j\\",pod_namespace=\\"koordinator-system\\",pod_uid=\\"3440fb9c-423b-48e9-8850-06a6c50f633d\\"} 2.228107503e+09\\nkoordlet_container_cpi{container_id=\\"containerd://498de02ddd3ad7c901b3c80f96c57db5b3ed9a817dbfab9d16b18be7e7d2d047\\",container_name=\\"koordlet\\",cpi_field=\\"instructions\\",node=\\"your-node-name\\",pod_name=\\"koordlet-x8g2j\\",pod_namespace=\\"koordinator-system\\",pod_uid=\\"3440fb9c-423b-48e9-8850-06a6c50f633d\\"} 4.1456092e+09\\n```\\n\\n\u53ef\u4ee5\u671f\u5f85\u7684\u662f\uff0cKoordinator \u5e72\u6270\u68c0\u6d4b\u7684\u80fd\u529b\u5728\u66f4\u590d\u6742\u7684\u771f\u5b9e\u573a\u666f\u4e0b\u8fd8\u9700\u8981\u66f4\u591a\u68c0\u6d4b\u6307\u6807\u7684\u8865\u5145\uff0c\u540e\u7eed\u5c06\u5728\u5982\u5185\u5b58\u3001\u78c1\u76d8 IO \u7b49\u5176\u4ed6\u8bf8\u591a\u8d44\u6e90\u7684\u6307\u6807\u91c7\u96c6\u5efa\u8bbe\u65b9\u9762\u6301\u7eed\u53d1\u529b\u3002\\n\\n### \u5176\u4ed6\u66f4\u65b0\u70b9\\n\\n\u901a\u8fc7 [v1.1 release](https://github.com/koordinator-sh/koordinator/releases/tag/v1.1.0) \u9875\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u66f4\u591a\u7248\u672c\u6240\u5305\u542b\u7684\u65b0\u589e\u529f\u80fd\u3002"},{"id":"release-v1.0.0","metadata":{"permalink":"/zh-Hans/blog/release-v1.0.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-11-03-release/index.md","source":"@site/blog/2022-11-03-release/index.md","title":"Koordinator v1.0: \u6b63\u5f0f\u53d1\u5e03","description":"Koordinator \u4eca\u5e743\u6708\u4efd\u5f00\u6e90\u4ee5\u6765\uff0c\u5148\u540e\u53d1\u5e03\u4e867\u4e2a\u7248\u672c\uff0c\u9010\u6b65\u7684\u628a\u963f\u91cc\u5df4\u5df4&\u963f\u91cc\u4e91\u5185\u90e8\u7684\u6df7\u90e8\u7cfb\u7edf\u7684\u6838\u5fc3\u80fd\u529b\u8f93\u51fa\u5230\u5f00\u6e90\u793e\u533a\uff0c\u5e76\u5728\u4e2d\u95f4\u8fc7\u7a0b\u4e2d\u9010\u6e10\u7684\u88ab Kubernetes\u3001\u5927\u6570\u636e\u3001\u9ad8\u6027\u80fd\u8ba1\u7b97\u3001\u673a\u5668\u5b66\u4e60\u9886\u57df\u6216\u8005\u793e\u533a\u7684\u5173\u6ce8\uff0cKoordinator \u793e\u533a\u4e5f\u9010\u6b65\u83b7\u5f97\u4e86\u4e00\u4e9b\u8d21\u732e\u8005\u7684\u652f\u6301\uff0c\u5e76\u6709\u4e00\u4e9b\u4f01\u4e1a\u5f00\u59cb\u9010\u6b65\u7684\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528 Koordinator \u89e3\u51b3\u5b9e\u9645\u751f\u4ea7\u4e2d\u9047\u5230\u7684\u6210\u672c\u95ee\u9898\u3001\u6df7\u90e8\u95ee\u9898\u7b49\u3002 \u7ecf\u8fc7 Koordinator \u793e\u533a\u7684\u52aa\u529b\uff0c\u6211\u4eec\u6000\u7740\u5341\u5206\u6fc0\u52a8\u7684\u5fc3\u60c5\u5411\u5927\u5bb6\u5ba3\u5e03 Koordinator 1.0 \u7248\u672c\u6b63\u5f0f\u53d1\u5e03\u3002","date":"2022-11-03T00:00:00.000Z","formattedDate":"2022\u5e7411\u67083\u65e5","tags":[{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":6.575,"truncated":false,"authors":[{"name":"Joseph","title":"Koordinator maintainer","url":"https://github.com/eahydra","imageURL":"https://github.com/eahydra.png","key":"joseph"}],"frontMatter":{"slug":"release-v1.0.0","title":"Koordinator v1.0: \u6b63\u5f0f\u53d1\u5e03","authors":["joseph"],"tags":["release"]},"prevItem":{"title":"Koordinator v1.1: \u8ba9\u8c03\u5ea6\u611f\u77e5\u8d1f\u8f7d\u4e0e\u5e72\u6270\u68c0\u6d4b\u91c7\u96c6","permalink":"/zh-Hans/blog/release-v1.1.0"},"nextItem":{"title":"Koordinator v0.7: \u4e3a\u4efb\u52a1\u8c03\u5ea6\u9886\u57df\u6ce8\u5165\u65b0\u6d3b\u529b","permalink":"/zh-Hans/blog/release-v0.7.0"}},"content":"Koordinator \u4eca\u5e743\u6708\u4efd\u5f00\u6e90\u4ee5\u6765\uff0c\u5148\u540e\u53d1\u5e03\u4e867\u4e2a\u7248\u672c\uff0c\u9010\u6b65\u7684\u628a\u963f\u91cc\u5df4\u5df4&\u963f\u91cc\u4e91\u5185\u90e8\u7684\u6df7\u90e8\u7cfb\u7edf\u7684\u6838\u5fc3\u80fd\u529b\u8f93\u51fa\u5230\u5f00\u6e90\u793e\u533a\uff0c\u5e76\u5728\u4e2d\u95f4\u8fc7\u7a0b\u4e2d\u9010\u6e10\u7684\u88ab Kubernetes\u3001\u5927\u6570\u636e\u3001\u9ad8\u6027\u80fd\u8ba1\u7b97\u3001\u673a\u5668\u5b66\u4e60\u9886\u57df\u6216\u8005\u793e\u533a\u7684\u5173\u6ce8\uff0cKoordinator \u793e\u533a\u4e5f\u9010\u6b65\u83b7\u5f97\u4e86\u4e00\u4e9b\u8d21\u732e\u8005\u7684\u652f\u6301\uff0c\u5e76\u6709\u4e00\u4e9b\u4f01\u4e1a\u5f00\u59cb\u9010\u6b65\u7684\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528 Koordinator \u89e3\u51b3\u5b9e\u9645\u751f\u4ea7\u4e2d\u9047\u5230\u7684\u6210\u672c\u95ee\u9898\u3001\u6df7\u90e8\u95ee\u9898\u7b49\u3002 \u7ecf\u8fc7 Koordinator \u793e\u533a\u7684\u52aa\u529b\uff0c\u6211\u4eec\u6000\u7740\u5341\u5206\u6fc0\u52a8\u7684\u5fc3\u60c5\u5411\u5927\u5bb6\u5ba3\u5e03 Koordinator 1.0 \u7248\u672c\u6b63\u5f0f\u53d1\u5e03\u3002\\n\\nKoordinator \u9879\u76ee\u65e9\u671f\u7740\u91cd\u5efa\u8bbe\u6838\u5fc3\u6df7\u90e8\u80fd\u529b -- \u5dee\u5f02\u5316 SLO\uff0c\u5e76\u4e14\u4e3a\u4e86\u8ba9\u7528\u6237\u66f4\u5bb9\u6613\u7684\u4f7f\u7528 Koordinator \u7684\u6df7\u90e8\u80fd\u529b\uff0cKoordinator \u63d0\u4f9b\u4e86 ClusterColocationProfile \u673a\u5236\u5e2e\u52a9\u7528\u6237\u53ef\u4ee5\u4e0d\u7528\u4fee\u6539\u5b58\u91cf\u4ee3\u7801\u5b8c\u6210\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6df7\u90e8\uff0c\u8ba9\u7528\u6237\u9010\u6b65\u7684\u719f\u6089\u6df7\u90e8\u6280\u672f\u3002\u968f\u540e Koordinaor \u9010\u6b65\u5728\u8282\u70b9\u4fa7 QoS \u4fdd\u969c\u673a\u5236\u4e0a\u505a\u4e86\u589e\u5f3a\uff0c\u63d0\u4f9b\u4e86\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e CPU Suppress\u3001CPU Burst\u3001 Memory QoS\u3001L3 Cache/MBA \u8d44\u6e90\u9694\u79bb\u673a\u5236\u548c\u57fa\u4e8e\u6ee1\u8db3\u5ea6\u9a71\u9010\u673a\u5236\u7b49\u591a\u79cd\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5927\u90e8\u5206\u8282\u70b9\u4fa7\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u914d\u5408\u4f7f\u7528 Koordinator Runtime Proxy \u7ec4\u4ef6\uff0c\u53ef\u4ee5\u66f4\u597d\u7684\u517c\u5bb9 Kubernetes kubelet \u539f\u751f\u7ba1\u7406\u673a\u5236\u3002\\n\\n\u5e76\u4e14 Koordinator \u5728\u4efb\u52a1\u8c03\u5ea6\u548c QoS \u611f\u77e5\u8c03\u5ea6\u4ee5\u53ca\u91cd\u8c03\u5ea6\u7b49\u65b9\u9762\u4e5f\u90fd\u63d0\u4f9b\u4e86\u4e00\u4e9b\u521b\u65b0\u65b9\u6848\uff0c\u5efa\u8bbe\u4e86\u5168\u9762\u517c\u5bb9 Kubernetes CPU \u7ba1\u7406\u673a\u5236\u7684\u7cbe\u7ec6\u5316 CPU \u8c03\u5ea6\u80fd\u529b\uff0c\u9762\u5411\u8282\u70b9\u5b9e\u9645\u8d1f\u8f7d\u7684\u5747\u8861\u8c03\u5ea6\u80fd\u529b\u3002\u4e3a\u4e86\u66f4\u597d\u7684\u8ba9\u7528\u6237\u7ba1\u7406\u597d\u8d44\u6e90\uff0c Koordinator \u8fd8\u63d0\u4f9b\u4e86\u8d44\u6e90\u9884\u7559\u80fd\u529b\uff08Reservation)\uff0c\u5e76\u4e14 Koordinator \u57fa\u4e8e Kubernetes \u793e\u533a\u5df2\u6709\u7684Coscheduling\u3001ElasticQuota Scheduling \u80fd\u529b\u505a\u4e86\u8fdb\u4e00\u6b65\u7684\u589e\u5f3a\uff0c\u4e3a\u4efb\u52a1\u8c03\u5ea6\u9886\u57df\u6ce8\u5165\u4e86\u65b0\u7684\u6d3b\u529b\u3002Koordinator \u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u91cd\u8c03\u5ea6\u5668\u6846\u67b6\uff0c\u7740\u91cd\u5efa\u8bbe Descheduler \u7684\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002\\n\\n# \u5b89\u88c5\u6216\u5347\u7ea7 Koordinator v1.0.0\\n\\n## \u4f7f\u7528 Helm \u5b89\u88c5\\n\\n\u60a8\u53ef\u4ee5\u901a\u8fc7 helm v3.5+ \u975e\u5e38\u65b9\u4fbf\u7684\u5b89\u88c5 Koordinator\uff0cHelm \u662f\u4e00\u4e2a\u7b80\u5355\u7684\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u60a8\u53ef\u4ee5\u4ece [\u8fd9\u91cc](https://github.com/helm/helm/releases) \u83b7\u53d6\u5b83\u3002\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Install the latest version.\\n$ helm install koordinator koordinator-sh/koordinator --version 1.0.0\\n```\\n\\n# \u7248\u672c\u529f\u80fd\u7279\u6027\u89e3\u8bfb\\n\\nKoordinator v1.0 \u6574\u4f53\u65b0\u589e\u7684\u7279\u6027\u5e76\u4e0d\u591a\uff0c\u4e3b\u8981\u6709\u4ee5\u4e0b\u4e00\u4e9b\u53d8\u5316\\n\\n## \u72ec\u7acb API Repo\\n\\n\u4e3a\u4e86\u66f4\u65b9\u4fbf\u96c6\u6210\u548c\u4f7f\u7528 Koordiantor \u5b9a\u4e49\u7684 API\uff0c\u5e76\u907f\u514d\u56e0\u4f9d\u8d56 Koordiantor \u5f15\u5165\u989d\u5916\u7684\u4f9d\u8d56\u6216\u8005\u4f9d\u8d56\u51b2\u7a81\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u72ec\u7acb\u7684 API Repo: [koordinator-sh/apis](https://github.com/koordinator-sh/apis)\\n\\n## \u65b0\u589e ElasticQuota Webhook\\n\\n\u5728 Koordinator v0.7 \u7248\u672c\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8e Kubernetes sig-scheduler \u63d0\u4f9b\u7684 ElasticQuota \u505a\u4e86\u8bf8\u591a\u589e\u5f3a\uff0c\u63d0\u4f9b\u4e86\u6811\u5f62\u7ba1\u7406\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\u7b49\uff0c\u53ef\u4ee5\u5f88\u597d\u7684\u5e2e\u52a9\u60a8\u89e3\u51b3\u4f7f\u7528 ElasticQuota \u9047\u5230\u7684\u95ee\u9898\u3002\u5728 Koordinator v1.0 \u7248\u672c\u4e2d\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u4f9b\u4e86 ElasticQuota Webhook\uff0c\u5e2e\u52a9\u60a8\u5728\u4f7f\u7528 ElasticQuota \u6811\u5f62\u7ba1\u7406\u673a\u5236\u65f6\uff0c\u4fdd\u969c\u65b0\u7684 ElasticQuota \u5bf9\u8c61\u9075\u5faa Koordinator \u5b9a\u4e49\u7684\u89c4\u8303\u6216\u7ea6\u675f\uff1a\\n\\n1. \u9664\u4e86\u6839\u8282\u70b9\uff0c\u5176\u4ed6\u6240\u6709\u5b50\u8282\u70b9\u7684 min \u4e4b\u548c\u8981\u5c0f\u4e8e\u7236\u8282\u70b9\u7684 min\u3002\\n2. \u4e0d\u9650\u5236\u5b50\u8282\u70b9 max\uff0c\u5141\u8bb8\u5b50\u8282\u70b9\u7684 max \u5927\u4e8e\u7236\u8282\u70b9\u7684 max\u3002\u8003\u8651\u4ee5\u4e0b\u573a\u666f\uff0c\u96c6\u7fa4\u4e2d\u6709 2 \u4e2a ElasticQuota \u5b50\u6811\uff1adev-parent \u548c production-parent\uff0c\u6bcf\u4e2a\u5b50\u6811\u90fd\u6709\u51e0\u4e2a\u5b50 ElasticQuota\u3002 \u5f53 production-parent \u5fd9\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u53ea\u964d\u4f4e dev-parent \u7684 max \u9650\u5236  dev-parent \u6574\u9897\u5b50\u6811\u7684\u8d44\u6e90\u4f7f\u7528\u91cf\uff0c\u800c\u4e0d\u662f\u964d\u4f4e dev-parent \u5b50\u6811\u7684\u6bcf\u4e2a\u5b50 ElasticQuota \u7684max\u9650\u5236\u7528\u91cf\u3002\\n3. Pod \u4e0d\u80fd\u4f7f\u7528\u7236\u8282\u70b9ElasticQuota\u3002\u5982\u679c\u653e\u5f00\u8fd9\u4e2a\u9650\u5236\uff0c\u4f1a\u5bfc\u81f4\u6574\u4e2a\u5f39\u6027 Quota \u7684\u673a\u5236\u53d8\u7684\u5f02\u5e38\u590d\u6742\uff0c\u6682\u65f6\u4e0d\u8003\u8651\u652f\u6301\u8fd9\u79cd\u573a\u666f\u3002\\n4. \u53ea\u6709\u7236\u8282\u70b9\u53ef\u4ee5\u6302\u5b50\u8282\u70b9\uff0c\u4e0d\u5141\u8bb8\u5b50\u8282\u70b9\u6302\u5b50\u8282\u70b9\\n5. \u6682\u65f6\u4e0d\u5141\u8bb8\u6539\u53d8 ElasticQuota \u7684 `quota.scheduling.koordinator.sh/is-parent`\u5c5e\u6027\\n\\n## \u8fdb\u4e00\u6b65\u5b8c\u5584 ElasticQuota Scheduling\\n\\n\u5728 Koordinator v0.7 \u7248\u672c\u4e2d\uff0ckoord-scheduler \u7684\u4e3b\u526f Pod \u90fd\u4f1a\u542f\u52a8 ElasticQuota Controller \u5e76\u90fd\u4f1a\u66f4\u65b0 ElasticQuota \u5bf9\u8c61\u3002\u5728 Koordinator v1.0 \u4e2d\u6211\u4eec\u4fee\u590d\u4e86\u8be5\u95ee\u9898\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b Pod \u53ef\u4ee5\u542f\u52a8 Controller \u5e76\u66f4\u65b0 ElasticQuota \u5bf9\u8c61\u3002 \u8fd8\u4f18\u5316\u4e86 ElasticQuota Controller \u6f5c\u5728\u7684\u9891\u7e41\u66f4\u65b0 ElasticQuota \u5bf9\u8c61\u7684\u95ee\u9898\uff0c\u5f53\u68c0\u67e5\u5230 ElasticQuota \u5404\u7ef4\u5ea6\u6570\u636e\u53d1\u751f\u53d8\u5316\u65f6\u624d\u4f1a\u66f4\u65b0\uff0c\u964d\u4f4e\u9891\u7e41\u66f4\u65b0\u7ed9 APIServer \u5e26\u6765\u7684\u538b\u529b\u3002\\n\\n## \u8fdb\u4e00\u6b65\u5b8c\u5584 Device Share Scheduling\\n\\nKoordinator v1.0 \u4e2d koordlet \u4f1a\u4e0a\u62a5 GPU \u7684\u578b\u53f7\u548c\u9a71\u52a8\u7248\u672c\u5230 Device CRD \u5bf9\u8c61\u4e2d\uff0c\u5e76\u4f1a\u7531 koord-manager \u540c\u6b65\u66f4\u65b0\u5230 Node \u5bf9\u8c61\uff0c\u8ffd\u52a0\u76f8\u5e94\u7684\u6807\u7b7e\u3002\\n\\n```yaml\\napiVersion: v1\\nkind: Node\\nmetadata:\\n  labels:\\n    kubernetes.io/gpu-driver: 460.91.03\\n    kubernetes.io/gpu-model: Tesla-T4\\n    ...\\n  name: cn-hangzhou.10.0.4.164\\nspec:\\n  ...\\nstatus:\\n  ...\\n```\\n\\n## Koordinator Runtime Proxy \u589e\u5f3a\u517c\u5bb9\u6027\\n\\n\u5728 Koordinator \u4e4b\u524d\u7684\u7248\u672c\u4e2d\uff0ckoord-runtime-proxy \u548c koordlet \u4e00\u8d77\u5b89\u88c5\u540e\uff0c\u5982\u679c koordlet \u5f02\u5e38\u6216\u8005 koordlet \u5378\u8f7d/\u91cd\u88c5\u7b49\u573a\u666f\u4e0b\uff0c\u4f1a\u9047\u5230\u65b0\u8c03\u5ea6\u5230\u8282\u70b9\u7684 Pod \u65e0\u6cd5\u521b\u5efa\u5bb9\u5668\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0ckoord-runtime-proxy \u4f1a\u611f\u77e5 Pod \u662f\u5426\u5177\u6709\u7279\u6b8a\u7684 label `runtimeproxy.koordinator.sh/skip-hookserver=true`\uff0c\u5982\u679c Pod \u5b58\u5728\u8be5\u6807\u7b7e\uff0ckoord-runtime-proxy \u4f1a\u76f4\u63a5\u628a CRI \u8bf7\u6c42\u8f6c\u53d1\u7ed9 containerd/docker \u7b49 runtime\u3002\\n\\n## \u5176\u4ed6\u6539\u52a8\\n\\n\u4f60\u53ef\u4ee5\u901a\u8fc7 [Github release](https://github.com/koordinator-sh/koordinator/releases/tag/v1.0.0) \u9875\u9762\uff0c\u6765\u67e5\u770b\u66f4\u591a\u7684\u6539\u52a8\u4ee5\u53ca\u5b83\u4eec\u7684\u4f5c\u8005\u4e0e\u63d0\u4ea4\u8bb0\u5f55\u3002"},{"id":"release-v0.7.0","metadata":{"permalink":"/zh-Hans/blog/release-v0.7.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-09-23-release/index.md","source":"@site/blog/2022-09-23-release/index.md","title":"Koordinator v0.7: \u4e3a\u4efb\u52a1\u8c03\u5ea6\u9886\u57df\u6ce8\u5165\u65b0\u6d3b\u529b","description":"Koordinator[1] \u7ee7\u4e0a\u6b21 v0.6\u7248\u672c[2] \u53d1\u5e03\u540e\uff0c\u7ecf\u8fc7 Koordinator \u793e\u533a\u7684\u52aa\u529b\uff0c\u6211\u4eec\u8fce\u6765\u4e86\u5177\u6709\u91cd\u5927\u610f\u4e49\u7684 v0.7 \u7248\u672c\u3002\u5728\u8fd9\u4e2a\u7248\u672c\u4e2d\u7740\u91cd\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u3001\u5927\u6570\u636e\u573a\u666f\u9700\u8981\u7684\u4efb\u52a1\u8c03\u5ea6\u80fd\u529b\uff0c\u4f8b\u5982 CoScheduling\u3001ElasticQuota\u548c\u7cbe\u7ec6\u5316\u7684 GPU \u5171\u4eab\u8c03\u5ea6\u80fd\u529b\u3002\u5e76\u5728\u8c03\u5ea6\u95ee\u9898\u8bca\u65ad\u5206\u6790\u65b9\u9762\u5f97\u5230\u4e86\u589e\u5f3a\uff0c\u91cd\u8c03\u5ea6\u5668\u4e5f\u6781\u5927\u7684\u63d0\u5347\u4e86\u5b89\u5168\u6027\uff0c\u964d\u4f4e\u4e86\u91cd\u8c03\u5ea6\u7684\u98ce\u9669\u3002","date":"2022-09-23T00:00:00.000Z","formattedDate":"2022\u5e749\u670823\u65e5","tags":[{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":33.82,"truncated":false,"authors":[{"name":"Joseph","title":"Koordinator maintainer","url":"https://github.com/eahydra","imageURL":"https://github.com/eahydra.png","key":"joseph"}],"frontMatter":{"slug":"release-v0.7.0","title":"Koordinator v0.7: \u4e3a\u4efb\u52a1\u8c03\u5ea6\u9886\u57df\u6ce8\u5165\u65b0\u6d3b\u529b","authors":["joseph"],"tags":["release"]},"prevItem":{"title":"Koordinator v1.0: \u6b63\u5f0f\u53d1\u5e03","permalink":"/zh-Hans/blog/release-v1.0.0"},"nextItem":{"title":"Koordinator v0.6: Complete fine-grained CPU orchestration, Resource Reservation and Descheduling","permalink":"/zh-Hans/blog/release-v0.6.0"}},"content":"[Koordinator[1]](https://koordinator.sh/) \u7ee7\u4e0a\u6b21 [v0.6\u7248\u672c[2]](https://mp.weixin.qq.com/s/YdoxVxz_91ZFemF8JuxRvQ) \u53d1\u5e03\u540e\uff0c\u7ecf\u8fc7 Koordinator \u793e\u533a\u7684\u52aa\u529b\uff0c\u6211\u4eec\u8fce\u6765\u4e86\u5177\u6709\u91cd\u5927\u610f\u4e49\u7684 v0.7 \u7248\u672c\u3002\u5728\u8fd9\u4e2a\u7248\u672c\u4e2d\u7740\u91cd\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u3001\u5927\u6570\u636e\u573a\u666f\u9700\u8981\u7684\u4efb\u52a1\u8c03\u5ea6\u80fd\u529b\uff0c\u4f8b\u5982 CoScheduling\u3001ElasticQuota\u548c\u7cbe\u7ec6\u5316\u7684 GPU \u5171\u4eab\u8c03\u5ea6\u80fd\u529b\u3002\u5e76\u5728\u8c03\u5ea6\u95ee\u9898\u8bca\u65ad\u5206\u6790\u65b9\u9762\u5f97\u5230\u4e86\u589e\u5f3a\uff0c\u91cd\u8c03\u5ea6\u5668\u4e5f\u6781\u5927\u7684\u63d0\u5347\u4e86\u5b89\u5168\u6027\uff0c\u964d\u4f4e\u4e86\u91cd\u8c03\u5ea6\u7684\u98ce\u9669\u3002\\n\\n# \u7248\u672c\u529f\u80fd\u7279\u6027\u89e3\u8bfb\\n\\n## 1. \u4efb\u52a1\u8c03\u5ea6\\n\\n### 1.1 Enhanced Coscheduling\\n\\nGang scheduling\u662f\u5728\u5e76\u53d1\u7cfb\u7edf\u4e2d\u5c06\u591a\u4e2a\u76f8\u5173\u8054\u7684\u8fdb\u7a0b\u8c03\u5ea6\u5230\u4e0d\u540c\u5904\u7406\u5668\u4e0a\u540c\u65f6\u8fd0\u884c\u7684\u7b56\u7565\uff0c\u5176\u6700\u4e3b\u8981\u7684\u539f\u5219\u662f\u4fdd\u8bc1\u6240\u6709\u76f8\u5173\u8054\u7684\u8fdb\u7a0b\u80fd\u591f\u540c\u65f6\u542f\u52a8\uff0c\u9632\u6b62\u90e8\u5206\u8fdb\u7a0b\u7684\u5f02\u5e38\uff0c\u5bfc\u81f4\u6574\u4e2a\u5173\u8054\u8fdb\u7a0b\u7ec4\u7684\u963b\u585e\u3002\u4f8b\u5982\u5f53\u63d0\u4ea4\u4e00\u4e2aJob\u65f6\u4f1a\u4ea7\u751f\u591a\u4e2a\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u671f\u671b\u8981\u4e48\u5168\u90e8\u8c03\u5ea6\u6210\u529f\uff0c\u8981\u4e48\u5168\u90e8\u5931\u8d25\u3002\u8fd9\u79cd\u9700\u6c42\u79f0\u4e3a All-or-Nothing\uff0c\u5bf9\u5e94\u7684\u5b9e\u73b0\u88ab\u79f0\u4f5c Gang Scheduling(or Coscheduling) \u3002<br />Koordinator \u5728\u542f\u52a8\u4e4b\u521d\uff0c\u671f\u671b\u652f\u6301 Kubernetes \u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6df7\u90e8\u8c03\u5ea6\uff0c\u63d0\u9ad8\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8fd0\u884c\u65f6\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5176\u4e2d\u5c31\u5305\u62ec\u4e86\u673a\u5668\u5b66\u4e60\u548c\u5927\u6570\u636e\u9886\u57df\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\u5177\u5907 All-or-Nothing \u9700\u6c42\u7684\u4f5c\u4e1a\u8d1f\u8f7d\u3002 \u4e3a\u4e86\u89e3\u51b3 All-or-Nothing \u8c03\u5ea6\u9700\u6c42\uff0cKoordinator v0.7.0 \u57fa\u4e8e\u793e\u533a\u5df2\u6709\u7684 Coscheduling \u5b9e\u73b0\u4e86 Enhanced Coscheduling\u3002<br />Enhanced Coscheduling \u79c9\u627f\u7740 Koordiantor \u517c\u5bb9\u793e\u533a\u7684\u539f\u5219\uff0c\u5b8c\u5168\u517c\u5bb9\u793e\u533a Coscheduling \u548c \u4f9d\u8d56\u7684 PodGroup CRD\u3002\u5df2\u7ecf\u4f7f\u7528 PodGroup \u7684\u7528\u6237\u53ef\u4ee5\u65e0\u7f1d\u5347\u7ea7\u5230 Koordinator\u3002<br />\u9664\u6b64\u4e4b\u5916\uff0cEnhanced Coscheduling \u8fd8\u5b9e\u73b0\u4e86\u5982\u4e0b\u589e\u5f3a\u80fd\u529b\uff1a\\n\\n#### \u652f\u6301 `Strict` \u548c `NonStrict` \u4e24\u79cd\u6a21\u5f0f\\n\\n\u4e24\u79cd\u6a21\u5f0f\u7684\u533a\u522b\u5728\u4e8e `Strict`\u6a21\u5f0f\uff08\u5373\u9ed8\u8ba4\u6a21\u5f0f\uff09\u4e0b\u8c03\u5ea6\u5931\u8d25\u4f1a Reject \u6240\u6709\u5206\u914d\u5230\u8d44\u6e90\u5e76\u5904\u4e8e Wait \u72b6\u6001\u7684 Pod\uff0c\u800c `NonStrict` \u6a21\u5f0f\u4e0d\u4f1a\u53d1\u8d77 Reject\u3002NonStrict \u6a21\u5f0f\u4e0b\uff0c\u540c\u5c5e\u4e8e\u4e00\u4e2a PodGroup \u7684 Pod A \u548c PodB \u8c03\u5ea6\u65f6\uff0c\u5982\u679c PodA \u8c03\u5ea6\u5931\u8d25\u4e0d\u4f1a\u5f71\u54cd PodB \u8c03\u5ea6\uff0c PodB \u8fd8\u4f1a\u7ee7\u7eed\u88ab\u8c03\u5ea6\u3002NonStrict \u6a21\u5f0f\u5bf9\u4e8e\u4f53\u91cf\u8f83\u5927\u7684 Job \u6bd4\u8f83\u53cb\u597d\uff0c\u53ef\u4ee5\u8ba9\u8fd9\u79cd\u5927\u4f53\u91cf Job \u66f4\u5feb\u7684\u8c03\u5ea6\u5b8c\u6210\uff0c\u4f46\u540c\u65f6\u4e5f\u589e\u52a0\u4e86\u8d44\u6e90\u6b7b\u9501\u7684\u98ce\u9669\u3002\u540e\u7eed Koordinator \u4f1a\u63d0\u4f9b NonStrict \u6a21\u5f0f\u4e0b\u89e3\u51b3\u6b7b\u9501\u7684\u65b9\u6848\u5b9e\u73b0\u3002 <br />\u7528\u6237\u5728\u4f7f\u7528\u65f6\uff0c\u53ef\u4ee5\u5728 PodGroup \u6216\u8005 Pod \u4e2d\u8ffd\u52a0 annotation `gang.scheduling.koordinator.sh/mode=NonStrict`\u5f00\u542f NonStrict \u6a21\u5f0f\u3002\\n\\n#### \u6539\u8fdb PodGroup \u8c03\u5ea6\u5931\u8d25\u7684\u5904\u7406\u673a\u5236\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u91cd\u8bd5\u8c03\u5ea6\\n\\n\u4e3e\u4e2a\u4f8b\u5b50\uff0cPodGroup A \u5173\u8054\u4e865\u4e2aPod\uff0c\u5176\u4e2d\u524d3\u4e2aPod\u901a\u8fc7Filter/Score\uff0c\u8fdb\u5165Wait\u9636\u6bb5\uff0c\u7b2c4\u4e2aPod\u8c03\u5ea6\u5931\u8d25\uff0c\u5f53\u8c03\u5ea6\u7b2c5\u4e2aPod\u65f6\uff0c\u53d1\u73b0\u7b2c4\u4e2aPod\u5df2\u7ecf\u5931\u8d25\uff0c\u5219\u62d2\u7edd\u8c03\u5ea6\u3002\u5728\u793e\u533a Coscheduling \u5b9e\u73b0\u4e2d\uff0c\u8c03\u5ea6\u5931\u8d25\u7684PodGroup \u4f1a\u52a0\u5165\u5230\u57fa\u4e8ecache\u673a\u5236\u7684 lastDeniedPG  \u5bf9\u8c61\u4e2d\uff0c\u5f53 cache \u6ca1\u6709\u8fc7\u671f\uff0c\u5219\u4f1a\u62d2\u7edd\u8c03\u5ea6\uff1b\u5982\u679c\u8fc7\u671f\u5c31\u5141\u8bb8\u7ee7\u7eed\u8c03\u5ea6\u3002\u53ef\u4ee5\u770b\u5230 cache \u7684\u8fc7\u671f\u65f6\u95f4\u5f88\u5173\u952e\uff0c\u8fc7\u671f\u65f6\u95f4\u8bbe\u7f6e\u7684\u8fc7\u957f\u4f1a\u5bfc\u81f4Pod\u8fdf\u8fdf\u5f97\u4e0d\u5230\u8c03\u5ea6\u673a\u4f1a\uff0c\u8bbe\u7f6e\u7684\u8fc7\u77ed\u4f1a\u51fa\u73b0\u9891\u7e41\u7684\u65e0\u6548\u8c03\u5ea6\u3002<br />\u800c\u5728Enhanced Coscheduling \u4e2d\uff0c\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e ScheduleCycle \u7684\u91cd\u8bd5\u673a\u5236\u3002\u4ee5\u4e0a\u573a\u666f\u4e3a\u4f8b\uff0c5\u4e2aPod\u7684 ScheduleCycle \u521d\u59cb\u503c\u4e3a 0\uff0cPodGroup \u5bf9\u5e94\u7684 ScheduleCycle \u521d\u59cb\u503c\u4e3a1\uff1b\u5f53\u6bcf\u4e00\u6b21\u5c1d\u8bd5\u8c03\u5ea6 Pod \u65f6\uff0c\u90fd\u4f1a\u66f4\u65b0 Pod ScheduleCycle \u4e3a PodGroup ScheduleCycle\u3002\u5982\u679c\u5176\u4e2d\u4e00\u4e2a Pod \u8c03\u5ea6\u5931\u8d25\uff0c\u4f1a\u6807\u8bb0\u5f53\u524d\u7684 PodGroup ScheduleCycle \u65e0\u6548\uff0c\u4e4b\u540e\u6240\u6709\u5c0f\u4e8e PodGroup ScheduleCycle \u7684 Pod \u90fd\u4f1a\u88ab\u62d2\u7edd\u8c03\u5ea6\u3002\u5f53\u540c\u4e00\u4e2a PodGroup \u4e0b\u7684\u6240\u6709 Pod \u90fd\u5c1d\u8bd5\u8c03\u5ea6\u4e00\u8f6e\u540e\uff0cPod ScheduleCycle \u90fd\u66f4\u65b0\u4e3a\u5f53\u524d PodGroup ScheduleCycle\uff0c\u5e76\u9012\u8fdb PodGroup ScheduleCycle\uff0c\u5e76\u6807\u8bb0\u5141\u8bb8\u8c03\u5ea6\u3002\u8fd9\u79cd\u65b9\u5f0f\u53ef\u4ee5\u6709\u6548\u89c4\u907f\u57fa\u4e8e\u8fc7\u671f\u65f6\u95f4\u7684\u7f3a\u9677\uff0c\u5b8c\u5168\u53d6\u51b3\u4e8e\u8c03\u5ea6\u961f\u5217\u7684\u914d\u7f6e\u91cd\u8bd5\u8c03\u5ea6\u3002<br />![image.png](../../static/img/gang-schedulue-cycle.png \\"\u57fa\u4e8e ScheduleCycle \u7684\u91cd\u8bd5\u673a\u5236\\")\\n\\n#### \u652f\u6301\u591a\u4e2a PodGroup \u4e3a\u4e00\u7ec4\u5b8c\u6210 Gang Scheduling\\n\\n\u4e00\u4e9b\u590d\u6742\u7684 Job \u6709\u591a\u79cd\u89d2\u8272\uff0c\u6bcf\u4e2a\u89d2\u8272\u7ba1\u7406\u4e00\u6279\u4efb\u52a1\uff0c\u6bcf\u4e2a\u89d2\u8272\u7684\u4efb\u52a1\u8981\u6c42\u652f\u6301 All-or-Nothing \uff0c\u6bcf\u4e2a\u89d2\u8272\u7684 MinMember \u8981\u6c42\u4e5f\u4e0d\u4e00\u6837\uff0c\u5e76\u4e14\u6bcf\u4e2a\u89d2\u8272\u4e4b\u95f4\u4e5f\u8981\u6c42 All-or-Nothing\u3002\u8fd9\u5c31\u5bfc\u81f4\u6bcf\u4e2a\u89d2\u8272\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684 PodGroup \uff0c\u5e76\u4e14\u8fd8\u8981\u6c42 PodGroup \u5373\u4f7f\u6ee1\u8db3\u4e86\u4e5f\u9700\u8981\u7b49\u5f85\u5176\u4ed6\u89d2\u8272\u7684 PodGroup \u5fc5\u987b\u6ee1\u8db3\u3002\u793e\u533a Coscheduling \u65e0\u6cd5\u6ee1\u8db3\u8fd9\u79cd\u573a\u666f\u9700\u6c42\u3002\u800c Koordinator \u5b9e\u73b0\u7684 Enhanced Coscheduling \u652f\u6301\u7528\u6237\u5728\u591a\u4e2a PodGroup \u4e2d\u589e\u52a0 anntation \u76f8\u5173\u5173\u8054\u5b9e\u73b0\uff0c\u5e76\u652f\u6301\u8de8Namespace\u3002\u4f8b\u5982\u7528\u6237\u67092\u4e2aPodGroup \uff0c\u540d\u5b57\u5206\u522b\u662fPodGroupA\u548cPodGroupB\uff0c\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u4f8b\u5b50\u5173\u8054\u4e24\u4e2a PodGroup\uff1a\\n```yaml\\napiVersion: v1alpha1\\nkind: PodGroup\\nmetadata:\\n  name: podGroupA\\n  namespace: default\\n  annotations:\\n    gang.scheduling.koordinator.sh/groups: [\\"namespaceA/podGroupA\\", \\"namespaceB/podGroupB\\"]\\nspec:\\n\\t...\\n```\\n\\n#### \u652f\u6301\u8f7b\u91cf\u5316 Gang \u534f\u8bae\\n\\n\u5982\u679c\u7528\u6237\u4e0d\u5e0c\u671b\u521b\u5efa PodGroup\uff0c\u8ba4\u4e3a\u521b\u5efa PodGroup \u592a\u7e41\u7410\uff0c\u90a3\u4e48\u53ef\u4ee5\u8003\u8651\u5728\u4e00\u7ec4 Pod \u4e2d\u586b\u5145\u76f8\u540c annotation  `gang.scheduling.koordinator.sh/name=<podGroupName>` \u8868\u793a\u8fd9\u4e00\u7ec4 Pod \u4f7f\u7528 Coscheduling \u8c03\u5ea6\u3002\u5982\u679c\u671f\u671b\u8bbe\u7f6e minMember \uff0c\u53ef\u4ee5\u8ffd\u52a0 Annotation `gang.scheduling.koordinator.sh/min-available=<availableNum>`\u3002\u4e3e\u4e2a\u4f8b\u5b50\uff1a\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  annotations:\\n    gang.scheduling.koordinator.sh/name: \\"pod-group-a\\"\\n    gang.scheduling.koordinator.sh/min-available: \\"5\\"\\n  name: demo-pod\\n  namespace: default\\nspec:\\n\\t...\\n```\\n\\n### 1.2 ElasticQuota Scheduling\\n\\n\u4e00\u5bb6\u4e2d\u5927\u578b\u516c\u53f8\u5185\u6709\u591a\u4e2a\u4ea7\u54c1\u548c\u7814\u53d1\u56e2\u961f\uff0c\u5171\u7528\u591a\u4e2a\u6bd4\u8f83\u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\uff0c\u8fd9\u4e9b\u96c6\u7fa4\u5185\u542b\u6709\u7684\u5927\u91cf CPU/Memory/Disk \u7b49\u8d44\u6e90\u88ab\u8d44\u6e90\u8fd0\u8425\u56e2\u961f\u7edf\u4e00\u7ba1\u7406\u3002\u8fd0\u8425\u56e2\u961f\u5f80\u5f80\u5728\u91c7\u8d2d\u8d44\u6e90\u524d\uff0c\u901a\u8fc7\u989d\u5ea6\u9884\u7b97\u7684\u673a\u5236\u8ba9\u516c\u53f8\u5185\u6bcf\u4e2a\u56e2\u961f\u6839\u636e\u81ea\u8eab\u7684\u9700\u6c42\u63d0\u4ea4\u989d\u5ea6\u9884\u7b97\u3002\u4e1a\u52a1\u56e2\u961f\u6b64\u65f6\u4e00\u822c\u6839\u636e\u4e1a\u52a1\u5f53\u524d\u548c\u5bf9\u672a\u6765\u7684\u9884\u671f\u505a\u597d\u989d\u5ea6\u9884\u7b97\u3002\u6700\u7406\u60f3\u7684\u60c5\u51b5\u662f\u6bcf\u4e00\u4efd\u989d\u5ea6\u90fd\u80fd\u591f\u88ab\u4f7f\u7528\uff0c\u4f46\u73b0\u5b9e\u544a\u8bc9\u6211\u4eec\u8fd9\u662f\u4e0d\u73b0\u5b9e\u7684\u3002\u5f80\u5f80\u51fa\u73b0\u7684\u95ee\u9898\u662f\uff1a\\n\\n1. \u56e2\u961f A \u9ad8\u4f30\u4e86\u4e1a\u52a1\u7684\u53d1\u5c55\u901f\u5ea6\uff0c\u7533\u8bf7\u4e86\u592a\u591a\u7684\u989d\u5ea6\u7528\u4e0d\u5b8c\\n2. \u56e2\u961f B \u4f4e\u4f30\u4e86\u4e1a\u52a1\u7684\u53d1\u5c55\u901f\u5ea6\uff0c\u7533\u8bf7\u7684\u989d\u5ea6\u4e0d\u591f\u7528\\n3. \u56e2\u961f C \u5b89\u6392\u4e86\u4e00\u573a\u6d3b\u52a8\uff0c\u624b\u4e0a\u7684\u989d\u5ea6\u4e0d\u591f\u591a\u4e86\uff0c\u4f46\u662f\u6d3b\u52a8\u53ea\u6301\u7eed\u51e0\u5468\uff0c\u7533\u8bf7\u592a\u591a\u989d\u5ea6\u548c\u8d44\u6e90\u4e5f\u4f1a\u6d6a\u8d39\u6389\u3002\\n4. \u56e2\u961f D \u4e0b\u9762\u8fd8\u6709\u5404\u4e2a\u5b50\u56e2\u961f\u548c\u4e1a\u52a1\uff0c\u6bcf\u4e2a\u5b50\u56e2\u961f\u5185\u4e5f\u4f1a\u51fa\u73b0\u7c7b\u4f3cA B C \u4e09\u4e2a\u56e2\u961f\u7684\u60c5\u51b5\uff0c\u800c\u4e14\u5176\u4e2d\u6709\u4e9b\u56e2\u961f\u7684\u4e1a\u52a1\u4e34\u65f6\u7a81\u53d1\u9700\u8981\u63d0\u4ea4\u4e00\u4e9b\u8ba1\u7b97\u4efb\u52a1\u8981\u4ea4\u4e2a\u5ba2\u6237\uff0c\u4f46\u662f\u6ca1\u6709\u989d\u5ea6\u4e86\uff0c\u8d70\u989d\u5ea6\u9884\u7b97\u5ba1\u6279\u4e5f\u4e0d\u591f\u4e86\u3002\\n5. ......\\n\\n\u4ee5\u4e0a\u5927\u5bb6\u65e5\u5e38\u7ecf\u5e38\u9047\u5230\u7684\u573a\u666f\uff0c\u5728\u6df7\u90e8\u573a\u666f\u3001\u5927\u6570\u636e\u573a\u666f\uff0c\u4e34\u65f6\u6027\u7a81\u53d1\u9700\u6c42\u53c8\u662f\u65f6\u5e38\u51fa\u73b0\u7684\uff0c\u8fd9\u4e9b\u8d44\u6e90\u7684\u9700\u6c42\u90fd\u7ed9\u989d\u5ea6\u7ba1\u7406\u5de5\u4f5c\u5e26\u6765\u4e86\u6781\u5927\u7684\u6311\u6218\u3002\u505a\u597d\u989d\u5ea6\u7ba1\u7406\u5de5\u4f5c\uff0c\u4e00\u65b9\u9762\u907f\u514d\u8fc7\u5ea6\u91c7\u8d2d\u8d44\u6e90\u964d\u4f4e\u6210\u672c\uff0c\u53c8\u8981\u5728\u4e34\u65f6\u9700\u8981\u989d\u5ea6\u65f6\u4e0d\u91c7\u8d2d\u8d44\u6e90\u6216\u8005\u5c3d\u91cf\u5c11\u7684\u91c7\u8d2d\u8d44\u6e90\uff1b\u53e6\u4e00\u65b9\u9762\u4e0d\u80fd\u56e0\u4e3a\u989d\u5ea6\u95ee\u9898\u9650\u5236\u8d44\u6e90\u4f7f\u7528\u7387\uff0c\u989d\u5ea6\u7ba1\u7406\u4e0d\u597d\u5c31\u4f1a\u5bfc\u81f4\u5373\u4f7f\u6709\u6bd4\u8f83\u597d\u7684\u6280\u672f\u5e2e\u52a9\u590d\u7528\u8d44\u6e90\uff0c\u4e5f\u65e0\u6cd5\u53d1\u6325\u5176\u4ef7\u503c\u3002 \u603b\u4e4b\uff0c\u989d\u5ea6\u7ba1\u7406\u5de5\u4f5c\u662f\u5e7f\u5927\u516c\u53f8\u6216\u7ec4\u7ec7\u9700\u957f\u671f\u9762\u5bf9\u4e14\u5fc5\u987b\u9762\u5bf9\u7684\u95ee\u9898\u3002<br />Kubernetes ResourceQuota \u53ef\u4ee5\u89e3\u51b3\u989d\u5ea6\u7ba1\u7406\u7684\u90e8\u5206\u95ee\u9898\u3002 \u539f\u751f Kubernetes ResourceQuota API \u7528\u4e8e\u6307\u5b9a\u6bcf\u4e2a Namespace \u7684\u6700\u5927\u8d44\u6e90\u989d\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7 admission \u673a\u5236\u5b8c\u6210\u51c6\u5165\u68c0\u67e5\u3002\u5982\u679c Namespace \u5f53\u524d\u8d44\u6e90\u5206\u914d\u603b\u91cf\u8d85\u8fc7ResourceQuota \u6307\u5b9a\u7684\u914d\u989d\uff0c\u5219\u62d2\u7edd\u521b\u5efa Pod\u3002 Kubernetes ResourceQuota \u8bbe\u8ba1\u6709\u4e00\u4e2a\u5c40\u9650\u6027\uff1aQuota  \u7528\u91cf\u662f\u6309\u7167 Pod Requests \u805a\u5408\u7684\u3002 \u867d\u7136\u8fd9\u79cd\u673a\u5236\u53ef\u4ee5\u4fdd\u8bc1\u5b9e\u9645\u7684\u8d44\u6e90\u6d88\u8017\u6c38\u8fdc\u4e0d\u4f1a\u8d85\u8fc7 ResourceQuota \u7684\u9650\u5236\uff0c\u4f46\u5b83\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\uff0c\u56e0\u4e3a\u4e00\u4e9b Pod \u53ef\u80fd\u5df2\u7ecf\u7533\u8bf7\u4e86\u8d44\u6e90\u4f46\u672a\u80fd\u8c03\u5ea6\u3002 <br />Kuberenetes Scheduler-Sig \u540e\u6765\u7ed9\u51fa\u4e86\u4e00\u4e2a\u501f\u9274 Yarn Capacity Scheduling\uff0c\u79f0\u4f5c ElasticQuota \u7684\u8bbe\u8ba1\u65b9\u6848\u5e76\u7ed9\u51fa\u4e86\u5177\u4f53\u7684\u5b9e\u73b0\u3002\u5141\u8bb8\u7528\u6237\u8bbe\u7f6e max \u548c min\uff1a\\n\\n- max \u8868\u793a\u7528\u6237\u53ef\u4ee5\u6d88\u8d39\u7684\u8d44\u6e90\u4e0a\u9650\\n- min \u8868\u793a\u9700\u8981\u4fdd\u969c\u7528\u6237\u5b9e\u73b0\u57fa\u672c\u529f\u80fd/\u6027\u80fd\u6240\u9700\u8981\u7684\u6700\u5c0f\u8d44\u6e90\u91cf\\n\\n\u901a\u8fc7\u8fd9\u4e24\u4e2a\u53c2\u6570\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u5b9e\u73b0\u5982\u4e0b\u7684\u9700\u6c42\uff1a\\n\\n1. \u7528\u6237\u8bbe\u7f6e min < max \u65f6\uff0c\u5f53\u6709\u7a81\u53d1\u8d44\u6e90\u9700\u6c42\u65f6\uff0c\u5373\u4f7f\u5f53\u524d ElasticQuota \u7684\u603b\u7528\u91cf\u8d85\u8fc7\u4e86 min\uff0c \u4f46\u53ea\u8981\u6ca1\u6709\u8fbe\u5230 max\uff0c\u90a3\u4e48\u7528\u6237\u53ef\u4ee5\u7ee7\u7eed\u521b\u5efa\u65b0\u7684 Pod \u5e94\u5bf9\u65b0\u7684\u4efb\u52a1\u8bf7\u6c42\u3002\\n2. \u5f53\u7528\u6237\u9700\u8981\u66f4\u591a\u8d44\u6e90\u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u4ece\u5176\u4ed6 ElasticQuota \u4e2d\u201c\u501f\u7528(borrow)\u201d \u8fd8\u6ca1\u6709\u88ab\u4f7f\u7528\u5e76\u4e14\u9700\u8981\u901a\u4fdd\u969c\u7684 min\u3002\\n3. \u5f53\u4e00\u4e2a ElasticQuota \u9700\u8981\u4f7f\u7528 min \u8d44\u6e90\u65f6\uff0c\u4f1a\u901a\u8fc7\u62a2\u5360\u673a\u5236\u4ece\u5176\u4ed6\u501f\u7528\u65b9\u62a2\u56de\u6765\uff0c\u5373\u9a71\u9010\u4e00\u4e9b\u5176\u4ed6ElasticQuota \u8d85\u8fc7 min \u7528\u91cf\u7684 Pod\u3002\\n\\nElasticQuota \u8fd8\u6709\u4e00\u4e9b\u5c40\u9650\u6027\uff1a\u6ca1\u6709\u5f88\u597d\u7684\u4fdd\u969c\u516c\u5e73\u6027\u3002\u5047\u5982\u540c\u4e00\u4e2a ElasticQuota \u6709\u5927\u91cf\u65b0\u5efa\u7684Pod\uff0c\u6709\u53ef\u80fd\u4f1a\u6d88\u8017\u6240\u6709\u5176\u4ed6\u53ef\u4ee5\u88ab\u501f\u7528\u7684Quota\uff0c\u4ece\u800c\u5bfc\u81f4\u540e\u6765\u7684 Pod \u53ef\u80fd\u62ff\u4e0d\u5230 Quota\u3002\u6b64\u65f6\u53ea\u80fd\u901a\u8fc7\u62a2\u5360\u673a\u5236\u62a2\u56de\u6765\u4e00\u4e9b Quota\u3002<br />\u53e6\u5916 ElasticQuota \u548c Kubernetes ResourceQuota \u90fd\u662f\u9762\u5411 Namespace\u7684\uff0c\u4e0d\u652f\u6301\u591a\u7ea7\u6811\u5f62\u7ed3\u6784\uff0c\u5bf9\u4e8e\u4e00\u4e9b\u672c\u8eab\u5177\u5907\u590d\u6742\u7ec4\u7ec7\u5173\u7cfb\u7684\u4f01\u4e1a/\u7ec4\u7ec7\u4e0d\u80fd\u5f88\u597d\u7684\u4f7f\u7528ElasticQuota/Kubenretes ResourceQuota \u5b8c\u6210\u989d\u5ea6\u7ba1\u7406\u5de5\u4f5c\u3002<br />Koordinator \u9488\u5bf9\u8fd9\u4e9b\u989d\u5ea6\u7ba1\u7406\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793e\u533a ElasticQuota \u5b9e\u73b0\u7684\u652f\u6301\u591a\u7ea7\u7ba1\u7406\u65b9\u5f0f\u7684\u5f39\u6027Quota\u7ba1\u7406\u673a\u5236(multi hierarchy quota management)\u3002\u5177\u5907\u5982\u4e0b\u7279\u6027\uff1a\\n\\n- \u517c\u5bb9\u793e\u533a\u7684 ElasticQuota API\u3002\u7528\u6237\u53ef\u4ee5\u65e0\u7f1d\u5347\u7ea7\u5230 Koordinator\\n- \u652f\u6301\u6811\u5f62\u7ed3\u6784\u7ba1\u7406 Quota\u3002\\n- \u652f\u6301\u6309\u7167\u5171\u4eab\u6743\u91cd(shared weight)\u4fdd\u969c\u516c\u5e73\u6027\u3002\\n- \u5141\u8bb8\u7528\u6237\u8bbe\u7f6e\u662f\u5426\u5141\u8bb8\u501f\u7528Quota \u7ed9\u5176\u4ed6\u6d88\u8d39\u5bf9\u8c61\u3002\\n\\n#### Pod \u5173\u8054 ElasticQuota \u65b9\u5f0f\\n\\n\u7528\u6237\u53ef\u4ee5\u975e\u5e38\u4f7f\u7528\u7684\u4f7f\u7528\u8be5\u80fd\u529b\uff0c\u53ef\u4ee5\u5b8c\u5168\u6309\u7167 ElasticQuota \u7684\u7528\u6cd5\uff0c\u5373\u6bcf\u4e2a Namespace \u8bbe\u7f6e\u4e00\u4e2a ElasticQuota \u5bf9\u8c61\u3002\u4e5f\u53ef\u4ee5\u5728 Pod \u4e2d\u8ffd\u52a0 Label \u5173\u8054 ElasticQuota\uff1a\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  labels:\\n    quota.scheduling.koordinator.sh/name: \\"elastic-quota-a\\"\\n  name: demo-pod\\n  namespace: default\\nspec:\\n\\t...\\n```\\n#### \u6811\u5f62\u7ed3\u6784\u7ba1\u7406\u673a\u5236\u548c\u4f7f\u7528\u65b9\u6cd5\\n\\n\u9700\u8981\u4f7f\u7528\u6811\u5f62\u7ed3\u6784\u7ba1\u7406 Quota \u65f6\uff0c\u9700\u8981\u5728 ElasticQuota \u4e2d\u8ffd\u52a0 Label  `quota.scheduling.koordinator.sh/is-parent`\u8868\u793a\u5f53\u524d ElasticQuota \u662f\u5426\u662f\u7236\u8282\u70b9\uff0c`quota.scheduling.koordinator.sh/parent`\u8868\u793a\u5f53\u524d ElasticQuota \u7684\u7236\u8282\u70b9 ElasticQuota \u7684\u540d\u5b57\u3002\u4e3e\u4e2a\u4f8b\u5b50\uff1a<br />![image.png](../../static/img/quota-tree.png)<br />\u6211\u4eec\u521b\u5efa\u4e00\u4e2a ElasticQuota Root \u4f5c\u4e3a\u6839\u8282\u70b9\uff0c\u8d44\u6e90\u603b\u91cf\u4e3aCPU 100C\uff0c\u5185\u5b58200Gi\uff0c\u4ee5\u53ca\u5b50\u8282\u70b9 quota-a\\n```yaml\\napiVersion: scheduling.sigs.k8s.io/v1alpha1\\nkind: ElasticQuota\\nmetadata:\\n  name: parentA\\n  namespace: default\\n  labels:\\n    quota.scheduling.koordinator.sh/is-parent: \\"true\\"\\n    quota.scheduling.koordinator.sh/allow-lent-resource: \\"true\\"\\nspec:\\n  max:\\n    cpu: 100\\n    memory: 200Gi\\n  min:\\n    cpu: 100\\n    memory: 200Gi\\n---\\napiVersion: scheduling.sigs.k8s.io/v1alpha1\\nkind: ElasticQuota\\nmetadata:\\n  name: childA1\\n  namespace: default\\n  labels:\\n    quota.scheduling.koordinator.sh/is-parent: \\"false\\"\\n    quota.scheduling.koordinator.sh/parent: \\"parentA\\"\\n    quota.scheduling.koordinator.sh/allow-lent-resource: \\"true\\"\\nspec:\\n  max:\\n    cpu: 40\\n    memory: 100Gi\\n  min:\\n    cpu: 20\\n    memory: 40Gi\\n```\\n\\n\u5728\u4f7f\u7528\u6811\u5f62\u7ed3\u6784\u7ba1\u7406 ElasticQuota \u65f6\uff0c\u6709\u4e00\u4e9b\u9700\u8981\u9075\u5faa\u7684\u7ea6\u675f\uff1a\\n\\n1. \u9664\u4e86\u6839\u8282\u70b9\uff0c\u5176\u4ed6\u6240\u6709\u5b50\u8282\u70b9\u7684 min \u4e4b\u548c\u8981\u5c0f\u4e8e\u7236\u8282\u70b9\u7684 min\u3002\\n2. \u4e0d\u9650\u5236\u5b50\u8282\u70b9 max\uff0c\u5141\u8bb8\u5b50\u8282\u70b9\u7684 max \u5927\u4e8e\u7236\u8282\u70b9\u7684 max\u3002\u8003\u8651\u4ee5\u4e0b\u573a\u666f\uff0c\u96c6\u7fa4\u4e2d\u6709 2 \u4e2a ElasticQuota \u5b50\u6811\uff1adev-parent \u548c production-parent\uff0c\u6bcf\u4e2a\u5b50\u6811\u90fd\u6709\u51e0\u4e2a\u5b50 ElasticQuota\u3002 \u5f53 production-parent \u5fd9\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u53ea\u964d\u4f4e dev-parent \u7684 max \u9650\u5236  dev-parent \u6574\u9897\u5b50\u6811\u7684\u8d44\u6e90\u4f7f\u7528\u91cf\uff0c\u800c\u4e0d\u662f\u964d\u4f4e dev-parent \u5b50\u6811\u7684\u6bcf\u4e2a\u5b50 ElasticQuota \u7684max\u9650\u5236\u7528\u91cf\u3002\\n3. Pod \u4e0d\u80fd\u4f7f\u7528\u7236\u8282\u70b9ElasticQuota\u3002\u5982\u679c\u653e\u5f00\u8fd9\u4e2a\u9650\u5236\uff0c\u4f1a\u5bfc\u81f4\u6574\u4e2a\u5f39\u6027 Quota \u7684\u673a\u5236\u53d8\u7684\u5f02\u5e38\u590d\u6742\uff0c\u6682\u65f6\u4e0d\u8003\u8651\u652f\u6301\u8fd9\u79cd\u573a\u666f\u3002\\n4. \u53ea\u6709\u7236\u8282\u70b9\u53ef\u4ee5\u6302\u5b50\u8282\u70b9\uff0c\u4e0d\u5141\u8bb8\u5b50\u8282\u70b9\u6302\u5b50\u8282\u70b9\\n5. \u6682\u65f6\u4e0d\u5141\u8bb8\u6539\u53d8 ElasticQuota \u7684 `quota.scheduling.koordinator.sh/is-parent`\u5c5e\u6027\\n\\n\u6211\u4eec\u5c06\u5728\u4e0b\u4e2a\u7248\u672c\u4e2d\u901a\u8fc7 webhook \u673a\u5236\u5b9e\u73b0\u8fd9\u4e9b\u7ea6\u675f\u3002\\n\\n#### \u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\\n\\n\u4e3a\u4e86\u65b9\u4fbf\u9605\u8bfb\u548c\u7406\u89e3\u5c06\u8981\u4ecb\u7ecd\u7684\u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\uff0c\u5148\u660e\u786e\u51e0\u4e2a\u65b0\u6982\u5ff5\uff1a\\n\\n- request \u8868\u793a\u540c\u4e00\u4e2a ElasticQuota \u5173\u8054\u7684\u6240\u6709 Pod \u7684\u8d44\u6e90\u8bf7\u6c42\u91cf\u3002\u5982\u679c\u4e00\u4e2a ElasticQuota A \u7684 request \u5c0f\u4e8e min\uff0cElasticQuota B \u7684 request \u5927\u4e8e min\uff0c\u90a3\u4e48 ElasticQuota A \u672a\u4f7f\u7528\u7684\u90e8\u5206\uff0c\u5373 min - request \u5269\u4f59\u7684\u91cf\u901a\u8fc7\u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\u501f\u7528\u7ed9 ElasticQuota B. \u5f53 ElasticQuota A \u9700\u8981\u4f7f\u7528\u8fd9\u4e9b\u501f\u8d70\u7684\u91cf\u65f6\uff0c\u8981\u6c42 ElasticQuota B \u4f9d\u636e\u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\u5f52\u8fd8\u7ed9 ElasticQuota A\u3002\\n- runtime \u8868\u793a ElasticQuota \u5f53\u524d\u53ef\u4ee5\u4f7f\u7528\u7684\u5b9e\u9645\u8d44\u6e90\u91cf\u3002\u5982\u679c request \u5c0f\u4e8e min\uff0cruntime \u7b49\u4e8e request\u3002\u8fd9\u4e5f\u610f\u5473\u7740\uff0c\u9700\u8981\u9075\u5faa min \u8bed\u4e49\uff0c\u5e94\u65e0\u6761\u4ef6\u6ee1\u8db3 request\u3002\u5982\u679c request \u5927\u4e8e min\uff0c\u4e14 min \u5c0f\u4e8e max\uff0c\u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\u4f1a\u5206\u914d runtime \u5728min \u4e0e max \u4e4b\u524d\uff0c\u5373 max >= runtime >= min\u3002\\n- shared-weight \u8868\u793a\u4e00\u4e2a ElasticQuota \u7684\u7ade\u4e89\u529b\uff0c\u9ed8\u8ba4\u7b49\u4e8e ElasticQuota Max\u3002\\n\\n\u901a\u8fc7\u51e0\u4e2a\u4f8b\u5b50\u4e3a\u5927\u5bb6\u4ecb\u7ecd\u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\u7684\u8fd0\u884c\u8fc7\u7a0b\uff0c\u5047\u8bbe\u5f53\u524d\u96c6\u7fa4\u7684 CPU \u603b\u91cf\u4e3a100C\uff0c\u5e76\u4e14\u67094\u4e2aElasticQuota\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7eff\u8272\u90e8\u5206\u4e3a Request \u91cf\uff1aA \u5f53\u524d\u7684request \u4e3a5\uff0cB\u5f53\u524d\u7684request\u4e3a20\uff0cC\u5f53\u524d\u7684Request\u4e3a30\uff0cD\u5f53\u524d\u7684Request\u4e3a70\u3002<br />![image.png](../../static/img/quota-init-example.png)<br />\u5e76\u4e14\u6211\u4eec\u6ce8\u610f\u5230\uff0c A, B, C, D \u7684 min \u4e4b\u548c\u662f60\uff0c\u5269\u4e0b 40 \u4e2a\u7a7a\u95f2\u989d\u5ea6\uff0c \u540c\u65f6 A \u8fd8\u53ef\u4ee5\u501f\u7ed9 B, C, D 5\u4e2a\u989d\u5ea6\uff0c\u6240\u4ee5\u4e00\u5171\u670945\u4e2a\u989d\u5ea6\u88abB\uff0cC\uff0cD\u5171\u4eab\uff0c\u6839\u636e\u5404\u4e2aElasticQuota\u7684 shared-weight\uff0cB\uff0cC\uff0cD\u5206\u522b\u5bf9\u5e9460\uff0c50\u548c80\uff0c\u8ba1\u7b97\u51fa\u5404\u81ea\u53ef\u4ee5\u5171\u4eab\u7684\u91cf\uff1a\\n\\n- B \u53ef\u4ee5\u83b7\u53d6 14\u4e2a\u989d\u5ea6\uff0c 45 * 60 / (60 + 50 + 80) = 14\\n- C \u53ef\u4ee5\u83b7\u53d6 12\u4e2a\u989d\u5ea6\uff0c 45 * 50 / (60 + 50 + 80) = 12\\n- D \u53ef\u4ee5\u83b7\u53d6 19\u4e2a\u989d\u5ea6\uff0c 45 * 80 / (60 + 50 + 80) = 19\\n\\n![image.png](../../static/img/quota-init-runtime-example.png)<br />\u4f46\u6211\u4eec\u4e5f\u8981\u6ce8\u610f\u7684\u662f\uff0cC\u548cD\u9700\u8981\u66f4\u591a\u989d\u5ea6\uff0c\u800c B\u53ea\u9700\u89815\u4e2a\u989d\u5ea6\u5c31\u80fd\u6ee1\u8db3 Request\uff0c\u5e76\u4e14 B \u7684min\u662f15\uff0c\u4e5f\u5c31\u610f\u5473\u7740\u6211\u4eec\u53ea\u9700\u8981\u7ed9 B 5\u4e2a\u989d\u5ea6\uff0c\u5269\u4f59\u76849\u4e2a\u989d\u5ea6\u7ee7\u7eed\u5206\u7ed9C\u548cD\u3002\\n\\n- C \u53ef\u4ee5\u83b7\u53d6 3\u4e2a\u989d\u5ea6\uff0c 9 * 50 / (50 + 80) = 3\\n- D \u53ef\u4ee5\u83b7\u53d6 6\u4e2a\u989d\u5ea6\uff0c  9 * 80 / (50 + 80) = 6\\n\\n[![](https://github.com/koordinator-sh/koordinator/raw/main/docs/images/runtimequota3.jpg#crop=0&crop=0&crop=1&crop=1&from=url&id=XJyFI&margin=%5Bobject%20Object%5D&originHeight=782&originWidth=1570&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)](https://github.com/koordinator-sh/koordinator/blob/main/docs/images/runtimequota3.jpg)<br />\u6700\u7ec8\u6211\u4eec\u5f97\u51fa\u5982\u4e0b\u7684\u5206\u914d\u7ed3\u679c\u7ed3\u679c\uff1a\\n\\n- A runtime = 5\\n- B runtime = 20\\n- C runtime = 35\\n- D runtime = 40\\n\\n[![](https://github.com/koordinator-sh/koordinator/raw/main/docs/images/runtimequota4.jpg#crop=0&crop=0&crop=1&crop=1&from=url&id=J8tN9&margin=%5Bobject%20Object%5D&originHeight=778&originWidth=1560&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)](https://github.com/koordinator-sh/koordinator/blob/main/docs/images/runtimequota4.jpg)<br />\u603b\u7ed3\u6574\u4e2a\u8fc7\u7a0b\u53ef\u4ee5\u77e5\u9053\uff1a\\n\\n1. \u5f53\u524d request < min \u65f6\uff0c\u9700\u8981\u501f\u51fa lent-to-quotas\uff1b\u5f53 request > min \u65f6\uff0c\u9700\u8981\u501f\u5165 borrowed-qutoas\\n2. \u7edf\u8ba1\u6240\u6709 runtime < min \u7684 Quota\uff0c\u8fd9\u4e9b\u603b\u91cf\u5c31\u662f\u63a5\u4e0b\u6765\u53ef\u88ab\u501f\u51fa\u7684\u91cf\u3002\\n3. \u6839\u636e shared-weight \u8ba1\u7b97\u6bcf\u4e2aElasticQuota\u53ef\u4ee5\u501f\u5165\u7684\u91cf\\n4. \u5982\u679c\u6700\u65b0\u7684 runtime > reuqest\uff0c\u90a3\u4e48 runtime - request \u5269\u4f59\u7684\u91cf\u53ef\u4ee5\u501f\u7ed9\u66f4\u9700\u8981\u7684\u5bf9\u8c61\u3002\\n\\n\u53e6\u5916\u8fd8\u6709\u4e00\u79cd\u65e5\u5e38\u751f\u4ea7\u65f6\u4f1a\u9047\u5230\u7684\u60c5\u51b5\uff1a\u5373\u96c6\u7fa4\u5185\u8d44\u6e90\u603b\u91cf\u4f1a\u968f\u7740\u8282\u70b9\u6545\u969c\u3001\u8d44\u6e90\u8fd0\u8425\u7b49\u539f\u56e0\u964d\u4f4e\uff0c\u5bfc\u81f4\u6240\u6709ElasticQuota\u7684 min \u4e4b\u548c\u5927\u4e8e\u8d44\u6e90\u603b\u91cf\u3002\u5f53\u51fa\u73b0\u8fd9\u79cd\u60c5\u51b5\u65f6\uff0c\u6211\u4eec\u65e0\u6cd5\u786e\u4fdd min \u7684\u8d44\u6e90\u8ff0\u6c42\u3002\u6b64\u65f6\u6211\u4eec\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u6bd4\u4f8b\u8c03\u6574\u5404\u4e2aElasticQuota\u7684min\uff0c\u786e\u4fdd\u6240\u6709min\u4e4b\u548c\u5c0f\u4e8e\u6216\u8005\u7b49\u4e8e\u5f53\u524d\u5b9e\u9645\u7684\u8d44\u6e90\u603b\u91cf\u3002\\n\\n#### \u62a2\u5360\u673a\u5236\\n\\nKoordinator ElasticQuota \u673a\u5236\u5728\u8c03\u5ea6\u9636\u6bb5\u5982\u679c\u53d1\u73b0 Quota \u4e0d\u8db3\uff0c\u4f1a\u8fdb\u5165\u62a2\u5360\u9636\u6bb5\uff0c\u6309\u7167\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u62a2\u5360\u5c5e\u4e8e\u540c\u4e00\u4e2aElasticQuota \u5185\u7684 \u4f4e\u4f18\u5148\u7ea7 Pod\u3002 \u540c\u65f6\uff0c\u6211\u4eec\u4e0d\u652f\u6301\u8de8 ElasticQuota \u62a2\u5360\u5176\u4ed6 Pod\u3002\u4f46\u662f\u6211\u4eec\u4e5f\u63d0\u4f9b\u4e86\u53e6\u5916\u7684\u673a\u5236\u652f\u6301\u4ece\u501f\u7528 Quota \u7684 ElasticQuota \u62a2\u56de\u3002<br />\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5728\u96c6\u7fa4\u4e2d\uff0c\u6709\u4e24\u4e2a ElasticQuota\uff0cElasticQuota A {min = 50, max = 100}\uff0c ElasticQuota B {min = 50,  max = 100}\u3002\u7528\u6237\u5728\u4e0a\u534810\u70b9\u4f7f\u7528 ElasticQuota A \u63d0\u4ea4\u4e86\u4e00\u4e2a Job\uff0c Request = 100 \uff0c\u6b64\u65f6\u56e0\u4e3a ElasticQuota B \u65e0\u4eba\u4f7f\u7528\uff0cElasticQuota A \u80fd\u4ece B \u624b\u91cc\u501f\u752850\u4e2aQuota\uff0c\u6ee1\u8db3\u4e86 Request = 100\uff0c \u5e76\u4e14\u6b64\u65f6 Used = 100\u3002\u572811\u70b9\u949f\u65f6\uff0c\u53e6\u4e00\u4e2a\u7528\u6237\u5f00\u59cb\u4f7f\u7528 ElasticQuota B \u63d0\u4ea4Job\uff0cRequest = 100\uff0c\u56e0\u4e3a ElasticQuota B \u7684 min = 50\uff0c\u662f\u5fc5\u987b\u4fdd\u969c\u7684\uff0c\u901a\u8fc7\u516c\u5e73\u6027\u4fdd\u969c\u673a\u5236\uff0c\u6b64\u65f6 A \u548c B \u7684 runtime \u5747\u4e3a50\u3002\u90a3\u4e48\u6b64\u65f6\u5bf9\u4e8e ElasticQuota A \uff0cUsed = 100 \u662f\u5927\u4e8e\u5f53\u524d runtime = 50 \u7684\uff0c\u56e0\u6b64\u6211\u4eec\u4f1a\u63d0\u4f9b\u4e00\u4e2a Controller\uff0c\u9a71\u9010\u6389\u4e00\u90e8\u5206 Pod \uff0c\u4f7f\u5f97\u5f53\u524d ElasticQuota A \u7684 Used \u964d\u4f4e\u5230 runtime \u76f8\u7b49\u7684\u6c34\u4f4d\u3002\\n\\n## 2. \u7cbe\u7ec6\u5316\u8d44\u6e90\u8c03\u5ea6\\n\\n### Device Share Scheduling\\n\\n\u673a\u5668\u5b66\u4e60\u9886\u57df\u91cc\u4f9d\u9760\u5927\u91cf\u5f3a\u5927\u7b97\u529b\u6027\u80fd\u7684 GPU \u8bbe\u5907\u5b8c\u6210\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u662f GPU \u81ea\u8eab\u4ef7\u683c\u5341\u5206\u6602\u8d35\u3002\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528GPU\u8bbe\u5907\uff0c\u53d1\u6325GPU\u7684\u4ef7\u503c\uff0c\u964d\u4f4e\u6210\u672c\uff0c\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002  Kubernetes \u793e\u533a\u73b0\u6709\u7684 GPU \u5206\u914d\u673a\u5236\u4e2d\uff0cGPU \u662f\u7531 kubelet \u5206\u914d\u7684\uff0c\u5e76\u53ea\u652f\u6301\u5206\u914d\u4e00\u4e2a\u6216\u591a\u4e2a\u5b8c\u6574\u7684 GPU \u5b9e\u4f8b\u3002 \u8fd9\u79cd\u65b9\u6cd5\u7b80\u5355\u53ef\u9760\uff0c\u4f46\u7c7b\u4f3c\u4e8e CPU \u548c Memory\uff0cGPU \u5e76\u4e0d\u662f\u4e00\u76f4\u5904\u4e8e\u9ad8\u5229\u7528\u7387\u6c34\u4f4d\uff0c\u540c\u6837\u5b58\u5728\u8d44\u6e90\u6d6a\u8d39\u7684\u95ee\u9898\u3002 \u56e0\u6b64\uff0cKoordinator \u5e0c\u671b\u652f\u6301\u591a\u5de5\u4f5c\u8d1f\u8f7d\u5171\u4eab\u4f7f\u7528 GPU \u8bbe\u5907\u4ee5\u8282\u7701\u6210\u672c\u3002 \u6b64\u5916\uff0cGPU \u6709\u5176\u7279\u6b8a\u6027\u3002 \u6bd4\u5982\u4e0b\u9762\u7684 NVIDIA GPU \u652f\u6301\u7684 NVLink \u548c\u8d85\u5356\u573a\u666f\uff0c\u90fd\u9700\u8981\u901a\u8fc7\u8c03\u5ea6\u5668\u8fdb\u884c\u4e2d\u592e\u51b3\u7b56\uff0c\u4ee5\u83b7\u5f97\u5168\u5c40\u6700\u4f18\u7684\u5206\u914d\u7ed3\u679c\u3002<br />![image.png](../../static/img/nvlink.png)\\n\\n\u4ece\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0c\u867d\u7136\u8be5\u8282\u70b9\u67098\u4e2a GPU \u5b9e\u4f8b\uff0c\u578b\u53f7\u4e3aA100/V100\uff0c\u4f46 GPU \u5b9e\u4f8b\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\u901f\u5ea6\u662f\u4e0d\u540c\u7684\u3002 \u5f53\u4e00\u4e2a Pod \u9700\u8981\u591a\u4e2a GPU \u5b9e\u4f8b\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a Pod \u5206\u914d\u5177\u6709\u6700\u5927\u6570\u636e\u4f20\u8f93\u901f\u5ea6\u7ec4\u5408\u5173\u7cfb\u7684 GPU \u5b9e\u4f8b\u3002 \u6b64\u5916\uff0c\u5f53\u6211\u4eec\u5e0c\u671b\u4e00\u7ec4 Pod \u4e2d\u7684 GPU \u5b9e\u4f8b\u5177\u6709\u6700\u5927\u6570\u636e\u4f20\u8f93\u901f\u5ea6\u7ec4\u5408\u5173\u7cfb\u65f6\uff0c\u8c03\u5ea6\u5668\u5e94\u8be5\u5c06\u6700\u4f73 GPU \u5b9e\u4f8b\u6279\u91cf\u5206\u914d\u7ed9\u8fd9\u4e9b Pod\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u914d\u5230\u540c\u4e00\u4e2a\u8282\u70b9\u3002\\n\\n#### GPU \u8d44\u6e90\u534f\u8bae\\n\\nKoordinator \u517c\u5bb9\u793e\u533a\u5df2\u6709\u7684 `nvidia.com/gpu`\u8d44\u6e90\u534f\u8bae\uff0c\u5e76\u4e14\u8fd8\u81ea\u5b9a\u4e49\u4e86\u6269\u5c55\u8d44\u6e90\u534f\u8bae\uff0c\u652f\u6301\u7528\u6237\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u914d GPU \u8d44\u6e90\u3002\\n\\n- kubernetes.io/gpu-core \u4ee3\u8868GPU\u7684\u8ba1\u7b97\u80fd\u529b\u3002 \u4e0e Kuberetes MilliCPU \u7c7b\u4f3c\uff0c\u6211\u4eec\u5c06 GPU \u7684\u603b\u7b97\u529b\u62bd\u8c61\u4e3a100\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u9700\u8981\u7533\u8bf7\u76f8\u5e94\u6570\u91cf\u7684 GPU \u7b97\u529b\u3002\\n- kubernetes.io/gpu-memory \u8868\u793a GPU \u7684\u5185\u5b58\u5bb9\u91cf\uff0c\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\u3002\\n- kubernetes.io/gpu-memory-ratio \u4ee3\u8868 GPU \u5185\u5b58\u7684\u767e\u5206\u6bd4\u3002\\n\\n\u5047\u8bbe\u4e00\u4e2a\u8282\u70b9\u67094\u4e2aGPU\u8bbe\u5907\u5b9e\u4f8b\uff0c\u6bcf\u4e2aGPU\u8bbe\u5907\u5b9e\u4f8b\u6709 8Gi \u663e\u5b58\u3002\u7528\u6237\u5982\u679c\u671f\u671b\u7533\u8bf7\u4e00\u4e2a\u5b8c\u6574\u7684 GPU \u5b9e\u4f8b\uff0c\u9664\u4e86\u4f7f\u7528 `nvidia.com/gpu`\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u65b9\u5f0f\u7533\u8bf7\uff1a\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: demo-pod\\n  namespace: default\\nspec:\\n  containers:\\n  - name: main\\n    resources:\\n      limits: \\n        kubernetes.io/gpu-core: 100\\n        kubernetes.io/gpu-memory: \\"8Gi\\"\\n      requests:\\n        kubernetes.io/gpu-core: 100\\n        kubernetes.io/gpu-memory: \\"8Gi\\"\\n```\\n\\n\u5982\u679c\u671f\u671b\u53ea\u4f7f\u7528\u4e00\u4e2a GPU \u5b9e\u4f8b\u4e00\u534a\u7684\u8d44\u6e90\uff0c\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u65b9\u5f0f\u7533\u8bf7\uff1a\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: demo-pod\\n  namespace: default\\nspec:\\n  containers:\\n  - name: main\\n    resources:\\n      limits: \\n        kubernetes.io/gpu-core: 50\\n        kubernetes.io/gpu-memory: \\"4Gi\\"\\n      requests:\\n        kubernetes.io/gpu-core: 50\\n        kubernetes.io/gpu-memory: \\"4Gi\\"\\n```\\n#### \u8bbe\u5907\u4fe1\u606f\u548c\u8bbe\u5907\u5bb9\u91cf\u4e0a\u62a5\\n\\n\u5728 Koordinator v0.7.0 \u7248\u672c\u4e2d\uff0c\u5355\u673a\u4fa7 koordlet \u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u8bc6\u522b\u8282\u70b9\u4e0a\u662f\u5426\u542b\u6709 GPU \u8bbe\u5907\uff0c\u5982\u679c\u5b58\u5728\u7684\u8bdd\uff0c\u4f1a\u4e0a\u62a5\u8fd9\u4e9b GPU \u8bbe\u5907\u7684 Minor ID\u3001 UUID\u3001\u7b97\u529b\u548c\u663e\u5b58\u5927\u5c0f\u5230\u4e00\u4e2a\u7c7b\u578b\u4e3a Device CRD \u4e2d\u3002\u6bcf\u4e2a\u8282\u70b9\u5bf9\u5e94\u4e00\u4e2a Device CRD \u5b9e\u4f8b\u3002Device CRD \u4e0d\u4ec5\u652f\u6301\u63cf\u8ff0 GPU\uff0c\u8fd8\u652f\u6301\u7c7b\u4f3c\u4e8e FPGA/RDMA\u7b49\u8bbe\u5907\u7c7b\u578b\uff0c\u76ee\u524d v0.7.0 \u7248\u672c\u53ea\u652f\u6301 GPU\uff0c \u6682\u672a\u652f\u6301\u8fd9\u4e9b\u8bbe\u5907\u7c7b\u578b\u3002 <br />Device CRD \u4f1a\u88ab koord-manager \u5185\u7684 NodeResource controller \u548c koord-scheduler \u6d88\u8d39\u3002NodeResource controller \u4f1a\u6839\u636e Device CRD \u4e2d\u63cf\u8ff0\u7684\u4fe1\u606f\uff0c\u6362\u7b97\u6210 Koordinator \u652f\u6301\u7684\u8d44\u6e90\u534f\u8bae `kubernetes.io/gpu-core`,`kubernetes.io/gpu-memory` \u66f4\u65b0\u5230  Node.Status.Allocatable \u548c Node.Status.Capacity \u5b57\u6bb5\uff0c\u5e2e\u52a9\u8c03\u5ea6\u5668\u548c kubelet \u5b8c\u6210\u8d44\u6e90\u8c03\u5ea6\u3002gpu-core \u8868\u793aGPU \u8bbe\u5907\u5b9e\u4f8b\u7684\u7b97\u529b\uff0c\u4e00\u4e2a\u5b9e\u4f8b\u7684\u5b8c\u6574\u7b97\u529b\u4e3a100\u3002\u5047\u8bbe\u4e00\u4e2a\u8282\u70b9\u6709 8 \u4e2a GPU \u8bbe\u5907\u5b9e\u4f8b\uff0c\u90a3\u4e48\u8282\u70b9\u7684 gpu-core \u5bb9\u91cf\u4e3a 8 * 100 = 800\uff1b gpu-memory \u8868\u793a GPU \u8bbe\u5907\u5b9e\u4f8b\u7684\u663e\u5b58\u5927\u5c0f\uff0c\u5355\u4f4d\u4e3a\u5b57\u8282\uff0c\u540c\u6837\u7684\u8282\u70b9\u53ef\u4ee5\u5206\u914d\u7684\u663e\u5b58\u603b\u91cf\u4e3a \u8bbe\u5907\u6570\u91cf * \u6bcf\u4e2a\u5b9e\u4f8b\u7684\u5355\u4f4d\u5bb9\u91cf\uff0c\u4f8b\u5982\u4e00\u4e2a GPU \u8bbe\u5907\u7684\u663e\u5b58\u662f 8G\uff0c\u8282\u70b9\u4e0a\u67098 \u4e2a GPU \u5b9e\u4f8b\uff0c\u603b\u91cf\u4e3a 8 * 8G = 64G\u3002\\n```yaml\\napiVersion: v1\\nkind: Node\\nmetadata:\\n  name: node-a\\nstatus:\\n  capacity:\\n    koordinator.sh/gpu-core: 800\\n    koordinator.sh/gpu-memory: \\"64Gi\\"\\n    koordinator.sh/gpu-memory-ratio: 800\\n  allocatable:\\n    koordinator.sh/gpu-core: 800\\n    koordinator.sh/gpu-memory: \\"64Gi\\"\\n    koordinator.sh/gpu-memory-ratio: 800\\n```\\n\\n#### \u4e2d\u5fc3\u8c03\u5ea6\u5206\u914d\u8bbe\u5907\u8d44\u6e90\\n\\nKuberetes \u793e\u533a\u539f\u751f\u63d0\u4f9b\u7684\u8bbe\u5907\u8c03\u5ea6\u673a\u5236\u4e2d\uff0c\u8c03\u5ea6\u5668\u53ea\u8d1f\u8d23\u6821\u9a8c\u8bbe\u5907\u5bb9\u91cf\u662f\u5426\u6ee1\u8db3 Pod\uff0c\u5bf9\u4e8e\u4e00\u4e9b\u7b80\u5355\u7684\u8bbe\u5907\u7c7b\u578b\u662f\u8db3\u591f\u7684\uff0c\u4f46\u662f\u5f53\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u5206\u914d GPU \u65f6\uff0c\u9700\u8981\u4e2d\u5fc3\u8c03\u5ea6\u5668\u7ed9\u4e88\u652f\u6301\u624d\u80fd\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u3002<br />Koordinator \u8c03\u5ea6\u5668 koord-scheduler \u65b0\u589e\u4e86\u8c03\u5ea6\u63d2\u4ef6 DeviceShare\uff0c\u8d1f\u8d23\u7cbe\u7ec6\u5ea6\u8bbe\u5907\u8d44\u6e90\u8c03\u5ea6\u3002DeviceShare \u63d2\u4ef6\u6d88\u8d39 Device CRD\uff0c\u8bb0\u5f55\u6bcf\u4e2a\u8282\u70b9\u53ef\u4ee5\u5206\u914d\u7684\u8bbe\u5907\u4fe1\u606f\u3002DeviceShare \u5728\u8c03\u5ea6\u65f6\uff0c\u4f1a\u628a Pod \u7684GPU\u8d44\u6e90\u8bf7\u6c42\u8f6c\u6362\u4e3a Koordinator \u7684\u8d44\u6e90\u534f\u8bae\uff0c\u5e76\u8fc7\u6ee4\u6bcf\u4e2a\u8282\u70b9\u7684\u672a\u5206\u914d\u7684 GPU \u8bbe\u5907\u5b9e\u4f8b\u3002\u786e\u4fdd\u6709\u8d44\u6e90\u53ef\u7528\u540e\uff0c\u5728 Reserve \u9636\u6bb5\u66f4\u65b0\u5185\u90e8\u72b6\u6001\uff0c\u5e76\u5728 PreBind \u9636\u6bb5\u66f4\u65b0 Pod Annotation\uff0c\u8bb0\u5f55\u5f53\u524d Pod \u5e94\u8be5\u4f7f\u7528\u54ea\u4e9b GPU \u8bbe\u5907\u3002<br />DeviceShare \u5c06\u5728\u540e\u7eed\u7248\u672c\u652f\u6301 Binpacking  \u548c Spread \u7b56\u7565\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8bbe\u5907\u8d44\u6e90\u8c03\u5ea6\u80fd\u529b\u3002\\n\\n#### \u5355\u673a\u4fa7\u7cbe\u51c6\u7ed1\u5b9a\u8bbe\u5907\u4fe1\u606f\\n\\nKubernetes \u793e\u533a\u5728 kubelet \u4e2d\u63d0\u4f9b\u4e86 DevicePlugin \u673a\u5236\uff0c\u652f\u6301\u8bbe\u5907\u5382\u5546\u5728 kubelet \u5206\u914d\u597d\u8bbe\u5907\u540e\u6709\u673a\u4f1a\u83b7\u5f97\u8bbe\u5907\u4fe1\u606f\uff0c\u5e76\u586b\u5145\u5230\u73af\u5883\u53d8\u91cf\u6216\u8005\u66f4\u65b0\u6302\u8f7d\u8def\u5f84\u3002\u4f46\u662f\u4e0d\u80fd\u652f\u6301 \u4e2d\u5fc3\u5316\u7684 GPU \u7cbe\u7ec6\u5316\u8c03\u5ea6\u573a\u666f\u3002<br />\u9488\u5bf9\u8fd9\u4e2a\u95ee\u9898\uff0c Koordinator \u6269\u5c55\u4e86 koord-runtime-proxy \uff0c\u652f\u6301\u5728 kubelet \u521b\u5efa\u5bb9\u5668\u65f6\u66f4\u65b0\u73af\u5883\u53d8\u91cf\uff0c\u6ce8\u5165\u8c03\u5ea6\u5668\u5206\u914d\u7684 GPU \u8bbe\u5907\u4fe1\u606f\u3002<br />![](../../static/img/koordlet-inject-env.jpeg)\\n\\n## 3. \u8c03\u5ea6\u5668\u8bca\u65ad\u5206\u6790\\n\\n\u5927\u5bb6\u5728\u4f7f\u7528 Kubernetes \u65f6\u7ecf\u5e38\u4f1a\u9047\u5230\u4e00\u4e9b\u8c03\u5ea6\u76f8\u5173\u7684\u95ee\u9898\uff1a\\n\\n1. \u6211\u8fd9\u4e2a Pod \u4e3a\u4ec0\u4e48\u4e0d\u80fd\u8c03\u5ea6\uff1f\\n2. \u8fd9\u4e2a Pod \u4e3a\u4ec0\u4e48\u4f1a\u8c03\u5ea6\u5230\u8fd9\u4e2a\u8282\u70b9\uff0c\u4e0d\u662f\u5e94\u8be5\u88ab\u53e6\u4e00\u4e2a\u6253\u5206\u63d2\u4ef6\u5f71\u54cd\u5230\u4e48\uff1f \\n3. \u6211\u65b0\u5f00\u53d1\u4e86\u4e00\u4e2a\u63d2\u4ef6\uff0c\u53d1\u73b0\u8c03\u5ea6\u7ed3\u679c\u4e0d\u7b26\u5408\u9884\u671f\uff0c\u4f46\u662f\u6709\u4e0d\u77e5\u9053\u54ea\u91cc\u51fa\u4e86\u95ee\u9898\u3002\\n\\n\u8981\u8bca\u65ad\u5206\u6790\u8fd9\u4e9b\u95ee\u9898\uff0c\u9664\u4e86\u8981\u638c\u63e1 Kubernetes \u57fa\u672c\u7684\u8c03\u5ea6\u673a\u5236\u548c\u8d44\u6e90\u5206\u914d\u673a\u5236\u5916\uff0c\u8fd8\u9700\u8981\u8c03\u5ea6\u5668\u81ea\u8eab\u7ed9\u4e88\u652f\u6301\u3002\u4f46\u662f Kubernetes kube-scheduler \u63d0\u4f9b\u7684\u8bca\u65ad\u80fd\u529b\u6bd4\u8f83\u6709\u9650\uff0c\u6709\u65f6\u5019\u751a\u81f3\u6ca1\u6709\u4ec0\u4e48\u65e5\u5fd7\u53ef\u4ee5\u67e5\u770b\u3002kube-scheduler \u539f\u751f\u662f\u652f\u6301\u901a\u8fc7 HTTP \u66f4\u6539\u65e5\u5fd7\u7b49\u7ea7\uff0c\u53ef\u4ee5\u83b7\u5f97\u66f4\u591a\u65e5\u5fd7\u4fe1\u606f\uff0c\u4f8b\u5982\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u53ef\u4ee5\u66f4\u6539\u65e5\u5fd7\u7b49\u7ea7\u52305\uff1a\\n```bash\\n$ curl -X PUT schedulerLeaderIP:10251/debug/flags/v --data \'5\' \\nsuccessfully set klog.logging.verbosity to 5\\n```\\n\\nKoordinator \u9488\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e00\u5957 Restful API \uff0c\u5e2e\u52a9\u7528\u6237\u63d0\u5347\u95ee\u9898\u8bca\u65ad\u5206\u6790\u7684\u6548\u7387\\n\\n### \u5206\u6790 Score \u7ed3\u679c\\n\\n`PUT /debug/flags/s`  \u5141\u8bb8\u7528\u6237\u6253\u5f00 Debug Score \u5f00\u5173\uff0c\u5728\u6253\u5206\u7ed3\u675f\u540e\uff0c\u4ee5Markdown \u683c\u5f0f\u6253\u5370 TopN \u8282\u70b9\u5404\u4e2a\u63d2\u4ef6\u7684\u5206\u503c\u3002\u4f8b\u5982\uff1a\\n```bash\\n$ curl -X PUT schedulerLeaderIP:10251/debug/flags/s --data \'100\'\\nsuccessfully set debugTopNScores to 100\\n```\\n \u5f53\u6709\u65b0 Pod \u8c03\u5ea6\u65f6\uff0c\u89c2\u5bdf scheduler log \u53ef\u4ee5\u770b\u5230\u5982\u4e0b\u4fe1\u606f\\n```bash\\n| # | Pod | Node | Score | ImageLocality | InterPodAffinity | LoadAwareScheduling | NodeAffinity | NodeNUMAResource | NodeResourcesBalancedAllocation | NodeResourcesFit | PodTopologySpread | Reservation | TaintToleration |\\n| --- | --- | --- | ---:| ---:| ---:| ---:| ---:| ---:| ---:| ---:| ---:| ---:| ---:|\\n| 0 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.51 | 577 | 0 | 0 | 87 | 0 | 0 | 96 | 94 | 200 | 0 | 100 |\\n| 1 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.50 | 574 | 0 | 0 | 85 | 0 | 0 | 96 | 93 | 200 | 0 | 100 |\\n| 2 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.19 | 541 | 0 | 0 | 55 | 0 | 0 | 95 | 91 | 200 | 0 | 100 |\\n| 3 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.18 | 487 | 0 | 0 | 15 | 0 | 0 | 90 | 82 | 200 | 0 | 100 |\\n\\n```\\n\\n\u627e\u4e2a Markdown \u5de5\u5177\uff0c\u5c31\u53ef\u4ee5\u8f6c\u4e3a\u5982\u4e0b\u8868\u683c\\n\\n| # | Pod | Node | Score | LoadAwareScheduling | NodeNUMAResource | NodeResourcesFit | PodTopologySpread |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| 0 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.51 | 577 | 87 | 0 | 94 | 200 |\\n| 1 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.50 | 574 | 85 | 0 | 93 | 200 |\\n| 2 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.19 | 541 | 55 | 0 | 91 | 200 |\\n| 3 | default/curlimage-545745d8f8-rngp7 | cn-hangzhou.10.0.4.18 | 487 | 15 | 0 | 82 | 200 |\\n\\n### \u8c03\u5ea6\u63d2\u4ef6\u5bfc\u51fa\u5185\u90e8\u72b6\u6001\\n\\n\u50cf koord-scheduler \u5185\u90e8\u7684 NodeNUMAResource \u3001 DeviceShare\u548cElasticQuota\u7b49\u63d2\u4ef6\u5185\u90e8\u90fd\u6709\u7ef4\u62a4\u4e00\u4e9b\u72b6\u6001\u5e2e\u52a9\u8c03\u5ea6\u3002 koord-scheduler \u81ea\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65b0\u7684\u63d2\u4ef6\u6269\u5c55\u63a5\u53e3\uff08\u5b9a\u4e49\u89c1\u4e0b\u6587\uff09\uff0c\u5e76\u4f1a\u5728\u521d\u59cb\u5316\u63d2\u4ef6\u540e\uff0c\u8bc6\u522b\u8be5\u63d2\u4ef6\u662f\u5426\u5b9e\u73b0\u4e86\u8be5\u63a5\u53e3\u5e76\u8c03\u7528\u8be5\u63a5\u53e3\uff0c\u8ba9\u63d2\u4ef6\u6ce8\u5165\u9700\u8981\u66b4\u9732\u7684 RestfulAPI\u3002\u4ee5 NodeNUMAResource \u63d2\u4ef6\u4e3a\u4f8b\uff0c\u4f1a\u63d0\u4f9b `/cpuTopologyOptions/:nodeName`\u548c `/availableCPUs/:nodeName`\u4e24\u4e2aEndpoints\uff0c\u53ef\u4ee5\u67e5\u770b\u63d2\u4ef6\u5185\u90e8\u8bb0\u5f55\u7684 CPU \u62d3\u6251\u4fe1\u606f\u548c\u5206\u914d\u7ed3\u679c\u3002\\n\\n```go\\ntype APIServiceProvider interface {\\n\\tRegisterEndpoints(group *gin.RouterGroup)\\n}\\n```\\n\\n\u7528\u6237\u5728\u4f7f\u7528\u65f6\uff0c\u6309\u7167 `/apis/v1/plugins/<pluginName>/<pluginEndpoints>`\u65b9 \u5f0f\u6784\u5efa URL \u67e5\u770b\u6570\u636e\uff0c\u4f8b\u5982\u8981\u67e5\u770b `/cpuTopologyOptions/:nodeName`\uff1a\\n\\n```bash\\n$ curl schedulerLeaderIP:10252/apis/v1/plugins/NodeNUMAResources/cpuTopologyOptions/node-1\\n{\\"cpuTopology\\":{\\"numCPUs\\":32,\\"numCores\\":16,\\"numNodes\\":1,\\"numSockets\\":1,\\"cpuDetails\\":....\\n```\\n\\n### \u67e5\u770b\u5f53\u524d\u652f\u6301\u7684\u63d2\u4ef6 API\\n\\n\u4e3a\u4e86\u65b9\u4fbf\u5927\u5bb6\u4f7f\u7528\uff0ckoord-scheduler \u63d0\u4f9b\u4e86 `/apis/v1/__services__` \u67e5\u770b\u652f\u6301\u7684 API Endpoints\\n```bash\\n$ curl schedulerLeaderIP:10251/apis/v1/__services__\\n{\\n    \\"GET\\": [\\n        \\"/apis/v1/__services__\\",\\n        \\"/apis/v1/nodes/:nodeName\\",\\n        \\"/apis/v1/plugins/Coscheduling/gang/:namespace/:name\\",\\n        \\"/apis/v1/plugins/DeviceShare/nodeDeviceSummaries\\",\\n        \\"/apis/v1/plugins/DeviceShare/nodeDeviceSummaries/:name\\",\\n        \\"/apis/v1/plugins/ElasticQuota/quota/:name\\",\\n        \\"/apis/v1/plugins/NodeNUMAResource/availableCPUs/:nodeName\\",\\n        \\"/apis/v1/plugins/NodeNUMAResource/cpuTopologyOptions/:nodeName\\"\\n    ]\\n}\\n```\\n\\n## 4. \u66f4\u5b89\u5168\u7684\u91cd\u8c03\u5ea6\\n\\n\u5728 Koordinator v0.6 \u7248\u672c\u4e2d\u6211\u4eec\u53d1\u5e03\u4e86\u5168\u65b0\u7684 koord-descheduler\uff0c\u652f\u6301\u63d2\u4ef6\u5316\u5b9e\u73b0\u9700\u8981\u7684\u91cd\u8c03\u5ea6\u7b56\u7565\u548c\u81ea\u5b9a\u4e49\u9a71\u9010\u673a\u5236\uff0c\u5e76\u5185\u7f6e\u4e86\u9762\u5411 PodMigrationJob \u7684\u8fc1\u79fb\u63a7\u5236\u5668\uff0c\u901a\u8fc7 Koordinator Reservation \u673a\u5236\u9884\u7559\u8d44\u6e90\uff0c\u786e\u4fdd\u6709\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u53d1\u8d77\u9a71\u9010\u3002\u89e3\u51b3\u4e86 Pod \u88ab\u9a71\u9010\u540e\u65e0\u8d44\u6e90\u53ef\u7528\u5f71\u54cd\u5e94\u7528\u7684\u53ef\u7528\u6027\u95ee\u9898\u3002<br />Koordinator v0.7 \u7248\u672c\u4e2d\uff0ckoord-descheduler \u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u7684\u91cd\u8c03\u5ea6\\n\\n- \u652f\u6301 Evict \u9650\u6d41\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u9700\u8981\u914d\u7f6e\u9650\u6d41\u7b56\u7565\uff0c\u4f8b\u5982\u5141\u8bb8\u6bcf\u5206\u949f\u9a71\u9010\u591a\u5c11\u4e2a Pod\\n- \u652f\u6301\u914d\u7f6e Namespace \u7070\u5ea6\u91cd\u8c03\u5ea6\u80fd\u529b\uff0c\u8ba9\u7528\u6237\u53ef\u4ee5\u66f4\u653e\u5fc3\u7684\u7070\u5ea6\\n- \u652f\u6301\u6309\u7167 Node/Namespace \u914d\u7f6e\u9a71\u9010\u6570\u91cf\uff0c\u4f8b\u5982\u914d\u7f6e\u8282\u70b9\u7ef4\u5ea6\u6700\u591a\u53ea\u9a71\u9010\u4e24\u4e2a\uff0c\u90a3\u4e48\u5373\u4f7f\u6709\u63d2\u4ef6\u8981\u6c42\u9a71\u9010\u8be5\u8282\u70b9\u4e0a\u7684\u66f4\u591aPod\uff0c\u4f1a\u88ab\u62d2\u7edd\u3002\\n- \u611f\u77e5 Workload \uff0c\u5982\u679c\u4e00\u4e2a Workload \u6b63\u5728\u53d1\u5e03\u3001\u7f29\u5bb9\u3001\u5df2\u7ecf\u6709\u4e00\u5b9a\u91cf\u7684 Pod \u6b63\u5728\u88ab\u9a71\u9010\u6216\u8005\u4e00\u4e9bPod NotReady\uff0c\u91cd\u8c03\u5ea6\u5668\u4f1a\u62d2\u7edd\u65b0\u7684\u91cd\u8c03\u5ea6\u8bf7\u6c42\u3002\u76ee\u524d\u652f\u6301\u539f\u751f\u7684 Deployment\uff0cStatefulSet \u4ee5\u53ca Kruise  CloneSet\uff0cKruise AdvancedStatefulSet\u3002\\n\\n\u540e\u7eed\u91cd\u8c03\u5ea6\u5668\u8fd8\u4f1a\u63d0\u5347\u516c\u5e73\u6027\uff0c\u9632\u6b62\u4e00\u76f4\u91cd\u590d\u7684\u91cd\u8c03\u5ea6\u540c\u4e00\u4e2a workload \uff0c\u5c3d\u91cf\u964d\u4f4e\u91cd\u8c03\u5ea6\u5bf9\u5e94\u7528\u7684\u53ef\u7528\u6027\u7684\u5f71\u54cd\u3002\\n\\n## 5. \u5176\u4ed6\u6539\u52a8\\n\\n- Koordinator \u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86 CPU \u7cbe\u7ec6\u5316\u8c03\u5ea6\u80fd\u529b\uff0c\u5b8c\u5168\u517c\u5bb9 kubelet ( <= v1.22) CPU Manager static \u7b56\u7565\u3002\u8c03\u5ea6\u5668\u5206\u914d CPU \u65f6\u4f1a\u907f\u514d\u5206\u914d\u88ab kubelet \u9884\u7559\u7684 CPU\uff0c\u5355\u673a\u4fa7koordlet\u5b8c\u6574\u9002\u914d\u4e86kubelet\u4ece1.18\u52301.22\u7248\u672c\u7684\u5206\u914d\u7b56\u7565\uff0c\u6709\u6548\u907f\u514d\u4e86 CPU \u51b2\u7a81\u3002\\n- \u8d44\u6e90\u9884\u7559\u673a\u5236\u652f\u6301 AllocateOnce \u8bed\u4e49\uff0c\u6ee1\u8db3\u5355\u6b21\u9884\u7559\u573a\u666f\u3002\u5e76\u6539\u8fdb\u4e86 Reservation \u72b6\u6001\u8bed\u4e49\uff0c\u66f4\u52a0\u51c6\u786e\u63cf\u8ff0 Reservation \u5bf9\u8c61\u5f53\u524d\u7684\u72b6\u6001\u3002\\n- \u6539\u8fdb\u4e86\u79bb\u7ebf\u8d44\u6e90(Batch CPU/Memory) \u7684\u58f0\u660e\u65b9\u5f0f\uff0c\u652f\u6301limit\u5927\u4e8erequest\u7684\u8d44\u6e90\u63cf\u8ff0\u5f62\u5f0f\uff0c\u53ef\u4ee5\u65b9\u4fbf\u539fburstable\u7c7b\u578b\u7684\u4efb\u52a1\u76f4\u63a5\u8f6c\u6362\u4e3a\u6df7\u90e8\u6a21\u5f0f\u8fd0\u884c\u3002\\n\\n\u4f60\u53ef\u4ee5\u901a\u8fc7 [Github release[6]](https://github.com/koordinator-sh/koordinator/releases/tag/v0.6.1) \u9875\u9762\uff0c\u6765\u67e5\u770b\u66f4\u591a\u7684\u6539\u52a8\u4ee5\u53ca\u5b83\u4eec\u7684\u4f5c\u8005\u4e0e\u63d0\u4ea4\u8bb0\u5f55\u3002\\n\\n# \u76f8\u5173\u94fe\u63a5\\n\\n- [[1] Koordinator](https://koordinator.sh) \\n- [[2]  Koordinator 0.6 Release Note](https://mp.weixin.qq.com/s/YdoxVxz_91ZFemF8JuxRvQ)\\n- [[3] Design: Gang Scheduling](https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/scheduling/20220901-gang-scheduling.md)\\n- [[4] Design: Multi Hierarchy ElasticQuota Management](https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/scheduling/20220722-multi-hierarchy-elastic-quota-management.md)\\n- [[5] Design: Fine-grained Device Scheduling](https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/scheduling/20220629-fine-grained-device-scheduling.md)\\n- [[6] Github Release](https://github.com/koordinator-sh/koordinator/releases/tag/v0.6.1)\\n- [[7] Slack Channel](https://join.slack.com/t/koordinator-sh/shared_invite/zt-1756qoub4-Cn4~esfdlfAPsD7cwO2NzA)\\n- [[8] \u4e91\u539f\u751f\u6df7\u90e8\u7cfb\u7edf Koordinator \u67b6\u6784\u8be6\u89e3](https://mp.weixin.qq.com/s/y8k_q6rhTIubQ-lqvDp2hw)"},{"id":"release-v0.6.0","metadata":{"permalink":"/zh-Hans/blog/release-v0.6.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-08-04-release/index.md","source":"@site/blog/2022-08-04-release/index.md","title":"Koordinator v0.6: Complete fine-grained CPU orchestration, Resource Reservation and Descheduling","description":"We are happy to announce the release of Koordinator v0.6.0. Koordinator v0.6.0 brings complete Fine-grained CPU Orchestration, Resource Reservation mechanism, safely Pod Migration mechanism and Descheduling Framework.","date":"2022-08-04T00:00:00.000Z","formattedDate":"2022\u5e748\u67084\u65e5","tags":[{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":8.425,"truncated":false,"authors":[{"name":"Joseph","title":"Koordinator maintainer","url":"https://github.com/eahydra","imageURL":"https://github.com/eahydra.png","key":"joseph"}],"frontMatter":{"slug":"release-v0.6.0","title":"Koordinator v0.6: Complete fine-grained CPU orchestration, Resource Reservation and Descheduling","authors":["joseph"],"tags":["release"]},"prevItem":{"title":"Koordinator v0.7: \u4e3a\u4efb\u52a1\u8c03\u5ea6\u9886\u57df\u6ce8\u5165\u65b0\u6d3b\u529b","permalink":"/zh-Hans/blog/release-v0.7.0"},"nextItem":{"title":"Koordinator v0.5: Now With Node Resource Topology And More","permalink":"/zh-Hans/blog/release-v0.5.0"}},"content":"We are happy to announce the release of Koordinator v0.6.0. Koordinator v0.6.0 brings complete Fine-grained CPU Orchestration, Resource Reservation mechanism, safely Pod Migration mechanism and Descheduling Framework.\\n\\n## Install or Upgrade to Koordinator v0.6.0\\n\\n### Install with helms\\n\\nKoordinator can be simply installed by helm v3.5+, which is a simple command-line tool, and you can get it\\nfrom [here](https://github.com/helm/helm/releases).\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Install the latest version.\\n$ helm install koordinator koordinator-sh/koordinator --version 0.6.0\\n```\\n\\n### Upgrade with helm\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Upgrade the latest version.\\n$ helm upgrade koordinator koordinator-sh/koordinator --version 0.6.0 [--force]\\n```\\n\\nFor more details, please refer to the [installation manual](/docs/installation).\\n\\n## Fine-grained CPU Orchestration\\n\\nIn Koordinator v0.5.0, we designed and implemented basic CPU orchestration capabilities. The koord-scheduler supports different CPU bind policies to help LSE/LSR Pods achieve better performance. \\n\\nNow in the v0.6 version, we have basically completed the CPU orchestration capabilities originally designed, such as:\\n- Support default CPU bind policy configured by koord-scheduler for LSR/LSE Pods that do not specify a CPU bind policy\\n- Support CPU exclusive policy that supports `PCPULevel` and `NUMANodeLevel`, which can spread the CPU-bound Pods to different physical cores or NUMA Nodes as much as possible to reduce the interference between Pods.\\n- Support Node CPU Orchestration API to helper cluster administrators control the CPU orchestration behavior of nodes. The label `node.koordinator.sh/cpu-bind-policy` constrains how to bind CPU logical CPUs when scheduling. If set with the `FullPCPUsOnly` that requires that the scheduler must allocate full physical cores. Equivalent to kubelet CPU manager policy option `full-pcpus-only=true`. If there is no `node.koordinator.sh/cpu-bind-policy` in the node\'s label, it will be executed according to the policy configured by the Pod or koord-scheduler. The label `node.koordinator.sh/numa-allocate-strategy` indicates how to choose satisfied NUMA Nodes when scheduling. Support `MostAllocated` and `LeastAllocated`.\\n- koordlet supports the LSE Pods and improve compatibility with existing Guaranteed Pods with static CPU Manager policy. \\n\\n\\nPlease check out our [user manual](/docs/user-manuals/fine-grained-cpu-orchestration) for a detailed introduction and\\ntutorial.\\n\\n## Resource Reservation\\n\\nWe completed the `Resource Reservation API` design proposal in v0.5, and implemented the basic Reservation mechanism in the current v0.6 version. \\n\\nWhen you want to use the Reservation mechanism to reserve resources, you do not need to modify the Pod or the existing workloads(e.g. Deployment, StatefulSet). koord-scheduler provides a simple to use API named `Reservation`, which allows us to reserve node resources for specified pods or workloads even if they haven\'t get created yet. You only need to write the Pod Template and the owner information in the ReservationSpec when creating a Reservation. When koord-scheduler perceives a new Reservation object, it will allocate resources to the Reservation object through the normal Pod scheduling process. After scheduling, koord-scheduler will update the success or failure information to ResourceStatus. If the reservation is successful, and the OwnerReference or Labels of the newly created Pod satisfy the owner information declared earlier, then the newly created Pod will directly reuse the resources held by the Reservation. When the Pod is destroyed, the Reservation object can be reused until the Reservation expires.\\n\\n![image](/img/resource-reservation.svg)\\n\\nThe resource reservation mechanism can help solve or optimize the problems in the following scenarios:\\n\\n1. Preemption: Existing preemption does not guarantee that only preempting pods can allocate preempted resources. With a\\n   reservation, the scheduler should be able to \\"lock\\" resources preventing from allocation of other pods with the same\\n   or higher priority.\\n2. Descheduling: For the descheduler, it is better to ensure sufficient resources with the reservation before pods get\\n   rescheduled. Otherwise, rescheduled pods may not be runnable anymore and make the belonging application disrupted.\\n3. Horizontal scaling: Using reservation to achieve more deterministic horizontal scaling. e.g. Submit a reservation and\\n   make sure it is available before scaling up replicas.\\n4. Resource Pre-allocation: Sometimes we want to pre-allocate node resources for future resource demands even if the\\n   resources are not currently allocatable. Reservation can help with this and it should make no physical cost.\\n\\n- Please check out our [user manual](/docs/user-manuals/resource-reservation) for a detailed introduction and\\ntutorial.\\n- For more information, please see [Design: Resource Reservation](/docs/designs/resource-reservation)\\n\\n## Pod Migration Job\\n\\nMigrating Pods is an important capability that many components (such as descheduler) rely on, and can be used to optimize scheduling or help resolve workload runtime quality issues. We believe that pod migration is a complex process, involving steps such as auditing, resource allocation, and application startup, and is mixed with application upgrading, scaling scenarios, resource operation and maintenance operations by cluster administrators. Therefore, how to manage the stability risk of this process to ensure that the application does not fail due to the migration of Pods is a very critical issue that must be resolved.\\n\\nThe descheduler in the K8s community evicts pods according to different strategies. However, it does not guarantee whether the evicted Pod has resources available after re-creation. If a large number of newly created Pods are in the Pending state when the resources in the cluster are tight, may lower the application availabilities.\\n\\nKoordinator defines a CRD-based Migration/Eviction API named `PodMigrationAPI`, through which the descheduler or other components can evict or delete Pods more safely. With PodMigrationJob we can track the status of each process in the migration, and perceive scenarios such as upgrading and scaling of the application.\\n\\nIt\'s simple to use the PodMigrationJob API. Create a `PodMigrationJob` with the YAML file below to migrate `pod-demo-0`. \\n\\n```yaml\\napiVersion: scheduling.koordinator.sh/v1alpha1\\nkind: PodMigrationJob\\nmetadata:\\n  name: migrationjob-demo\\nspec:\\n  paused: false\\n  ttl: 5m\\n  mode: ReservationFirst\\n  podRef:\\n    namespace: default\\n    name: pod-demo-5f9b977566-c7lvk\\nstatus:\\n  phase: Pending\\n```\\n\\n```bash\\n$ kubectl create -f migrationjob-demo.yaml\\npodmigrationjob.scheduling.koordinator.sh/migrationjob-demo created\\n```\\n\\nThen you can query the migration status and query the migration events\\n\\n```bash\\n$ kubectl get podmigrationjob migrationjob-demo\\nNAME                PHASE     STATUS     AGE   NODE     RESERVATION                            PODNAMESPACE   POD                         NEWPOD                      TTL\\nmigrationjob-demo   Succeed   Complete   37s   node-1   d56659ab-ba16-47a2-821d-22d6ba49258e   default        pod-demo-5f9b977566-c7lvk   pod-demo-5f9b977566-nxjdf   5m0s\\n\\n$ kubectl describe podmigrationjob migrationjob-demo\\n...\\nEvents:\\n  Type    Reason                Age    From               Message\\n  ----    ------                ----   ----               -------\\n  Normal  ReservationCreated    8m33s  koord-descheduler  Successfully create Reservation \\"d56659ab-ba16-47a2-821d-22d6ba49258e\\"\\n  Normal  ReservationScheduled  8m33s  koord-descheduler  Assigned Reservation \\"d56659ab-ba16-47a2-821d-22d6ba49258e\\" to node \\"node-1\\"\\n  Normal  Evicting              8m33s  koord-descheduler  Try to evict Pod \\"default/pod-demo-5f9b977566-c7lvk\\"\\n  Normal  EvictComplete         8m     koord-descheduler  Pod \\"default/pod-demo-5f9b977566-c7lvk\\" has been evicted\\n  Normal  Complete              8m     koord-descheduler  Bind Pod \\"default/pod-demo-5f9b977566-nxjdf\\" in Reservation \\"d56659ab-ba16-47a2-821d-22d6ba49258e\\"\\n```\\n\\n- Please check out our [user manual](/docs/user-manuals/pod-migration-job) for a detailed introduction and\\ntutorial.\\n- For more information, please see [Design: PodMigrationJob](/docs/designs/pod-migration-job).\\n\\n## Descheduling Framework\\n\\nWe implemented a brand new Descheduling Framework in v0.6. \\n\\nThe existing descheduler in the community can solve some problems, but we think that there are still many aspects of the descheduler that can be improved, for example, it only supports the mode of periodic execution, and does not support the event-triggered mode. It is not possible to extend and configure custom descheduling strategies without invading the existing code of descheduler like kube-scheduler; it also does not support implementing custom evictor.\\n\\nWe also noticed that the K8s descheduler community also found these problems and proposed corresponding solutions such as [#753 Descheduler framework Proposal](https://github.com/kubernetes-sigs/descheduler/issues/753) and [PoC #781](https://github.com/kubernetes-sigs/descheduler/pull/781). The K8s descheduler community tries to implement a descheduler framework similar to the k8s scheduling framework. This coincides with our thinking.\\n\\nOverall, these solutions solved most of our problems, but we also noticed that the related implementations were not merged into the main branch. But we review these implementations and discussions, and we believe this is the right direction. Considering that Koordiantor has clear milestones for descheduler-related features, we will implement Koordinator\'s own descheduler independently of the upstream community. We try to use some of the designs in the [#753 PR](https://github.com/kubernetes-sigs/descheduler/issues/753) proposed by the community and we will follow the Koordinator\'s compatibility principle with K8s to maintain compatibility with the upstream community descheduler when implementing. Such as independent implementation can also drive the evolution of the upstream community\'s work on the descheduler framework. And when the upstream community has new changes or switches to the architecture that Koordinator deems appropriate, Koordinator will follow up promptly and actively.\\n\\nBased on this descheduling framework, it is very easy to be compatible with the existing descheduling strategies in the K8s community, and users can implement and integrate their own descheduling plugins as easily as K8s Scheduling Framework. At the same time, users are also supported to implement Controller in the form of plugins to realize event-based descheduling scenarios. At the same time, the framework integrates the `MigrationController` based on PodMigrationJob API and serves as the default Evictor plugin to help safely migrate Pods in various descheduling scenarios.\\n\\nAt present, we have implemented the main body of the framework, including the MigrationController based on PodMigrationJob, which is available as a whole. And we also provide [a demo descheduling plugin](https://github.com/koordinator-sh/koordinator/blob/main/pkg/descheduler/framework/plugins/removepodsviolatingnodeaffinity/node_affinity.go). In the future, we will migrate and be compatible with the existing descheduling policies of the community, as well as the load balancing descheduling plugin provided for co-location scenarios. \\n\\nThe current framework is still in the early stage of rapid evolution, and there are still many details that need to be improved. Everyone who is interested is welcome to participate in the construction together. We hope that more people can be more assured and simpler to realize the descheduling capabilities they need.\\n\\n- For more information, please see [Design: descheduling framework](/docs/designs/descheduler-framework).\\n- For specific implementation, please see [pkg/descheduler](https://github.com/koordinator-sh/koordinator/tree/main/pkg/descheduler).\\n\\n## About GPU Scheduling\\n\\nThere are also some new developments in GPU scheduling capabilities that everyone cares about. \\n\\nDuring the iteration of v0.6, we completed the design of [GPU Share Scheduling](https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/scheduling/20220629-fine-grained-device-scheduling.md), and also completed the design of [Gang Scheduling](https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/scheduling/20220701-schedule-gang.md). Development of these capabilities is ongoing and will be released in v0.7. \\n\\nIn addition, in order to explore the mechanism of GPU overcommitment, we have implemented the ability to [report GPU Metric](https://github.com/koordinator-sh/koordinator/pull/361) in v0.6.\\n\\n## What\u2019s coming next in Koordinator\\n\\nDon\'t forget that Koordinator is developed in the open. You can check out our Github milestone to know more about what\\nis happening and what we have planned. For more details, please refer to\\nour [milestone](https://github.com/koordinator-sh/koordinator/milestones). Hope it helps!"},{"id":"release-v0.5.0","metadata":{"permalink":"/zh-Hans/blog/release-v0.5.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-06-30-release/index.md","source":"@site/blog/2022-06-30-release/index.md","title":"Koordinator v0.5: Now With Node Resource Topology And More","description":"In addition to the usual updates to supporting utilities, Koordinator v0.5 adds a couple of new useful features we think","date":"2022-06-30T00:00:00.000Z","formattedDate":"2022\u5e746\u670830\u65e5","tags":[{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":7.265,"truncated":false,"authors":[{"name":"Jason","title":"Koordinator maintainer","url":"https://github.com/jasonliu747","imageURL":"https://github.com/jasonliu747.png","key":"jason"}],"frontMatter":{"slug":"release-v0.5.0","title":"Koordinator v0.5: Now With Node Resource Topology And More","authors":["jason"],"tags":["release"]},"prevItem":{"title":"Koordinator v0.6: Complete fine-grained CPU orchestration, Resource Reservation and Descheduling","permalink":"/zh-Hans/blog/release-v0.6.0"},"nextItem":{"title":"What\'s New in Koordinator v0.4.0?","permalink":"/zh-Hans/blog/release-v0.4.0"}},"content":"In addition to the usual updates to supporting utilities, Koordinator v0.5 adds a couple of new useful features we think\\nyou\'ll like.\\n\\n## Install or Upgrade to Koordinator v0.5.0\\n\\n### Install with helms\\n\\nKoordinator can be simply installed by helm v3.5+, which is a simple command-line tool, and you can get it\\nfrom [here](https://github.com/helm/helm/releases).\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Install the latest version.\\n$ helm install koordinator koordinator-sh/koordinator --version 0.5.0\\n```\\n\\n### Upgrade with helm\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Upgrade the latest version.\\n$ helm upgrade koordinator koordinator-sh/koordinator --version 0.5.0 [--force]\\n```\\n\\nFor more details, please refer to the [installation manual](/docs/installation).\\n\\n## Fine-grained CPU Orchestration\\n\\nIn this version, we introduced a fine-grained CPU orchestration. Pods in the Kubernetes cluster may interfere with\\nothers\' running when they share the same physical resources and both demand many resources. The sharing of CPU resources\\nis almost inevitable. e.g. SMT threads (i.e. logical processors) share execution units of the same core, and cores in\\nthe same chip share one last-level cache. The resource contention can slow down the running of these CPU-sensitive\\nworkloads, resulting in high response latency (RT).\\n\\nTo improve the performance of CPU-sensitive workloads, koord-scheduler provides a mechanism of fine-grained CPU\\norchestration. It enhances the CPU management of Kubernetes and supports detailed NUMA-locality and CPU exclusions.\\n\\nPlease check out our [user manual](/docs/user-manuals/fine-grained-cpu-orchestration) for a detailed introduction and\\ntutorial.\\n\\n## Resource Reservation\\n\\nPods are fundamental units for allocating node resources in Kubernetes, which bind resource requirements with business\\nlogic. The scheduler is not able to reserve node resources for specific pods or workloads. We may try using a fake pod\\nto prepare resources by the preemption mechanism. However, fake pods can be preempted by any scheduled pods with higher\\npriorities, which make resources get scrambled unexpectedly.\\n\\nIn Koordinator, a resource reservation mechanism is proposed to enhance scheduling and especially benefits scenarios\\nbelow:\\n\\n1. Preemption: Existing preemption does not guarantee that only preempting pods can allocate preempted resources. With a\\n   reservation, the scheduler should be able to \\"lock\\" resources preventing from allocation of other pods with the same\\n   or\\n   higher priority.\\n2. De-scheduling: For the descheduler, it is better to ensure sufficient resources with the reservation before pods get\\n   rescheduled. Otherwise, rescheduled pods may not be runnable anymore and make the belonging application disrupted.\\n3. Horizontal scaling: Using reservation to achieve more deterministic horizontal scaling. e.g. Submit a reservation and\\n   make sure it is available before scaling up replicas.\\n4. Resource Pre-allocation: Sometimes we want to pre-allocate node resources for future resource demands even if the\\n   resources are not currently allocatable. Reservation can help with this and it should make no physical cost.\\n\\nThis feature is still under development. We\'ve finalized the API, feel free to check it out.\\n\\n```\\ntype Reservation struct {\\n\\tmetav1.TypeMeta `json:\\",inline\\"`\\n\\t// A Reservation object is non-namespaced.\\n\\t// It can reserve resources for pods of any namespace. Any affinity/anti-affinity of reservation scheduling can be\\n\\t// specified in the pod template.\\n\\tmetav1.ObjectMeta `json:\\"metadata,omitempty\\"`\\n\\tSpec              ReservationSpec   `json:\\"spec,omitempty\\"`\\n\\tStatus            ReservationStatus `json:\\"status,omitempty\\"`\\n}\\n\\ntype ReservationSpec struct {\\n\\t// Template defines the scheduling requirements (resources, affinities, images, ...) processed by the scheduler just\\n\\t// like a normal pod.\\n\\t// If the `template.spec.nodeName` is specified, the scheduler will not choose another node but reserve resources on\\n\\t// the specified node.\\n\\tTemplate *corev1.PodTemplateSpec `json:\\"template,omitempty\\"`\\n\\t// Specify the owners who can allocate the reserved resources.\\n\\t// Multiple owner selectors and ANDed.\\n\\tOwners []ReservationOwner `json:\\"owners,omitempty\\"`\\n\\t// By default, the resources requirements of reservation (specified in `template.spec`) is filtered by whether the\\n\\t// node has sufficient free resources (i.e. ReservationRequest <  NodeFree).\\n\\t// When `preAllocation` is set, the scheduler will skip this validation and allow overcommitment. The scheduled\\n\\t// reservation would be waiting to be available until free resources are sufficient.\\n\\tPreAllocation bool `json:\\"preAllocation,omitempty\\"`\\n\\t// Time-to-Live period for the reservation.\\n\\t// `expires` and `ttl` are mutually exclusive. If both `ttl` and `expires` are not specified, a very\\n\\t// long TTL will be picked as default.\\n\\tTTL *metav1.Duration `json:\\"ttl,omitempty\\"`\\n\\t// Expired timestamp when the reservation expires.\\n\\t// `expires` and `ttl` are mutually exclusive. Defaults to being set dynamically at runtime based on the `ttl`.\\n\\tExpires *metav1.Time `json:\\"expires,omitempty\\"`\\n}\\n\\ntype ReservationStatus struct {\\n\\t// The `phase` indicates whether is reservation is waiting for process (`Pending`), available to allocate\\n\\t// (`Available`) or expired to get cleanup (Expired).\\n\\tPhase ReservationPhase `json:\\"phase,omitempty\\"`\\n\\t// The `conditions` indicate the messages of reason why the reservation is still pending.\\n\\tConditions []ReservationCondition `json:\\"conditions,omitempty\\"`\\n\\t// Current resource owners which allocated the reservation resources.\\n\\tCurrentOwners []corev1.ObjectReference `json:\\"currentOwners,omitempty\\"`\\n}\\n\\ntype ReservationOwner struct {\\n\\t// Multiple field selectors are ORed.\\n\\tObject        *corev1.ObjectReference         `json:\\"object,omitempty\\"`\\n\\tController    *ReservationControllerReference `json:\\"controller,omitempty\\"`\\n\\tLabelSelector *metav1.LabelSelector           `json:\\"labelSelector,omitempty\\"`\\n}\\n\\ntype ReservationControllerReference struct {\\n\\t// Extend with a `namespace` field for reference different namespaces.\\n\\tmetav1.OwnerReference `json:\\",inline\\"`\\n\\tNamespace             string `json:\\"namespace,omitempty\\"`\\n}\\n\\ntype ReservationPhase string\\n\\nconst (\\n\\t// ReservationPending indicates the Reservation has not been processed by the scheduler or is unschedulable for\\n\\t// some reasons (e.g. the resource requirements cannot get satisfied).\\n\\tReservationPending ReservationPhase = \\"Pending\\"\\n\\t// ReservationAvailable indicates the Reservation is both scheduled and available for allocation.\\n\\tReservationAvailable ReservationPhase = \\"Available\\"\\n\\t// ReservationWaiting indicates the Reservation is scheduled, but the resources to reserve are not ready for\\n\\t// allocation (e.g. in pre-allocation for running pods).\\n\\tReservationWaiting ReservationPhase = \\"Waiting\\"\\n\\t// ReservationExpired indicates the Reservation is expired, which the object is not available to allocate and will\\n\\t// get cleaned in the future.\\n\\tReservationExpired ReservationPhase = \\"Expired\\"\\n)\\n\\ntype ReservationCondition struct {\\n\\tLastProbeTime      metav1.Time `json:\\"lastProbeTime\\"`\\n\\tLastTransitionTime metav1.Time `json:\\"lastTransitionTime\\"`\\n\\tReason             string      `json:\\"reason\\"`\\n\\tMessage            string      `json:\\"message\\"`\\n}\\n```\\n\\n## QoS Manager\\n\\nCurrently, plugins from resmanager in Koordlet are mixed together, they should be classified into two\\ncategories: `static` and `dynamic`. Static plugins will be called and run only once when a container created, updated,\\nstarted or stopped. However, for dynamic plugins, they may be called and run at any time according the real-time runtime\\nstates of node, such as CPU suppress, CPU burst, etc. This proposal only focuses on refactoring dynamic plugins. Take a\\nlook at current plugin implementation, there are many function calls to resmanager\'s methods directly, such as\\ncollecting node/pod/container metrics, fetching metadata of node/pod/container, fetching configurations(NodeSLO, etc.).\\nIn the feature, we may need a flexible and powerful framework with scalability for special external plugins.\\n\\nThe below is directory tree of qos-manager inside koordlet, all existing dynamic plugins(as built-in plugins) will be\\nmoved into sub-directory `plugins`.\\n\\n```\\npkg/koordlet/qosmanager/\\n                       - manager.go\\n                       - context.go   // plugin context\\n                       - /plugins/    // built-in plugins\\n                                 - /cpubrust/\\n                                 - /cpusuppress/\\n                                 - /cpuevict/\\n                                 - /memoryevict/\\n```\\n\\nWe only have the proposal in this version. Stay tuned, further implementation is coming soon!\\n\\n## Multiple Running Hook Modes\\n\\n`Runtime Hooks` includes a set of plugins which are responsible for the injections of resource isolation parameters\\nby pod attribute. When `Koord Runtime Proxy` running as a CRI Proxy, `Runtime Hooks` acts as the backend server. The\\nmechanism of CRI Proxy can ensure the consistency of resource parameters during pod lifecycle. However,\\n`Koord Runtime Proxy` can only hijack CRI requests from kubelet for pods, the consistency of resource parameters in\\nQoS class directory cannot be guaranteed. Besides, modification of pod parameters from third-party(e.g. manually) will\\nalso break the correctness of hook plugins.\\n\\nTherefore, a standalone running mode with reconciler for `Runtime Hooks` is necessary. Under `Standalone` running\\nmode, resource isolation parameters will be injected asynchronously, keeping eventual consistency of the injected\\nparameters for pod and QoS class even without `Runtime Hook Manager`.\\n\\n## Some minor works\\n\\n1. We fix the backward compatibility issues reported by our users\\n   in [here](https://github.com/koordinator-sh/koordinator/issues/310). If you\'ve ever encountered similar problem,\\n   please upgrade to the latest version.\\n2. Two more interfaces were added into runtime-proxy. One is `PreCreateContainerHook`, which could set container\\n   resources setting before creating, and the other is `PostStopSandboxHook`, which could do the resource setting\\n   garbage collecting before pods deleted.\\n3. `cpuacct.usage` is more precise than `cpuacct.stat`, and `cpuacct.stat` is in USER_HZ unit, while `cpuacct.usage` is\\n   nanoseconds. After thorough discussion, we were on the same page that we replace `cpuacct.stat` with `cpuacct.usage`\\n   in koordlet.\\n4. Koordlet needs to keep fetching data from kubelet. Before this version, we only support accessing kubelet via\\n   read-only port over HTTP. Due to security concern, we\'ve enabled HTTPS access in this version. For more details,\\n   please refer to this [PR](https://github.com/koordinator-sh/koordinator/pull/320).\\n\\n## What\u2019s coming next in Koordinator\\n\\nDon\'t forget that Koordinator is developed in the open. You can check out our Github milestone to know more about what\\nis happening and what we have planned. For more details, please refer to\\nour [milestone](https://github.com/koordinator-sh/koordinator/milestones). Hope it helps!"},{"id":"release-v0.4.0","metadata":{"permalink":"/zh-Hans/blog/release-v0.4.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-05-31-release/index.md","source":"@site/blog/2022-05-31-release/index.md","title":"What\'s New in Koordinator v0.4.0?","description":"We are happy to announce the release of Koordinator v0.4.0. Koordinator v0.4.0 brings in some notable changes that are most wanted by the community while continuing to expand on experimental features. And in this version, we started to gradually enhance the capabilities of the scheduler.","date":"2022-05-31T00:00:00.000Z","formattedDate":"2022\u5e745\u670831\u65e5","tags":[{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":7.525,"truncated":false,"authors":[{"name":"Joseph","title":"Koordinator maintainer","url":"https://github.com/eahydra","imageURL":"https://github.com/eahydra.png","key":"joseph"}],"frontMatter":{"slug":"release-v0.4.0","title":"What\'s New in Koordinator v0.4.0?","authors":["joseph"],"tags":["release"]},"prevItem":{"title":"Koordinator v0.5: Now With Node Resource Topology And More","permalink":"/zh-Hans/blog/release-v0.5.0"},"nextItem":{"title":"What\'s New in Koordinator v0.3.0?","permalink":"/zh-Hans/blog/release-v0.3.0"}},"content":"We are happy to announce the release of Koordinator v0.4.0. Koordinator v0.4.0 brings in some notable changes that are most wanted by the community while continuing to expand on experimental features. And in this version, we started to gradually enhance the capabilities of the scheduler.\\n\\n## Install or Upgrade to Koordinator v0.4.0\\n\\n### Install with helms\\n\\nKoordinator can be simply installed by helm v3.5+, which is a simple command-line tool, and you can get it\\nfrom [here](https://github.com/helm/helm/releases).\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Install the latest version.\\n$ helm install koordinator koordinator-sh/koordinator --version 0.4.0\\n```\\n\\n### Upgrade with helm\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Upgrade the latest version.\\n$ helm upgrade koordinator koordinator-sh/koordinator --version 0.4.0 [--force]\\n```\\n\\nFor more details, please refer to the [installation manual](/docs/installation).\\n\\n## Enhanced node-side scheduling capabilities\\n\\n### Custom memory evict threshold\\n\\nIn the Koordinator v0.2.0, an ability to improve the stability of the node side in the co-location scenario was introduced: [Active eviction mechanism based on memory safety thresholds](/blog/release-v0.2.0#active-eviction-mechanism-based-on-memory-safety-thresholds). The current memory utilization safety threshold default value is 70%, now in the v0.4.0 version, you can modify the `memoryEvictThresholdPercent` with 60% in ConfigMap `slo-controller-config` according to the actual situation:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: koordinator-system\\ndata:\\n  colocation-config: |\\n    {\\n      \\"enable\\": true\\n    }\\n  resource-threshold-config: |\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"enable\\": true,\\n        \\"memoryEvictThresholdPercent\\": 60\\n      }\\n    }\\n```\\n\\n### BE Pods eviction based on satisfaction\\n\\nIn order to ensure the runtime quality of different workloads in co-location scenarios, Koordinator uses the CPU Suppress mechanism provided by koordlet on the node side to suppress workloads of the best effort type when the load increases. Or increase the resource quota for best effort type workloads when the load decreases. \\n\\nHowever, it is not suitable if there are many best effort Pods on the node and they are frequently suppressed. Therefore, in version v0.4.0, Koordinator provides an eviction mechanism based on satisfaction of the requests for the best effort Pods. If the best effort Pods are frequently suppressed, the requests of the best effort Pods cannot be satisfied, and the satisfaction is generally less than 1; if the best effort Pods are not suppressed and more CPU resources are obtained when the node resources are idle, then the requests of the best effort Pods can be satisfied, and the satisfaction is greater than or equal to 1. If the satisfaction is less than the specified threshold, and the CPU utilization of the best effort Pods is close to 100%, `koordlet` will evict some best effort Pods to improve the runtime quality of the node. The priority with lower priority or with higher CPU utilization of the same priority is evicted.\\n\\nYou can modify the ConfigMap `slo-controller-config` according to the actual situation:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: koordinator-system\\ndata:\\n  colocation-config: |\\n    {\\n      \\"enable\\": true\\n    }\\n  resource-threshold-config: |\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"enable\\": true,\\n        \\"cpuEvictBESatisfactionUpperPercent\\": 80,\\n        \\"cpuEvictBESatisfactionLowerPercent\\": 60\\n      }\\n    }\\n```\\n\\n### Group identity\\n\\nWhen latency-sensitive applications and best effort workloads are deployed on the same node, the Linux kernel scheduler must provide more scheduling opportunities to high-priority applications to minimize scheduling latency and the impacts of low-priority workloads on kernel scheduling. For this scenario, Koordinator integrated with the group identity allowing users to configure scheduling priorities to CPU cgroups. \\n\\nAlibaba Cloud Linux 2 with a kernel of the kernel-4.19.91-24.al7 version or later supports the group identity feature. The group identity feature relies on a dual red-black tree architecture. A low-priority red-black tree is added based on the red-black tree of the Completely Fair Scheduler (CFS) scheduling queue to store low-priority workloads. When the kernel schedules the workloads that have identities, the kernel processes the workloads based on their priorities. For more details, please refer to the [doc](https://www.alibabacloud.com/help/en/elastic-compute-service/latest/group-identity-feature).\\n\\nKoordinator defines group identity default values for Pods of different QoS types:\\n\\n| QoS | Default Value |\\n|-----|---------------|\\n| LSR | 2 |\\n| LS | 2 | \\n| BE | -1 |\\n\\nYou can modify the ConfigMap `slo-controller-config` to set group identity values according to the actual situation:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: koordinator-system\\ndata:\\n  colocation-config: |\\n    {\\n      \\"enable\\": true\\n    }\\n  resource-qos-config: |\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"lsrClass\\": {\\n            \\"cpuQOS\\": {\\n                \\"enable\\": true,\\n                \\"groupIdentity\\": 2\\n            }\\n        },\\n        \\"lsClass\\": {\\n            \\"cpuQOS\\": {\\n                \\"enable\\": true,\\n                \\"groupIdentity\\": 2\\n            }\\n        },\\n        \\"beClass\\": {\\n            \\"cpuQOS\\": {\\n                \\"enable\\": true,\\n                \\"groupIdentity\\": -1\\n            }\\n        },\\n        \\"systemClass\\": {\\n            \\"cpuQOS\\": {\\n                \\"enable\\": true,\\n                \\"groupIdentity\\": 2\\n            }\\n        }\\n      }\\n    }\\n```\\n\\nTo enable this feature, you need to update the kernel and configuration file, then install the new component `koord-runtime-proxy` of koordinator.\\n\\n## koord-runtime-proxy (experimental)\\n\\n`koord-runtime-proxy` acts as a proxy between kubelet and containerd(dockerd under dockershim scenario), which is designed to intercept CRI request, and apply some resource management policies, such as setting different cgroup parameters by pod priorities under hybrid workload orchestration scenario, applying new isolation policies for latest Linux kernel, CPU architecture, and etc.\\n\\nThere are two components involved, koord-runtime-proxy and RuntimePlugins.\\n\\n![image](../../static/img/koord-runtime-proxy-architecture.svg)\\n\\n### koord-runtime-proxy\\nkoord-runtime-proxy is in charge of intercepting request during pod\'s lifecycle, such as RunPodSandbox, CreateContainer etc., and then calling RuntimePlugins to do resource isolation policies before transferring request to backend containerd(dockerd) and after transferring response to kubelet. koord-runtime-proxy provides an isolation-policy-execution framework which allows customized plugins registered to do specified isolation policies, these plugins are called RuntimePlugins. koord-runtime-proxy itself does NOT do any isolation policies.\\n\\n### RuntimePlugins\\nRuntimePlugins register events(RunPodSandbox etc.) to koord-runtime-proxy and would receive notifications when events happen. RuntimePlugins should complete resource isolation policies basing on the notification message, and then response koord-runtime-proxy, koord-runtime-proxy would decide to transfer request to backend containerd or discard request according to plugins\' response.\\n\\nIf no RuntimePlugins registered, koord-runtime-proxy would become a transparent proxy between kubelet and containerd.\\n\\nFor more details, please refer to the [design doc](https://github.com/koordinator-sh/koordinator/blob/main/docs/design-archive/runtime-manager-design-doc.md).\\n\\n### Installation\\n\\nWhen installing koord-runtime-proxy, you need to change the startup parameters of the kubelet, set the CRI parameters to point to the koord-runtime-proxy, and configure the CRI parameters of the corresponding container runtime when installing the koord-runtime-proxy. \\n\\nkoord-runtime-proxy is in the Alpha experimental version stage. Currently, it provides a minimum set of extension points. At the same time, there may be some bugs. You are welcome to try it and give feedback.\\n\\nFor detailed installation process, please refer to the [manual](/docs/installation#install-koord-runtime-proxy-experimental).\\n\\n## Load-Aware Scheduling\\n\\nAlthough Koordinator provides the co-location mechanism to improve the resource utilization of the cluster and reduce costs, it does not yet have the ability to control the utilization level of the cluster dimension, Best Effort workloads may also interfere with latency-sensitive applications. Load-aware scheduling plugin helps Koordinator to achieve this capability.\\n\\nThe scheduling plugin filters abnormal nodes and scores them according to resource usage. This scheduling plugin extends the Filter/Score/Reserve/Unreserve extension points defined in the Kubernetes scheduling framework.\\n\\nBy default, abnormal nodes are filtered, and users can decide whether to enable or not by configuring as needed.\\n- Filter nodes where koordlet fails to update NodeMetric. \\n- Filter nodes by utilization thresholds. If the configuration enables, the plugin will exclude nodes with *latestUsageUtilization >= utilizationThreshold*.\\n\\nThis plugin is dependent on NodeMetric\'s reporting period. Different reporting periods need to be set according to different scenarios and workloads. Therefore, NodeMetricSpec has been extended to support user-defined reporting period and aggregation period. Users can modify `slo-controller-config` to complete the corresponding configuration, and the controller in `koord-manager` will be responsible for updating the reporting period and aggregation period fields of NodeMetrics of related nodes.\\n\\nCurrently, the resource utilization thresholds of nodes are configured based on experience to ensure the runtime quality of nodes. But there are also ways to evaluate the workload running on the node to arrive at a more appropriate threshold for resource utilization. For example, in a time-sharing scenario, a higher threshold can be set to allow scheduling to run more best effort workloads during the valley of latency-sensitive applications. When the peak of latency-sensitive applications comes up, lower the threshold and evict some best effort workloads. In addition, 3-sigma can be used to analyze the utilization level in the cluster to obtain a more appropriate threshold.\\n\\nThe core logic of the scoring algorithm is to select the node with the smallest resource usage. However, considering the delay of resource usage reporting and the delay of Pod startup time, the resource requests of the Pods that have been scheduled and the Pods currently being scheduled within the time window will also be estimated, and the estimated values will be involved in the calculation.\\n\\nAt present, Koordinator does not have the ability to profile workloads. Different types of workloads have different ways of building profiles. For example, long-running pods need to be scheduled with long-period profiling, while short-period pods should be scheduled with short-period profiling.\\n\\nFor more details, please refer to the [proposal](https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/scheduling/20220510-load-aware-scheduling.md).\\n\\n## What Comes Next\\n\\nFor more details, please refer to our [milestone](https://github.com/koordinator-sh/koordinator/milestones). Hope it\\nhelps!"},{"id":"release-v0.3.0","metadata":{"permalink":"/zh-Hans/blog/release-v0.3.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-05-07-release/index.md","source":"@site/blog/2022-05-07-release/index.md","title":"What\'s New in Koordinator v0.3.0?","description":"We are happy to announce the v0.3.0 release of Koordinator. After starting small and learning what users needed, we","date":"2022-05-07T00:00:00.000Z","formattedDate":"2022\u5e745\u67087\u65e5","tags":[{"label":"koordinator","permalink":"/zh-Hans/blog/tags/koordinator"},{"label":"colocation","permalink":"/zh-Hans/blog/tags/colocation"},{"label":"kubernetes","permalink":"/zh-Hans/blog/tags/kubernetes"},{"label":"scheduling","permalink":"/zh-Hans/blog/tags/scheduling"},{"label":"orchestration","permalink":"/zh-Hans/blog/tags/orchestration"},{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":11.605,"truncated":false,"authors":[{"name":"Jason","title":"Koordinator maintainer","url":"https://github.com/jasonliu747","imageURL":"https://github.com/jasonliu747.png","key":"jason"}],"frontMatter":{"slug":"release-v0.3.0","title":"What\'s New in Koordinator v0.3.0?","authors":["jason"],"tags":["koordinator","colocation","kubernetes","scheduling","orchestration","release"]},"prevItem":{"title":"What\'s New in Koordinator v0.4.0?","permalink":"/zh-Hans/blog/release-v0.4.0"},"nextItem":{"title":"Koordinator v0.2.0 - Enhanced node-side scheduling capabilities","permalink":"/zh-Hans/blog/release-v0.2.0"}},"content":"We are happy to announce the v0.3.0 release of **Koordinator**. After starting small and learning what users needed, we\\nare able to adjust its path and develop features needed for a stable community release.\\n\\nThe release of Koordinator v0.3.0 brings in some notable changes that are most wanted by the community while continuing\\nto expand on experimental features.\\n\\n## Install or Upgrade to Koordinator v0.3.0\\n\\n### Install with helms\\n\\nKoordinator can be simply installed by helm v3.5+, which is a simple command-line tool, and you can get it\\nfrom [here](https://github.com/helm/helm/releases).\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Install the latest version.\\n$ helm install koordinator koordinator-sh/koordinator --version 0.3.0\\n```\\n\\n### Upgrade with helm\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Upgrade the latest version.\\n$ helm upgrade koordinator koordinator-sh/koordinator --version 0.3.0 [--force]\\n```\\n\\nFor more details, please refer to the [installation manual](/docs/installation).\\n\\n## CPU Burst\\n\\nCPU Burst is a service level objective (SLO)-aware resource scheduling feature provided by Koordinator. You can use CPU\\nBurst to improve the performance of latency-sensitive applications. CPU scheduling for\\na container may be throttled by the kernel due to the CPU limit, which downgrades the performance of the application.\\nKoordlet automatically detects CPU throttling events and automatically adjusts the CPU limit to a\\nproper value. This greatly improves the performance of latency-sensitive applications.\\n\\n### How CPU Burst works\\n\\nKubernetes allows you to specify CPU limits, which can be reused based on time-sharing. If you specify a CPU limit for a\\ncontainer, the OS limits the amount of CPU resources that can be used by the container within a specific time period.\\nFor example, you set the CPU limit of a container to 2. The OS kernel limits the CPU time slices that the container can\\nuse to 200 milliseconds within each 100-millisecond period.\\n\\nCPU utilization is a key metric that is used to evaluate the performance of a container. In most cases, the CPU limit is\\nspecified based on CPU utilization. CPU utilization on a per-millisecond basis shows more spikes than on a per-second\\nbasis. If the CPU utilization of a container reaches the limit within a 100-millisecond period, CPU throttling is\\nenforced by the OS kernel and threads in the container are suspended for the rest of the time period.\\n\\n### How to use CPU Burst\\n\\n- Use an annotation to enable CPU Burst\\n\\n  Add the following annotation to the pod configuration to enable CPU Burst:\\n\\n```yaml\\nannotations:\\n  # Set the value to auto to enable CPU Burst for the pod. \\n  koordinator.sh/cpuBurst: \'{\\"policy\\": \\"auto\\"}\'\\n  # To disable CPU Burst for the pod, set the value to none. \\n  #koordinator.sh/cpuBurst: \'{\\"policy\\": \\"none\\"}\'\\n```\\n\\n- Use a ConfigMap to enable CPU Burst for all pods in a cluster\\n\\n  Modify the slo-controller-config ConfigMap based on the\\n  following content to enable CPU Burst for all pods in a cluster:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: koordinator-system\\ndata:\\n  cpu-burst-config: \'{\\"clusterStrategy\\": {\\"policy\\": \\"auto\\"}}\'\\n  #cpu-burst-config: \'{\\"clusterStrategy\\": {\\"policy\\": \\"cpuBurstOnly\\"}}\'\\n  #cpu-burst-config: \'{\\"clusterStrategy\\": {\\"policy\\": \\"none\\"}}\'\\n```\\n\\n- Advanced configurations\\n\\n  The following code block shows the pod annotations and ConfigMap fields that you can use for advanced configurations:\\n\\n```yaml\\n# Example of the slo-controller-config ConfigMap. \\ndata:\\n  cpu-burst-config: |\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"policy\\": \\"auto\\",\\n        \\"cpuBurstPercent\\": 1000,\\n        \\"cfsQuotaBurstPercent\\": 300,\\n        \\"sharePoolThresholdPercent\\": 50,\\n        \\"cfsQuotaBurstPeriodSeconds\\": -1\\n      }\\n    }\\n\\n  # Example of pod annotations. \\n  koordinator.sh/cpuBurst: \'{\\"policy\\": \\"auto\\", \\"cpuBurstPercent\\": 1000, \\"cfsQuotaBurstPercent\\": 300, \\"cfsQuotaBurstPeriodSeconds\\": -1}\'\\n```\\n\\nThe following table describes the ConfigMap fields that you can use for advanced configurations of CPU Burst.\\n\\n| Field                      | Data type | Description                                                                                                                                                                                                                                                                                                                                                                                                                             |\\n|----------------------------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| policy                     | string    | <ul> <li> none: disables CPU Burst. If you set the value to none, the related fields are reset to their original values. This is the default value.</li> <li>cpuBurstOnly: enables the CPU Burst feature only for the kernel of Alibaba Cloud Linux 2. </li><li> cfsQuotaBurstOnly: enables automatic adjustment of CFS quotas of general kernel versions. </li> <li> auto: enables CPU Burst and all the related features. </li> </ul> |\\n| cpuBurstPercent            | int       | Default value:`1000`. Unit: %. This field specifies the percentage to which the CPU limit can be increased by CPU Burst. If the CPU limit is set to `1`, CPU Burst can increase the limit to 10 by default.                                                                                                                                                                                                                             |\\n| cfsQuotaBurstPercent       | int       | Default value: `300`. Unit: %. This field specifies the maximum percentage to which the value of cfs_quota in the cgroup parameters can be increased. By default, the value of cfs_quota can be increased to at most three times.                                                                                                                                                                                                       |\\n| cfsQuotaBurstPeriodSeconds | int       | Default value: `-1`. Unit: seconds. This indicates that the time period in which the container can run with an increased CFS quota is unlimited. This field specifies the time period in which the container can run with an increased CFS quota, which cannot exceed the upper limit specified by `cfsQuotaBurstPercent`.                                                                                                              |\\n| sharePoolThresholdPercent  | int       | Default value: `50`. Unit: %. This field specifies the CPU utilization threshold of the node. If the CPU utilization of the node exceeds the threshold, the value of cfs_quota in cgroup parameters is reset to the original value.                                                                                                                                                                                                     |\\n\\n## L3 cache and MBA resource isolation\\n\\nPods of different priorities are usually deployed on the same machine. This may cause pods to compete for computing\\nresources. As a result, the quality of service (QoS) of your service cannot be ensured. The Resource Director\\nTechnology (RDT) controls the Last Level Cache (L3 cache) that can be used by workloads of different priorities. RDT\\nalso uses the Memory Bandwidth Allocation (MBA) feature to control the memory bandwidth that can be used by workloads.\\nThis isolates the L3 cache and memory bandwidth used by workloads, ensures the QoS of high-priority workloads, and\\nimproves overall resource utilization. This topic describes how to improve the resource isolation of pods with\\ndifferent priorities by controlling the L3 cache and using the MBA feature.\\n\\n### How to use L3 cache and MBA resource isolation\\n\\n- Use a ConfigMap to enable L3 cache and MBA resource isolation for all pods in a cluster\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: koordinator-system\\ndata:\\n  resource-qos-config: |-\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"lsClass\\": {\\n           \\"resctrlQOS\\": {\\n             \\"enable\\": true,\\n             \\"catRangeStartPercent\\": 0,\\n             \\"catRangeEndPercent\\": 100,\\n             \\"MBAPercent\\": 100\\n           }\\n         },\\n        \\"beClass\\": {\\n           \\"resctrlQOS\\": {\\n             \\"enable\\": true,\\n             \\"catRangeStartPercent\\": 0,\\n             \\"catRangeEndPercent\\": 30,\\n             \\"MBAPercent\\": 100\\n           }\\n         }\\n      }\\n    }\\n```\\n\\n## Memory QoS\\n\\nThe Koordlet provides the memory quality of service (QoS) feature for containers. You can use this\\nfeature to optimize the performance of memory-sensitive applications while ensuring fair memory scheduling among\\ncontainers. This topic describes how to enable the memory QoS feature for containers.\\n\\n### Background information\\n\\nThe following memory limits apply to containers:\\n\\n- The memory limit of the container. If the amount of memory that a container uses, including the page cache, is about\\n  to reach the memory limit of the container, the memory reclaim mechanism of the OS kernel is triggered. As a result,\\n  the application in the container may not be able to request or release memory resources as normal.\\n- The memory limit of the node. If the memory limit of a container is greater than the memory request of the container,\\n  the container can overcommit memory resources. In this case, the available memory on the node may become insufficient.\\n  This causes the OS kernel to reclaim memory from containers. As a result, the performance of your application is\\n  downgraded. In extreme cases, the node cannot run as normal.\\n\\nTo improve the performance of applications and the stability of nodes, Koordinator provides the memory QoS feature for\\ncontainers. We recommend that you use Anolis OS as the node OS. For other OS, we will try our best to adapt, and users\\ncan still enable it without side effects. After you enable the memory QoS feature for a container, Koordlet\\nautomatically configures the memory control group (memcg) based on the configuration of the container. This helps you\\noptimize the performance of memory-sensitive applications while ensuring fair memory scheduling on the node.\\n\\n### How to use Memory QoS\\n\\nWhen you enable memory QoS for the containers in a pod, the memcg is automatically configured based on the specified\\nratios and pod parameters. To enable memory QoS for the containers in a pod, perform the following steps:\\n\\n1. Add the following annotations to enable memory QoS for the containers in a pod:\\n\\n```yaml\\nannotations:\\n  # To enable memory QoS for the containers in a pod, set the value to auto. \\n  koordinator.sh/memoryQOS: \'{\\"policy\\": \\"auto\\"}\'\\n  # To disable memory QoS for the containers in a pod, set the value to none. \\n  #koordinator.sh/memoryQOS: \'{\\"policy\\": \\"none\\"}\'\\n```\\n\\n2. Use a ConfigMap to enable memory QoS for all the containers in a cluster.\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: koordinator-system\\ndata:\\n  resource-qos-config: |-\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"lsClass\\": {\\n           \\"memoryQOS\\": {\\n             \\"enable\\": true\\n           }\\n         },\\n        \\"beClass\\": {\\n           \\"memoryQOS\\": {\\n             \\"enable\\": true\\n           }\\n         }\\n      }\\n    }\\n```\\n\\n3. Optional. Configure advanced parameters.\\n\\n   The following table describes the advanced parameters that you can use to configure fine-grained memory QoS\\n   configurations at the pod level and cluster level.\\n\\n| Parameter         | Data type | Valid value                                                   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\\n| ------------------- | ----------- | --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| enable            | Boolean   | <ul> <li> true </li> <li> false </li> </ul>                   | <ul> <li> true: enables memory QoS for all the containers in a cluster. The default memory QoS settings for the QoS class of the containers are used. </li> <li> false: disables memory QoS for all the containers in a cluster. The memory QoS settings are restored to the original settings for the QoS class of the containers. </li> </ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\\n| policy            | String    | <ul> <li> auto </li> <li> default </li> <li> none </li> </ul> | <ul> <li> auto: enables memory QoS for the containers in the pod and uses the recommended memory QoS settings. The recommended memory QoS settings are prioritized over the cluster-wide memory QoS settings. </li> <li> default: specifies that the pod inherits the cluster-wide memory QoS settings. </li> <li> none: disables memory QoS for the pod. The relevant memory QoS settings are restored to the original settings. The original settings are prioritized over the cluster-wide memory QoS settings. </li> </ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\\n| minLimitPercent   | Int       | 0~100                                                         | Unit: %. Default value:`0`. The default value indicates that this parameter is disabled. This parameter specifies the unreclaimable proportion of the memory request of a pod. The amount of unreclaimable memory is calculated based on the following formula: `Value of memory.min = Memory request \xd7 Value of minLimitPercent/100`. This parameter is suitable for scenarios where applications are sensitive to the page cache. You can use this parameter to cache files to optimize read and write performance. For example, if you specify Memory `Request=100MiB` and `minLimitPercent=100` for a container, `the value of memory.min is 104857600`.                                                                                                                                                                                                                                                                                                                                                                                                             |\\n| lowLimitPercent   | Int       | 0~100                                                         | Unit: %. Default value:`0`. The default value indicates that this parameter is disabled. This parameter specifies the relatively unreclaimable proportion of the memory request of a pod. The amount of relatively unreclaimable memory is calculated based on the following formula: `Value of memory.low = Memory request \xd7 Value of lowLimitPercent/100`. For example, if you specify `Memory Request=100MiB` and `lowLimitPercent=100` for a container, `the value of memory.low is 104857600`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\\n| throttlingPercent | Int       | 0~100                                                         | Unit: %. Default value:`0`. The default value indicates that this parameter is disabled. This parameter specifies the memory throttling threshold for the ratio of the memory usage of a container to the memory limit of the container. The memory throttling threshold for memory usage is calculated based on the following formula: `Value of memory.high = Memory limit \xd7 Value of throttlingPercent/100`. If the memory usage of a container exceeds the memory throttling threshold, the memory used by the container will be reclaimed. This parameter is suitable for container memory overcommitment scenarios. You can use this parameter to cgroups from triggering OOM. For example, if you specify `Memory Limit=100MiB` and `throttlingPercent=80` for a container, `the value of memory.high is 83886080`, which is equal to 80 MiB.                                                                                                                                                                                                                     |\\n| wmarkRatio        | Int       | 0~100                                                         | Unit: %. Default value:`95`. A value of `0` indicates that this parameter is disabled. This parameter specifies the threshold of the usage of the memory limit or the value of `memory.high` that triggers asynchronous memory reclaim. If `throttlingPercent` is disabled, the asynchronous memory reclaim threshold for memory usage is calculated based on the following formula: `Value of memory.wmark_high = Memory limit \xd7 wmarkRatio/100`. If `throttlingPercent` is enabled, the asynchronous memory reclaim threshold for memory usage is calculated based on the following formula: `Value of memory.wmark_high = Value of memory.high \xd7 wmarkRatio/100`. If the usage of the memory limit or the value of memory.high exceeds the threshold, the memcg backend asynchronous reclaim feature is triggered. For example, if you specify `Memory Limit=100MiB`for a container, the memory throttling setting is`memory.high=83886080`, the reclaim ratio setting is `memory.wmark_ratio=95`, and the reclaim threshold setting is `memory.wmark_high=79691776`. |\\n| wmarkMinAdj       | Int       | -25~50                                                        | Unit: %. The default value is `-25` for the `LS`/ `LSR` QoS class and `50` for the `BE` QoS class. A value of 0 indicates that this parameter is disabled. This parameter specifies the adjustment to the global minimum watermark for a container. A negative value decreases the global minimum watermark and therefore postpones memory reclaim for the container. A positive value increases the global minimum watermark and therefore antedates memory reclaim for the container. For example, if you create a pod whose QoS class is LS, the default setting of this parameter is `memory.wmark_min_adj=-25`, which indicates that the minimum watermark is decreased by 25% for the containers in the pod.                                                                                                                                                                                                                                                                                                                                                       |\\n\\n## What Comes Next\\n\\nFor more details, please refer to our [milestone](https://github.com/koordinator-sh/koordinator/milestones). Hope it\\nhelps!"},{"id":"release-v0.2.0","metadata":{"permalink":"/zh-Hans/blog/release-v0.2.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-04-19-release/index.md","source":"@site/blog/2022-04-19-release/index.md","title":"Koordinator v0.2.0 - Enhanced node-side scheduling capabilities","description":"We\u2019re pleased to announce the release of Koordinator v0.2.0.","date":"2022-04-19T00:00:00.000Z","formattedDate":"2022\u5e744\u670819\u65e5","tags":[{"label":"koordinator","permalink":"/zh-Hans/blog/tags/koordinator"},{"label":"colocation","permalink":"/zh-Hans/blog/tags/colocation"},{"label":"kubernetes","permalink":"/zh-Hans/blog/tags/kubernetes"},{"label":"scheduling","permalink":"/zh-Hans/blog/tags/scheduling"},{"label":"orchestration","permalink":"/zh-Hans/blog/tags/orchestration"},{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":3.49,"truncated":false,"authors":[{"name":"Joseph","title":"Koordinator maintainer","url":"https://github.com/eahydra","imageURL":"https://github.com/eahydra.png","key":"joseph"}],"frontMatter":{"slug":"release-v0.2.0","title":"Koordinator v0.2.0 - Enhanced node-side scheduling capabilities","authors":["joseph"],"tags":["koordinator","colocation","kubernetes","scheduling","orchestration","release"]},"prevItem":{"title":"What\'s New in Koordinator v0.3.0?","permalink":"/zh-Hans/blog/release-v0.3.0"},"nextItem":{"title":"Koordinator v0.1.0 - QoS based scheduling system","permalink":"/zh-Hans/blog/release-v0.1.0"}},"content":"We\u2019re pleased to announce the release of Koordinator v0.2.0.\\n\\n## Overview\\n\\nKoordinator v0.1.0 implements basic co-location scheduling capabilities, and after the project was released, it has received attention and positive responses from the community.\\nFor some issues that everyone cares about, such as how to isolate resources for best-effort workloads, how to ensure the runtime stability of latency-sensitiv applications in co-location scenarios, etc., we have enhanced node-side scheduling capabilities in koordinator v0.2.0 to solve these problems.\\n\\n## Install or Upgrade to Koordinator v0.2.0\\n\\n### Install with helms\\n\\nKoordinator can be simply installed by helm v3.5+, which is a simple command-line tool and you can get it from [here](https://github.com/helm/helm/releases).\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Install the latest version.\\n$ helm install koordinator koordinator-sh/koordinator --version 0.2.0\\n```\\n\\n### Upgrade with helm\\n\\n```shell\\n# Firstly add koordinator charts repository if you haven\'t do this.\\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\\n\\n# [Optional]\\n$ helm repo update\\n\\n# Upgrade the latest version.\\n$ helm upgrade koordinator koordinator-sh/koordinator --version 0.2.0 [--force]\\n```\\n\\nFor more details, please refer to the [installation manual](/docs/installation).\\n\\n## Isolate resources for best-effort workloads\\n\\nIn Koodinator v0.2.0, we refined the ability to isolate resources for best-effort worklods. \\n\\n`koordlet` will set the cgroup parameters according to the resources described in the Pod Spec. Currently supports setting CPU Request/Limit, and Memory Limit.\\n\\nFor CPU resources, only the case of `request == limit` is supported, and the support for the scenario of `request <= limit` will be supported in the next version.\\n\\n## Active eviction mechanism based on memory safety thresholds\\n\\nWhen latency-sensitiv applications are serving, memory usage may increase due to bursty traffic. Similarly, there may be similar scenarios for best-effort workloads, for example, the current computing load exceeds the expected resource Request/Limit. \\n\\nThese scenarios will lead to an increase in the overall memory usage of the node, which will have an unpredictable impact on the runtime stability of the node side. For example, it can reduce the quality of service of latency-sensitiv applications or even become unavailable. Especially in a co-location environment, it is more challenging.\\n\\nWe implemented an active eviction mechanism based on memory safety thresholds in Koodinator. \\n\\n`koordlet` will regularly check the recent memory usage of node and Pods to check whether the safty threshold is exceeded. If it exceeds, it will evict some best-effort Pods to release memory. This mechanism can better ensure the stability of node and latency-sensitiv applications.\\n\\n`koordlet` currently only evicts best-effort Pods, sorted according to the Priority specified in the Pod Spec. The lower the priority, the higher the priority to be evicted, the same priority will be sorted according to the memory usage rate (RSS), the higher the memory usage, the higher the priority to be evicted. This eviction selection algorithm is not static. More dimensions will be considered in the future, and more refined implementations will be implemented for more scenarios to achieve more reasonable evictions.\\n\\nThe current memory utilization safety threshold default value is 70%. You can modify the `memoryEvictThresholdPercent` in ConfigMap `slo-controller-config` according to the actual situation, \\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: koordinator-system\\ndata:\\n  colocation-config: |\\n    {\\n      \\"enable\\": true\\n    }\\n  resource-threshold-config: |\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"enable\\": true,\\n        \\"memoryEvictThresholdPercent\\": 70\\n      }\\n    }\\n```\\n\\n## CPU Burst - Improve the performance of latency-sensitive applications\\n\\nCPU Burst is a service level objective (SLO)-aware resource scheduling feature. You can use CPU Burst to improve the performance of latency-sensitive applications. CPU scheduling for a container may be throttled by the kernel due to the CPU limit, which downgrades the performance of the application. Koordinator automatically detects CPU throttling events and automatically adjusts the CPU limit to a proper value. This greatly improves the performance of latency-sensitive applications. \\n\\nThe code of CPU Burst has been developed and is still under review and testing. It will be released in the next version. If you want to use this ability early, you are welcome to participate in Koordiantor and improve it together. For more details, please refer to the PR [#73](https://github.com/koordinator-sh/koordinator/pull/73).\\n\\n## More\\n\\nFor more details, please refer to the [Documentation](/docs). Hope it helps!"},{"id":"release-v0.1.0","metadata":{"permalink":"/zh-Hans/blog/release-v0.1.0","editUrl":"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2022-03-31-release/index.md","source":"@site/blog/2022-03-31-release/index.md","title":"Koordinator v0.1.0 - QoS based scheduling system","description":"We\u2019re pleased to announce the release of Koordinator v0.1.0.","date":"2022-03-31T00:00:00.000Z","formattedDate":"2022\u5e743\u670831\u65e5","tags":[{"label":"koordinator","permalink":"/zh-Hans/blog/tags/koordinator"},{"label":"colocation","permalink":"/zh-Hans/blog/tags/colocation"},{"label":"kubernetes","permalink":"/zh-Hans/blog/tags/kubernetes"},{"label":"scheduling","permalink":"/zh-Hans/blog/tags/scheduling"},{"label":"orchestration","permalink":"/zh-Hans/blog/tags/orchestration"},{"label":"release","permalink":"/zh-Hans/blog/tags/release"}],"readingTime":4.21,"truncated":false,"authors":[{"name":"Joseph","title":"Koordinator maintainer","url":"https://github.com/eahydra","imageURL":"https://github.com/eahydra.png","key":"joseph"},{"name":"Fangsong Zeng","title":"Koordinator maintainer","url":"https://github.com/hormes","imageURL":"https://github.com/hormes.png","key":"hormes"}],"frontMatter":{"slug":"release-v0.1.0","title":"Koordinator v0.1.0 - QoS based scheduling system","authors":["joseph","hormes"],"tags":["koordinator","colocation","kubernetes","scheduling","orchestration","release"]},"prevItem":{"title":"Koordinator v0.2.0 - Enhanced node-side scheduling capabilities","permalink":"/zh-Hans/blog/release-v0.2.0"}},"content":"We\u2019re pleased to announce the release of Koordinator v0.1.0.\\n\\n## Overview\\nKoordinator is a QoS based scheduling system for hybrid workloads orchestration on Kubernetes. It aims to improve the runtime efficiency and reliability of both latency sensitive workloads and batch jobs, simplify the complexity of resource-related configuration tuning, and increase pod deployment density to improve resource utilizations.\\n\\n## Key Features\\nKoordinator enhances the kubernetes user experiences in the workload management by providing the following:\\n\\n- Well-designed [priority](/docs/architecture/priority) and [QoS](/docs/architecture/qos) mechanism to co-locate different types of workloads in a cluster and run different types of pods on a single node.\\nAllowing for resource overcommitments to achieve high resource utilizations but still satisfying the QoS guarantees by leveraging an application profiling mechanism.\\n- Fine-grained resource orchestration and isolation mechanism to improve the efficiency of latency-sensitive workloads and batch jobs.\\n- Flexible job scheduling mechanism to support workloads in specific areas, e.g., big data, AI, audio and video.\\n- A set of tools for monitoring, troubleshooting and operations.\\n\\n## Node Metrics \\n\\nKoordinator defines the `NodeMetrics` CRD, which is used to record the resource utilization of a single node and all Pods on the node. koordlet will regularly report and update `NodeMetrics`. You can view `NodeMetrics` with the following commands.\\n\\n```shell\\n$ kubectl get nodemetrics node-1 -o yaml\\napiVersion: slo.koordinator.sh/v1alpha1\\nkind: NodeMetric\\nmetadata:\\n  creationTimestamp: \\"2022-03-30T11:50:17Z\\"\\n  generation: 1\\n  name: node-1\\n  resourceVersion: \\"2687986\\"\\n  uid: 1567bb4b-87a7-4273-a8fd-f44125c62b80\\nspec: {}\\nstatus:\\n  nodeMetric:\\n    nodeUsage:\\n      resources:\\n        cpu: 138m\\n        memory: \\"1815637738\\"\\n  podsMetric:\\n  - name: storage-service-6c7c59f868-k72r5\\n    namespace: default\\n    podUsage:\\n      resources:\\n        cpu: \\"300m\\"\\n        memory: 17828Ki\\n```\\n\\n## Colocation Resources\\n\\nAfter the Koordinator is deployed in the K8s cluster, the Koordinator will calculate the CPU and Memory resources that have been allocated but not used according to the data of `NodeMetrics`. These resources are updated in Node in the form of extended resources. \\n\\n`koordinator.sh/batch-cpu` represents the CPU resources for Best Effort workloads, \\n`koordinator.sh/batch-memory` represents the Memory resources for Best Effort workloads. \\n\\nYou can view these resources with the following commands.\\n\\n```shell\\n$ kubectl describe node node-1\\nName:               node-1\\n....\\nCapacity:\\n  cpu:                          8\\n  ephemeral-storage:            103080204Ki\\n  koordinator.sh/batch-cpu:     4541\\n  koordinator.sh/batch-memory:  17236565027\\n  memory:                       32611012Ki\\n  pods:                         64\\nAllocatable:\\n  cpu:                          7800m\\n  ephemeral-storage:            94998715850\\n  koordinator.sh/batch-cpu:     4541\\n  koordinator.sh/batch-memory:  17236565027\\n  memory:                       28629700Ki\\n  pods:                         64\\n```\\n\\n\\n## Cluster-level Colocation Profile\\n\\nIn order to make it easier for everyone to use Koordinator to co-locate different workloads, we defined `ClusterColocationProfile` to help gray workloads use co-location resources. A `ClusterColocationProfile` is CRD like the one below. Please do edit each parameter to fit your own use cases.\\n\\n```yaml\\napiVersion: config.koordinator.sh/v1alpha1\\nkind: ClusterColocationProfile\\nmetadata:\\n  name: colocation-profile-example\\nspec:\\n  namespaceSelector:\\n    matchLabels:\\n      koordinator.sh/enable-colocation: \\"true\\"\\n  selector:\\n    matchLabels:\\n      sparkoperator.k8s.io/launched-by-spark-operator: \\"true\\"\\n  qosClass: BE\\n  priorityClassName: koord-batch\\n  koordinatorPriority: 1000\\n  schedulerName: koord-scheduler\\n  labels:\\n    koordinator.sh/mutated: \\"true\\"\\n  annotations: \\n    koordinator.sh/intercepted: \\"true\\"\\n  patch:\\n    spec:\\n      terminationGracePeriodSeconds: 30\\n```\\n\\nVarious Koordinator components ensure scheduling and runtime quality through labels `koordinator.sh/qosClass`, `koordinator.sh/priority` and kubernetes native priority.\\n\\nWith the webhook mutating mechanism provided by Kubernetes, koord-manager will modify Pod resource requirements to co-located resources, and inject the QoS and Priority defined by Koordinator into Pod.\\n\\nTaking the above Profile as an example, when the Spark Operator creates a new Pod in the namespace with the `koordinator.sh/enable-colocation=true` label, the Koordinator QoS label `koordinator.sh/qosClass` will be injected into the Pod. According to the Profile definition PriorityClassName, modify the Pod\'s PriorityClassName and the corresponding Priority value. Users can also set the Koordinator Priority according to their needs to achieve more fine-grained priority management, so the Koordinator Priority label `koordinator.sh/priority` is also injected into the Pod. Koordinator provides an enhanced scheduler koord-scheduler, so you need to modify the Pod\'s scheduler name koord-scheduler through Profile.\\n\\nIf you expect to integrate Koordinator into your own system, please learn more about the [core concepts](/docs/architecture/overview).\\n\\n## CPU Suppress\\n\\nIn order to ensure the runtime quality of different workloads in co-located scenarios, Koordinator uses the CPU Suppress mechanism provided by koordlet on the node side to suppress workloads of the Best Effort type when the load increases. Or increase the resource quota for Best Effort type workloads when the load decreases.\\n\\nWhen installing through the helm chart, the ConfigMap `slo-controller-config` will be created in the koordinator-system namespace, and the CPU Suppress mechanism is enabled by default. If it needs to be closed, refer to the configuration below, and modify the configuration of the resource-threshold-config section to take effect.\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: slo-controller-config\\n  namespace: {{ .Values.installation.namespace }}\\ndata:\\n  ...\\n  resource-threshold-config: |\\n    {\\n      \\"clusterStrategy\\": {\\n        \\"enable\\": false\\n      }\\n    }\\n```\\n\\n## Colocation Resources Balance\\nKoordinator currently adopts a strategy for node co-location resource scheduling, which prioritizes scheduling to machines with more resources remaining in co-location to avoid Best Effort workloads crowding together. More rich scheduling capabilities are on the way.\\n\\n## Tutorial - Colocation of Spark Jobs\\n\\nApache Spark is an analysis engine for large-scale data processing, which is widely used in Big Data, SQL Analysis and Machine Learning scenarios. \\nWe provide a tutorial to help you how to quickly use Koordinator to run Spark Jobs in colocation mode with other latency sensitive applications. For more details, please refer to the [tutorial](/docs/best-practices/colocation-of-spark-jobs).\\n\\n## Summary\\n\\nFore More details, please refer to the [Documentation](/docs). Hope it helps!"}]}')}}]);